<FONT color=blue><STRONG>: </STRONG>Can we use the process abstraction to simplify the construction of the operating system itself and to make it more secure, more reliable, and more flexible? </FONT>
<H3 class=sectionHead><SPAN class=headers>Title:</SPAN></B><SPAN class=RefText> 3.5 Operating System Structure</SPAN></FONT></H3>
<P class=sectionHead>&nbsp;</P></A><FONT style="BACKGROUND-COLOR: #ffffff">We started this chapter with a list of functionality that users and applications need from the operating system. We have shown that by careful design of the system call interface, we can offload some of the work of the operating system to user programs, such as to a shell or to a print server. </FONT>
<P>In the rest of this chapter, we ask how should we organize the remaining parts of the operating system. There are many dependencies among the modules inside the operating system, and there is often quite frequent interaction between these modules: </P>
<UL class=itemize1>
<LI class=itemize>
<P>Many parts of the operating system depend on synchronization primitives for coordinating access to shared data structures with the kernel. </P>
<LI class=itemize>
<P>The virtual memory system depends on low-level hardware support for address translation, support that is specific to a particular processor architecture. </P>
<LI class=itemize>
<P>Both the file system and the virtual memory system share a common pool of blocks of physical memory. They also both depend on the disk device driver. </P>
<LI class=itemize>
<P>The file system can depend on the network protocol stack if the disk is physically located on a different machine.</P></LI></UL>
<P>This has led operating system designers to wrestle with a fundamental tradeoff: by centralizing functionality in the kernel, performance is improved and it makes it easier to arrange tight integration between kernel modules. However, the resulting systems are less flexible, less easy to change, and less adaptive to user or application needs. We discuss these tradeoffs by describing several options for the operating system architecture. <A id=x1-65001r111 name=x1-65001r111></A></P>
<H4 class=subsectionHead>3.5.1 <A id=x1-660001 name=x1-660001></A>Monolithic Kernels</H4><A id=x1-6600113 name=x1-6600113></A>
<HR>

<P></P>
<CENTER><img alt="" src="file:///[PrimaryStorage]Images/image00212.gif" data-calibre-src="OEBPS/Images/image00212.gif"></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><B>Figure&nbsp;3.13: </B>In a monolithic operating system kernel, most of the operating system functionality is linked together inside the kernel. Kernel modules directly call into other kernel modules to perform needed functions. For example, the virtual memory system uses buffer management, synchronization, and the hardware abstraction layer.</P></TD></TR></TBODY></TABLE>
<HR>
Almost all widely used commercial operating systems, such as Windows, MacOS, and Linux, take a similar approach to the architecture of the kernel &#8212; a monolithic design. As shown in Figure&nbsp;<A data-ixbpernmnjoud4qz9ktzkg='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6600113"}'>3.13</A>, with a <EM><A data-ixbpernmnjoud4qz9ktzkg='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:monolithic kernel"}'>monolithic kernel</A></EM>, most of the operating system functionality runs inside the operating system kernel. In truth, the term is a bit of a misnomer, because even in so-called monolithic systems, there are often large segments of what users consider the operating system that runs outside the kernel, either as utilities like the shell, or in system libraries, such as libraries to manage the user interface. 
<P>Internal to a monolithic kernel, the operating system designer is free to develop whatever interfaces between modules that make sense, and so there is quite a bit of variation from operating system to operating system in those internal structures. However, two common themes emerge across systems: to improve portability, almost all modern operating systems have both a hardware abstraction layer and dynamically loaded device drivers. </P>
<H5 class=subsubsectionHead><A id=x1-670001 name=x1-670001></A>Hardware Abstraction Layer</H5>A key goal of operating systems is to be portable across a wide variety of hardware platforms. To accomplish this, especially within a monolithic system, requires careful design of the <EM><A data-ixbpernmnjoud4qz9ktzkg='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:hardware abstraction layer"}'>hardware abstraction layer</A></EM>. The hardware abstraction layer (HAL) is a portable interface to machine configuration and processor-specific operations within the kernel. For example, within the same processor family, such as an Intel x86, different computer manufacturers will require different machine-specific code to configure and manage interrupts and hardware timers. 
<P>Operating systems that are portable across processor families, say between an ARM and an x86 or between a 32-bit and a 64-bit x86, will need processor-specific code for process and thread context switches. The interrupt, processor exception, and system call trap handling is also processor-specific; all systems have those functions, but the specific implementation will vary. As we will see in Chapter&nbsp;8, machines differ quite a bit in their architecture for managing virtual address spaces; most kernels provide portable abstractions on top of the machine-dependent routines, such as to translate virtual addresses to physical addresses or to copy memory from applications to kernel memory and vice versa. </P>
<P>With a well-defined hardware abstraction layer in place, most of the operating system is machine- and processor-independent. Thus, porting an operating system to a new computer is just a matter of creating new implementations of these low-level HAL routines and re-linking. </P>
<P></P>
<DIV class=sidebar align=center>
<HR>

<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><SPAN class=sidebar_name><B><I>The hardware abstraction layer in Windows</I></B></SPAN> </P>
<P>As a concrete example, Windows has a two-pronged strategy for portability. To allow the same Windows kernel binary to be used across personal computers manufactured by different vendors, the kernel is dynamically linked at boot time with a set of library routines specifically written for each hardware configuration. This isolates the kernel from the specifics of the motherboard hardware. </P>
<P>Windows also runs across a number of different processor architectures. Typically, a different kernel binary is produced for each type of processor, with any needed processor-specific code; sometimes, conditional execution is used to allow a kernel binary to be shared across closely related processor designs. </P>
<P></P></TD></TR></TBODY></TABLE>
<HR>
</DIV>
<H5 class=subsubsectionHead><A id=x1-680001 name=x1-680001></A>Dynamically Installed Device Drivers</H5>A similar consideration leads to operating systems that can easily accommodate a wide variety of physical I/O devices. Although there are only a handful of different instruction set architectures in wide use today, there are a huge number of different types of physical I/O devices, manufactured by a large number of companies. There is diversity in the hardware interfaces to devices as well as in the hardware chip sets for managing the devices. A recent survey found that approximately 70% of the code in the Linux kernel was in device-specific software. 
<P>To keep the rest of the operating system kernel portable, we want to decouple the operating system source code from the specifics of each device. For instance, suppose a manufacturer creates a new printer &#8212; what steps does the operating system manufacturer need to take to accommodate that change? </P>
<P>The key innovation, widely adopted today, is a <EM><A data-ixbpernmnjoud4qz9ktzkg='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:dynamically loadable device driver"}'>dynamically loadable device driver</A></EM>. A dynamically loadable device driver is software to manage a specific device, interface, or chipset, added to the operating system kernel after the kernel starts running, to handle the devices that are present on a particular machine. The device manufacturer typically provides the driver code, using a standard interface supported by the kernel. The operating system kernel calls into the driver whenever it needs to read or write data to the device. </P>
<P>The operating system boots with a small number of device drivers &#8212; e.g., for the disk (to read the operating system binary into memory). For the devices physically attached to the computer, the computer manufacturer bundles those drivers into a file it stores along with the bootloader. When the operating system starts up, it queries the I/O bus for which devices are attached to the computer and then loads those drivers from the file on disk. Finally, for any network-attached devices, such as a network printer, the operating system can load those drivers over the Internet. </P>
<P>While dynamically loadable device drivers solve one problem, they pose a different one. Errors in a device driver can corrupt the operating system kernel and application data structures; just as with a regular program, errors may not be caught immediately, so that user may be unaware that their data is being silently modified. Even worse, a malicious attacker can use device drivers to introduce a computer virus into the operating system kernel, and thereby silently gain control over the entire computer. Recent studies have found that 90% of all system crashes were due to bugs in device drivers, rather than in the operating system itself. </P>
<P>Operating system developers have taken five approaches to dealing with this issue: </P>
<UL class=itemize1>
<LI class=itemize>
<P><B>Code inspection.</B> Operating system vendors typically require all device driver code to be submitted in advance for inspection and testing, before being allowed into the kernel. </P>
<LI class=itemize>
<P><B>Bug tracking.</B> After every system crash, the operating system can collect information about the system configuration and the current kernel stack, and sends this information back to a central database for analysis. Microsoft does this on a wide scale. With hundreds of millions of installed computers, even a low rate of failure can yield millions of bug reports per day. Many crashes happen inside the device driver itself, but even those that do not can sometimes be tracked down. For example, if failures are correlated with the presence of a particular device driver, or increase after the release of a new version of the driver, that can indicate the source of a problem. </P>
<LI class=itemize>
<P><B>User-level device drivers.</B> Both Apple and Microsoft strongly encourage new device drivers to run at user-level rather than in the kernel. Each device driver runs in a separate user-level process, using system calls to manipulate the physical device. This way, a buggy driver can only affect its own internal data structures and not the rest of the operating system kernel; if the device driver crashes, the kernel can restart it easily. </P>
<P>Although user-level device drivers are becoming more common, it can be time-consuming to port existing device drivers to run at user-level. Unfortunately, there is a huge amount of existing device driver code that directly addresses internal kernel data structures; drawing a boundary around these drivers has proven difficult. Of course, supporting legacy drivers is less of a problem as completely new hardware and operating system platforms, such as smartphones and tablets, are developed. <A id=x1-6800114 name=x1-6800114></A></P>
<HR>

<CENTER><img alt="" src="file:///[PrimaryStorage]Images/image00213.gif" data-calibre-src="OEBPS/Images/image00213.gif"> </CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><B>Figure&nbsp;3.14: </B>Legacy device drivers can run inside a guest operating system on top of a virtual machine in order to isolate the effect of implementation errors in driver code.</P></TD></TR></TBODY></TABLE>
<HR>

<LI class=itemize>
<P><B>Virtual machine device drivers.</B> To handle legacy device drivers, one approach that has gained some traction is to run device driver code inside a guest operating system running on a virtual machine, as shown in Figure&nbsp;<A data-ixbpernmnjoud4qz9ktzkg='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6800114"}'>3.14</A>. The guest operating system loads the device drivers as if it was running directly on the real hardware, but when the devices attempt to access the physical hardware, the underlying virtual machine monitor regains control to ensure safety. Device drivers can still have bugs, but they can only corrupt the guest operating system and not other applications running on the underlying virtual machine monitor. </P>
<LI class=itemize>
<P><B>Driver sandboxing.</B> A further challenge for both user-level device drivers and virtual machine drivers is performance. Some device drivers need frequent interaction with hardware and the rest of the kernel. Some researchers have proposed running device drivers in their own restricted execution environment inside the kernel. This requires lightweight sandboxing techniques, a topic we will return to at the end of Chapter&nbsp;8.</P></LI></UL><A id=x1-68002r116 name=x1-68002r116></A>
<H4 class=subsectionHead>3.5.2 <A id=x1-690002 name=x1-690002></A>Microkernel</H4>An alternative to the monolithic kernel approach is to run as much of the operating system as possible in one or more user-level servers. The window manager on most operating systems works this way: individual applications draw items on their portion of the screen by sending requests to the window manager. The window manager adjudicates which application window is in front or in back for each pixel on the screen, and then renders the result. If the system has a hardware graphics accelerator present, the window manager can use it to render items more quickly. Some systems have moved other parts of the operating system into user-level servers: the network stack, the file system, device drivers, and so forth. 
<P>The difference between a monolithic and a microkernel design is often transparent to the application programmer. The location of the service can be hidden in a user-level library &#8212; calls go to the library, which casts the requests either as system calls or as reads and writes to the server through a pipe. The location of the server can also be hidden inside the kernel &#8212; the application calls the kernel as if the kernel implements the service, but instead the kernel reformats the request into a pipe that the server can read. </P>
<P>A microkernel design offers considerable benefit to the operating system developer, as it easier to modularize and debug user-level services than kernel code. Aside from a potential reliability improvement, however, microkernels offer little in the way of visible benefit to end users and can slow down overall performance by inserting extra steps between the application and the services it needs. Thus, in practice, most systems adopt a hybrid model where some operating system services are run at user-level and some are in the kernel, depending on the specific tradeoff between code complexity and performance. <A id=x1-69001r115 name=x1-69001r115></A></P><A id=x1-700006 name=x1-700006></A>