<DIV class=title-box>
<DIV class=title-large><FONT style="BACKGROUND-COLOR: #7be1e1"><EM><SPAN class=headers>Title:</SPAN></B><SPAN class=RefText> Operating Systems: Principles and Practice (Second Edition) Volume II</SPAN></FONT></EM><EM>: Concurrency</EM> by Thomas Anderson and Michael Dahlin<BR>Copyright &#169;Thomas Anderson and Michael Dahlin, 2011-2015. </FONT></DIV></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">ISBN 978-0-9856735-4-3<BR><EM>Publisher:</EM> Recursive Books, Ltd., </FONT><A href="http://recursivebooks.com/"><FONT style="BACKGROUND-COLOR: #7be1e1">http://recursivebooks.com/</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> <BR><EM>Cover:</EM> Reflection Lake, Mt. Rainier <BR><EM>Cover design:</EM> Cameron Neat <BR><EM>Illustrations:</EM> Cameron Neat <BR><EM>Copy editors:</EM> Sandy Kaplan, Whitney Schmidt <BR><EM>Ebook design:</EM> Robin Briggs <BR><EM>Web design:</EM> Adam Anderson </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">SUGGESTIONS, COMMENTS, and ERRORS. We welcome suggestions, comments and error reports, by email to suggestions@recursivebooks.com </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice of rights. All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form by any means &#8212; electronic, mechanical, photocopying, recording, or otherwise &#8212; without the prior written permission of the publisher. For information on getting permissions for reprints and excerpts, contact permissions@recursivebooks.com </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice of liability. The information in this book is distributed on an &#8220;As Is" basis, without warranty. Neither the authors nor Recursive Books shall have any liability to any person or entity with respect to any loss or damage caused or alleged to be caused directly or indirectly by the information or instructions contained in this book or by the computer software and hardware products described in it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Trademarks: Throughout this book trademarked names are used. Rather than put a trademark symbol in every occurrence of a trademarked name, we state we are using the names only in an editorial fashion and to the benefit of the trademark owner with no intention of infringement of the trademark. All trademarks or service marks are the property of their respective owners. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<TABLE cellSpacing=0 cellPadding=0 width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=right><FONT style="BACKGROUND-COLOR: #7be1e1"><I>To Robin, Sandra, Katya, and Adam<BR>Tom Anderson</I><BR></FONT></DIV></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<TABLE cellSpacing=0 cellPadding=0 width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=right><FONT style="BACKGROUND-COLOR: #7be1e1"><I>To Marla, Kelly, and Keith<BR>Mike Dahlin</I><BR></FONT></DIV></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-1000 name=x1-1000>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">Contents</FONT></I></H2></A><A id=TOC name=TOC></A>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A id=QQ2-1-2 name=QQ2-1-2 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2000"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Preface</FONT></A></P>
<P class=toc_part><FONT style="BACKGROUND-COLOR: #7be1e1">I: Kernels and Processes</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">1. Introduction</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">2. The Kernel Abstraction</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">3. The Programming Interface</FONT></P>
<P class=toc_part><FONT style="BACKGROUND-COLOR: #7be1e1">II&nbsp;</FONT><A id=QQ2-1-19 name=QQ2-1-19 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9000II"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrency</FONT></A></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">4&nbsp;</FONT><A id=QQ2-1-20 name=QQ2-1-20 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrency and Threads</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.1&nbsp;</FONT><A id=QQ2-1-22 name=QQ2-1-22 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-110001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Use Cases</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1.1&nbsp;</FONT><A id=QQ2-1-24 name=QQ2-1-24 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-120001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Four Reasons to Use Threads</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.2&nbsp;</FONT><A id=QQ2-1-25 name=QQ2-1-25 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-130002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Abstraction</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.2.1&nbsp;</FONT><A id=QQ2-1-26 name=QQ2-1-26 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-140001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Running, Suspending, and Resuming Threads</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.2.2&nbsp;</FONT><A id=QQ2-1-29 name=QQ2-1-29 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-150002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Why &#8220;Unpredictable Speed&#8221;?</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.3&nbsp;</FONT><A id=QQ2-1-30 name=QQ2-1-30 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-160003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Simple Thread API</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3.1&nbsp;</FONT><A id=QQ2-1-32 name=QQ2-1-32 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-170001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">A Multi-Threaded Hello World</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.3.2&nbsp;</FONT><A id=QQ2-1-34 name=QQ2-1-34 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-180002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Fork-Join Parallelism</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.4&nbsp;</FONT><A id=QQ2-1-36 name=QQ2-1-36 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-190004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Data Structures and Life Cycle</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4.1&nbsp;</FONT><A id=QQ2-1-38 name=QQ2-1-38 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-200001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Per-Thread State and Thread Control Block (TCB)</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.4.2&nbsp;</FONT><A id=QQ2-1-39 name=QQ2-1-39 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-210002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Shared State</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.5&nbsp;</FONT><A id=QQ2-1-40 name=QQ2-1-40 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-220005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Life Cycle</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.6&nbsp;</FONT><A id=QQ2-1-43 name=QQ2-1-43 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-230006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Kernel Threads</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6.1&nbsp;</FONT><A id=QQ2-1-46 name=QQ2-1-46 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-240001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Creating a Thread</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6.2&nbsp;</FONT><A id=QQ2-1-48 name=QQ2-1-48 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-250002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Deleting a Thread</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.6.3&nbsp;</FONT><A id=QQ2-1-49 name=QQ2-1-49 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-260003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Context Switch</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.7&nbsp;</FONT><A id=QQ2-1-52 name=QQ2-1-52 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-270007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Combining Kernel Threads and Single-Threaded User Processes</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.8&nbsp;</FONT><A id=QQ2-1-53 name=QQ2-1-53 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-280008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Multi-Threaded Processes</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.8.1&nbsp;</FONT><A id=QQ2-1-54 name=QQ2-1-54 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-290001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Multi-Threaded Processes Using Kernel Threads</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.8.2&nbsp;</FONT><A id=QQ2-1-55 name=QQ2-1-55 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-300002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing User-Level Threads Without Kernel Support</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.8.3&nbsp;</FONT><A id=QQ2-1-56 name=QQ2-1-56 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-310003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing User-Level Threads With Kernel Support</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.9&nbsp;</FONT><A id=QQ2-1-57 name=QQ2-1-57 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-320009"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Alternative Abstractions</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.9.1&nbsp;</FONT><A id=QQ2-1-58 name=QQ2-1-58 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-330001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Asynchronous I/O and Event-Driven Programming</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.9.2&nbsp;</FONT><A id=QQ2-1-63 name=QQ2-1-63 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-350002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Data Parallel Programming</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">4.10&nbsp;</FONT><A id=QQ2-1-64 name=QQ2-1-64 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3600010"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Summary and Future Directions</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.10.1&nbsp;</FONT><A id=QQ2-1-65 name=QQ2-1-65 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-370001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Historical Notes</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Q1-1-66"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></A></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">5&nbsp;</FONT><A id=QQ2-1-70 name=QQ2-1-70 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Synchronizing Access to Shared Objects</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.1&nbsp;</FONT><A id=QQ2-1-71 name=QQ2-1-71 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-400001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Challenges</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1.1&nbsp;</FONT><A id=QQ2-1-72 name=QQ2-1-72 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-410001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Race Conditions</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1.2&nbsp;</FONT><A id=QQ2-1-73 name=QQ2-1-73 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-420002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Atomic Operations</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1.3&nbsp;</FONT><A id=QQ2-1-74 name=QQ2-1-74 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-430003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Too Much Milk</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1.4&nbsp;</FONT><A id=QQ2-1-75 name=QQ2-1-75 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-440004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Discussion</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1.5&nbsp;</FONT><A id=QQ2-1-76 name=QQ2-1-76 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-450005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">A Better Solution</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.2&nbsp;</FONT><A id=QQ2-1-77 name=QQ2-1-77 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-460002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Structuring Shared Objects</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.2.1&nbsp;</FONT><A id=QQ2-1-79 name=QQ2-1-79 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-470001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Shared Objects</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.2.2&nbsp;</FONT><A id=QQ2-1-81 name=QQ2-1-81 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-480002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Scope and Roadmap</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.3&nbsp;</FONT><A id=QQ2-1-82 name=QQ2-1-82 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-490003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Locks: Mutual Exclusion</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.3.1&nbsp;</FONT><A id=QQ2-1-83 name=QQ2-1-83 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-500001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Locks: API and Properties</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.3.2&nbsp;</FONT><A id=QQ2-1-84 name=QQ2-1-84 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-510002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Thread-Safe Bounded Queue</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.4&nbsp;</FONT><A id=QQ2-1-90 name=QQ2-1-90 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-540004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Condition Variables: Waiting for a Change</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4.1&nbsp;</FONT><A id=QQ2-1-92 name=QQ2-1-92 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-550001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Condition Variable Definition</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4.2&nbsp;</FONT><A id=QQ2-1-94 name=QQ2-1-94 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-560002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Life Cycle Revisited</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4.3&nbsp;</FONT><A id=QQ2-1-96 name=QQ2-1-96 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-570003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Blocking Bounded Queue</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.5&nbsp;</FONT><A id=QQ2-1-97 name=QQ2-1-97 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-580005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Designing and Implementing Shared Objects</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.5.1&nbsp;</FONT><A id=QQ2-1-98 name=QQ2-1-98 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-590001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">High Level Methodology</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.5.2&nbsp;</FONT><A id=QQ2-1-99 name=QQ2-1-99 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-600002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementation Best Practices</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.5.3&nbsp;</FONT><A id=QQ2-1-100 name=QQ2-1-100 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-610003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Three Pitfalls</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.6&nbsp;</FONT><A id=QQ2-1-101 name=QQ2-1-101 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-620006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Three Case Studies</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.6.1&nbsp;</FONT><A id=QQ2-1-102 name=QQ2-1-102 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-630001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Readers/Writers Lock</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.6.2&nbsp;</FONT><A id=QQ2-1-105 name=QQ2-1-105 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-640002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Synchronization Barriers</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.6.3&nbsp;</FONT><A id=QQ2-1-109 name=QQ2-1-109 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-650003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO Blocking Bounded Queue</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.7&nbsp;</FONT><A id=QQ2-1-111 name=QQ2-1-111 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-660007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Synchronization Objects</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7.1&nbsp;</FONT><A id=QQ2-1-112 name=QQ2-1-112 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-670001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Uniprocessor Locks by Disabling Interrupts</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7.2&nbsp;</FONT><A id=QQ2-1-113 name=QQ2-1-113 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-680002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Uniprocessor Queueing Locks</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7.3&nbsp;</FONT><A id=QQ2-1-115 name=QQ2-1-115 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-690003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Multiprocessor Spinlocks</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7.4&nbsp;</FONT><A id=QQ2-1-117 name=QQ2-1-117 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-700004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Multiprocessor Queueing Locks</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7.5&nbsp;</FONT><A id=QQ2-1-119 name=QQ2-1-119 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-710005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Linux 2.6 Kernel Mutex Lock</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7.6&nbsp;</FONT><A id=QQ2-1-120 name=QQ2-1-120 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-720006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Condition Variables</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.7.7&nbsp;</FONT><A id=QQ2-1-122 name=QQ2-1-122 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-730007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Application-level Synchronization</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.8&nbsp;</FONT><A id=QQ2-1-123 name=QQ2-1-123 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-740008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Semaphores Considered Harmful</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">5.9&nbsp;</FONT><A id=QQ2-1-124 name=QQ2-1-124 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-750009"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Summary and Future Directions</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.9.1&nbsp;</FONT><A id=QQ2-1-125 name=QQ2-1-125 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-760001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Historical Notes</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Q1-1-126"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></A></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">6&nbsp;</FONT><A id=QQ2-1-129 name=QQ2-1-129 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-780006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Multi-Object Synchronization</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">6.1&nbsp;</FONT><A id=QQ2-1-130 name=QQ2-1-130 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-790001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Multiprocessor Lock Performance</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">6.2&nbsp;</FONT><A id=QQ2-1-132 name=QQ2-1-132 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-800002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Lock Design Patterns</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.2.1&nbsp;</FONT><A id=QQ2-1-133 name=QQ2-1-133 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-810001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Fine-Grained Locking</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.2.2&nbsp;</FONT><A id=QQ2-1-134 name=QQ2-1-134 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-820002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Per-Processor Data Structures</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.2.3&nbsp;</FONT><A id=QQ2-1-135 name=QQ2-1-135 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-830003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Ownership Design Pattern</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.2.4&nbsp;</FONT><A id=QQ2-1-137 name=QQ2-1-137 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-840004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Staged Architecture</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">6.3&nbsp;</FONT><A id=QQ2-1-139 name=QQ2-1-139 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-850003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Lock Contention</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.3.1&nbsp;</FONT><A id=QQ2-1-140 name=QQ2-1-140 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-860001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">MCS Locks</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.3.2&nbsp;</FONT><A id=QQ2-1-144 name=QQ2-1-144 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-870002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Read-Copy-Update (RCU)</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">6.4&nbsp;</FONT><A id=QQ2-1-155 name=QQ2-1-155 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-920004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Multi-Object Atomicity</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.4.1&nbsp;</FONT><A id=QQ2-1-156 name=QQ2-1-156 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-930001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Careful Class Design</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.4.2&nbsp;</FONT><A id=QQ2-1-157 name=QQ2-1-157 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-940002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Acquire-All/Release-All</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.4.3&nbsp;</FONT><A id=QQ2-1-159 name=QQ2-1-159 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-950003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Two-Phase Locking</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">6.5&nbsp;</FONT><A id=QQ2-1-160 name=QQ2-1-160 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-960005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.5.1&nbsp;</FONT><A id=QQ2-1-163 name=QQ2-1-163 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-970001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock vs. Starvation</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.5.2&nbsp;</FONT><A id=QQ2-1-164 name=QQ2-1-164 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-980002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Necessary Conditions for Deadlock</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.5.3&nbsp;</FONT><A id=QQ2-1-167 name=QQ2-1-167 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-990003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Preventing Deadlock</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.5.4&nbsp;</FONT><A id=QQ2-1-168 name=QQ2-1-168 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1000004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">The Banker&#8217;s Algorithm for Avoiding Deadlock</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.5.5&nbsp;</FONT><A id=QQ2-1-173 name=QQ2-1-173 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1010005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Detecting and Recovering From Deadlocks</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">6.6&nbsp;</FONT><A id=QQ2-1-178 name=QQ2-1-178 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1040006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Non-Blocking Synchronization</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">6.7&nbsp;</FONT><A id=QQ2-1-179 name=QQ2-1-179 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1050007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Summary and Future Directions</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Q1-1-180"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></A></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">7&nbsp;</FONT><A id=QQ2-1-183 name=QQ2-1-183 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1070007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.1&nbsp;</FONT><A id=QQ2-1-184 name=QQ2-1-184 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1080001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Uniprocessor Scheduling</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.1&nbsp;</FONT><A id=QQ2-1-185 name=QQ2-1-185 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1090001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">First-In-First-Out (FIFO)</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.2&nbsp;</FONT><A id=QQ2-1-187 name=QQ2-1-187 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1100002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Shortest Job First (SJF)</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.3&nbsp;</FONT><A id=QQ2-1-188 name=QQ2-1-188 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1110003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Round Robin</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.4&nbsp;</FONT><A id=QQ2-1-192 name=QQ2-1-192 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1120004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Max-Min Fairness</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.5&nbsp;</FONT><A id=QQ2-1-193 name=QQ2-1-193 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1130005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Multi-Level Feedback</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.1.6&nbsp;</FONT><A id=QQ2-1-195 name=QQ2-1-195 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1140006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Summary</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.2&nbsp;</FONT><A id=QQ2-1-196 name=QQ2-1-196 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1150002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Multiprocessor Scheduling</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.2.1&nbsp;</FONT><A id=QQ2-1-197 name=QQ2-1-197 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1160001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Sequential Applications on Multiprocessors</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.2.2&nbsp;</FONT><A id=QQ2-1-199 name=QQ2-1-199 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1170002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Parallel Applications</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.3&nbsp;</FONT><A id=QQ2-1-210 name=QQ2-1-210 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1210003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Energy-Aware Scheduling</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.4&nbsp;</FONT><A id=QQ2-1-212 name=QQ2-1-212 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1220004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Real-Time Scheduling</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.5&nbsp;</FONT><A id=QQ2-1-214 name=QQ2-1-214 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1230005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Queueing Theory</FONT></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.1&nbsp;</FONT><A id=QQ2-1-215 name=QQ2-1-215 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1240001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Definitions</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.2&nbsp;</FONT><A id=QQ2-1-217 name=QQ2-1-217 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1250002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Little&#8217;s Law</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.3&nbsp;</FONT><A id=QQ2-1-218 name=QQ2-1-218 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1260003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Response Time Versus Utilization</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.4&nbsp;</FONT><A id=QQ2-1-224 name=QQ2-1-224 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1270004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;What if?&#8221; Questions</FONT></A><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5.5&nbsp;</FONT><A id=QQ2-1-229 name=QQ2-1-229 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1320005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Lessons</FONT></A><BR>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.6&nbsp;</FONT><A id=QQ2-1-230 name=QQ2-1-230 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1330006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Overload Management</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.7&nbsp;</FONT><A id=QQ2-1-232 name=QQ2-1-232 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1340007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Servers in a Data Center</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">7.8&nbsp;</FONT><A id=QQ2-1-233 name=QQ2-1-233 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1350008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Summary and Future Directions</FONT></A></P>
<P class=toc_section><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Q1-1-235"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></A></P>
<P class=toc_part><FONT style="BACKGROUND-COLOR: #7be1e1">III: Memory Management</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">8. Address Translation</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">9. Caching and Virtual Memory</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">10. Advanced Memory Management</FONT></P>
<P class=toc_part><FONT style="BACKGROUND-COLOR: #7be1e1">IV: Persistent Storage</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">11. File Systems: Introduction and Overview</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">12. Storage Devices</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">13. Files and Directories</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">14. Reliable Storage</FONT></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A id=QQ2-1-248 name=QQ2-1-248 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1370008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">References</FONT></A></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A id=QQ2-1-249 name=QQ2-1-249 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1380008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Glossary</FONT></A></P>
<P class=toc_chapter><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT><A id=QQ2-1-250 name=QQ2-1-250 data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1390008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">About the Authors</FONT></A></P>
<P><A id=start name=start><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></A></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-2000 name=x1-2000>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">Preface</FONT></I></H2></A><A id=Q1-1-3 name=Q1-1-3></A><A id=x1-3000 name=x1-3000>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Preface to the eBook Edition</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Operating Systems: Principles and Practice is a textbook for a first course in undergraduate operating systems. In use at over 50 colleges and universities worldwide, this textbook provides: </FONT></P>
<UL class=itemize1>
<LI class=itemize><FONT style="BACKGROUND-COLOR: #7be1e1">A path for students to understand high level concepts all the way down to working code. </FONT>
<LI class=itemize><FONT style="BACKGROUND-COLOR: #7be1e1">Extensive worked examples integrated throughout the text provide students concrete guidance for completing homework assignments. </FONT>
<LI class=itemize><FONT style="BACKGROUND-COLOR: #7be1e1">A focus on up-to-date industry technologies and practice</FONT></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The eBook edition is split into four volumes that together contain exactly the same material as the (2nd) print edition of Operating Systems: Principles and Practice, reformatted for various screen sizes. Each volume is self-contained and can be used as a standalone text, e.g., at schools that teach operating systems topics across multiple courses. </FONT></P>
<UL class=itemize1>
<LI class=itemize><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Volume 1: Kernels and Processes.</B> This volume contains Chapters 1-3 of the print edition. We describe the essential steps needed to isolate programs to prevent buggy applications and computer viruses from crashing or taking control of your system. </FONT>
<LI class=itemize><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Volume 2: Concurrency.</B> This volume contains Chapters 4-7 of the print edition. We provide a concrete methodology for writing correct concurrent programs that is in widespread use in industry, and we explain the mechanisms for context switching and synchronization from fundamental concepts down to assembly code. </FONT>
<LI class=itemize><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Volume 3: Memory Management.</B> This volume contains Chapters 8-10 of the print edition. We explain both the theory and mechanisms behind 64-bit address space translation, demand paging, and virtual machines. </FONT>
<LI class=itemize><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Volume 4: Persistent Storage.</B> This volume contains Chapters 11-14 of the print edition. We explain the technologies underlying modern extent-based, journaling, and versioning file systems.</FONT></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A more detailed description of each chapter is given in the preface to the print edition. </FONT><A id=Q1-1-5 name=Q1-1-5></A></P><A id=x1-4000 name=x1-4000>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Preface to the Print Edition</FONT></H3></A><A id=Q1-1-7 name=Q1-1-7></A><A id=x1-5000 name=x1-5000>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Why We Wrote This Book</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Many of our students tell us that operating systems was the best course they took as an undergraduate and also the most important for their careers. We are not alone &#8212; many of our colleagues report receiving similar feedback from their students. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Part of the excitement is that the core ideas in a modern operating system &#8212; protection, concurrency, virtualization, resource allocation, and reliable storage &#8212; have become widely applied throughout computer science, not just operating system kernels. Whether you get a job at Facebook, Google, Microsoft, or any other leading-edge technology company, it is impossible to build resilient, secure, and flexible computer systems without the ability to apply operating systems concepts in a variety of settings. In a modern world, nearly everything a user does is distributed, nearly every computer is multi-core, security threats abound, and many applications such as web browsers have become mini-operating systems in their own right. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It should be no surprise that for many computer science students, an undergraduate operating systems class has become a <EM>de facto</EM> requirement: a ticket to an internship and eventually to a full-time position. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, many operating systems textbooks are still stuck in the past, failing to keep pace with rapid technological change. Several widely-used books were initially written in the mid-1980&#8217;s, and they often act as if technology stopped at that point. Even when new topics are added, they are treated as an afterthought, without pruning material that has become less important. The result are textbooks that are very long, very expensive, and yet fail to provide students more than a superficial understanding of the material. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our view is that operating systems have changed dramatically over the past twenty years, and that justifies a fresh look at both <EM>how</EM> the material is taught and <EM>what</EM> is taught. The pace of innovation in operating systems has, if anything, increased over the past few years, with the introduction of the iOS and Android operating systems for smartphones, the shift to multicore computers, and the advent of cloud computing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To prepare students for this new world, we believe students need three things to succeed at understanding operating systems at a deep level: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Concepts and code.</B> We believe it is important to teach students both <EM>principles</EM> and <EM>practice</EM>, concepts and implementation, rather than either alone. This textbook takes concepts all the way down to the level of working code, e.g., how a context switch works in assembly code. In our experience, this is the only way students will really understand and master the material. All of the code in this book is available from the author&#8217;s web site, ospp.washington.edu. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Extensive worked examples.</B> In our view, students need to be able to apply concepts in practice. To that end, we have integrated a large number of example exercises, along with solutions, throughout the text. We uses these exercises extensively in our own lectures, and we have found them essential to challenging students to go beyond a superficial understanding. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Industry practice.</B> To show students how to apply operating systems concepts in a variety of settings, we use detailed, concrete examples from Facebook, Google, Microsoft, Apple, and other leading-edge technology companies throughout the textbook. Because operating systems concepts are important in a wide range of computer systems, we take these examples not only from traditional operating systems like Linux, Windows, and OS&nbsp;X but also from other systems that need to solve problems of protection, concurrency, virtualization, resource allocation, and reliable storage like databases, web browsers, web servers, mobile applications, and search engines. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Taking a fresh perspective on what students need to know to apply operating systems concepts in practice has led us to innovate in every major topic covered in an undergraduate-level course: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Kernels and Processes.</B> The safe execution of untrusted code has become central to many types of computer systems, from web browsers to virtual machines to operating systems. Yet existing textbooks treat protection as a side effect of UNIX processes, as if they are synonyms. Instead, we start from first principles: what are the minimum requirements for process isolation, how can systems implement process isolation efficiently, and what do students need to know to implement functions correctly when the caller is potentially malicious? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Concurrency.</B> With the advent of multi-core architectures, most students today will spend much of their careers writing concurrent code. Existing textbooks provide a blizzard of concurrency alternatives, most of which were abandoned decades ago as impractical. Instead, we focus on providing students a <EM>single</EM> methodology based on Mesa monitors that will enable students to write correct concurrent programs &#8212; a methodology that is by far the dominant approach used in industry. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Memory Management.</B> Even as demand-paging has become less important, virtualization has become even more important to modern computer systems. We provide a deep treatment of address translation hardware, sparse address spaces, TLBs, and on-chip caches. We then use those concepts as a springboard for describing virtual machines and related concepts such as checkpointing and copy-on-write. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Persistent Storage.</B> Reliable storage in the presence of failures is central to the design of most computer systems. Existing textbooks survey the history of file systems, spending most of their time ad hoc approaches to failure recovery and de-fragmentation. Yet no modern file systems still use those ad hoc approaches. Instead, our focus is on how file systems use extents, journaling, copy-on-write, and RAID to achieve both high performance and high reliability. </FONT></P></LI></UL><A id=Q1-1-9 name=Q1-1-9></A><A id=x1-6000 name=x1-6000>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Intended Audience</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Operating Systems: Principles and Practice is a textbook for a first course in undergraduate operating systems. We believe operating systems should be taken as early as possible in an undergraduate&#8217;s course of study; many students use the course as a springboard to an internship and a career. To that end, we have designed the textbook to assume minimal pre-requisites: specifically, students should have taken a data structures course and one on computer organization. The code examples are written in a combination of x86 assembly, C, and C++. In particular, we have designed the book to interface well with the Bryant and O&#8217;Halloran textbook. We review and cover in much more depth the material from the second half of that book. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We should note what this textbook is <EM>not</EM>: it is not intended to teach the API or internals of any specific operating system, such as Linux, Android, Windows 8, OS X, or iOS. We use many concrete examples from these systems, but our focus is on the shared problems these systems face and the technologies these systems use to solve those problems. </FONT><A id=Q1-1-11 name=Q1-1-11></A></P><A id=x1-7000 name=x1-7000>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">A Guide to Instructors</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One of our goals is enable instructors to choose an appropriate level of depth for each course topic. Each chapter begins at a conceptual level, with implementation details and the more advanced material towards the end. The more advanced material can be omitted without compromising the ability of students to follow later material. No single-quarter or single-semester course is likely to be able to cover every topic we have included, but we think it is a good thing for students to come away from an operating systems course with an appreciation that there is <EM>always</EM> more to learn. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For each topic, we attempt to convey it at three levels: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>How to reason about systems.</B> We describe core systems concepts, such as protection, concurrency, resource scheduling, virtualization, and storage, and we provide practice applying these concepts in various situations. In our view, this provides the biggest long-term payoff to students, as they are likely to need to apply these concepts in their work throughout their career, almost regardless of what project they end up working on. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Power tools.</B> We introduce students to a number of abstractions that they can apply in their work in industry immediately after graduation, and that we expect will continue to be useful for decades such as sandboxing, protected procedure calls, threads, locks, condition variables, caching, checkpointing, and transactions. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Details of specific operating systems.</B> We include numerous examples of how different operating systems work in practice. However, this material changes rapidly, and there is an order of magnitude more material than can be covered in a single semester-length course. The purpose of these examples is to illustrate how to use the operating systems principles and power tools to solve concrete problems. We do not attempt to provide a comprehensive description of Linux, OS&nbsp;X, or any other particular operating system.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The book is divided into five parts: an introduction (Chapter 1), kernels and processes (Chapters 2-3), concurrency, synchronization, and scheduling (Chapters 4-7), memory management (Chapters 8-10), and persistent storage (Chapters 11-14). </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Introduction.</B> The goal of Chapter 1 is to introduce the recurring themes found in the later chapters. We define some common terms, and we provide a bit of the history of the development of operating systems. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The Kernel Abstraction.</B> Chapter 2 covers kernel-based process protection &#8212; the concept and implementation of executing a user program with restricted privileges. Given the increasing importance of computer security issues, we believe protected execution and safe transfer across privilege levels are worth treating in depth. We have broken the description into sections, to allow instructors to choose either a quick introduction to the concepts (up through Section 2.3), or a full treatment of the kernel implementation details down to the level of interrupt handlers. Some instructors start with concurrency, and cover kernels and kernel protection afterwards. While our textbook can be used that way, we have found that students benefit from a basic understanding of the role of operating systems in executing user programs, before introducing concurrency. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The Programming Interface.</B> Chapter 3 is intended as an impedance match for students of differing backgrounds. Depending on student background, it can be skipped or covered in depth. The chapter covers the operating system from a programmer&#8217;s perspective: process creation and management, device-independent input/output, interprocess communication, and network sockets. Our goal is that students should understand at a detailed level what happens when a user clicks a link in a web browser, as the request is transferred through operating system kernels and user space processes at the client, server, and back again. This chapter also covers the organization of the operating system itself: how device drivers and the hardware abstraction layer work in a modern operating system; the difference between a monolithic and a microkernel operating system; and how policy and mechanism are separated in modern operating systems. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Concurrency and Threads.</B> Chapter 4 motivates and explains the concept of threads. Because of the increasing importance of concurrent programming, and its integration with modern programming languages like Java, many students have been introduced to multi-threaded programming in an earlier class. This is a bit dangerous, as students at this stage are prone to writing programs with race conditions, problems that may or may not be discovered with testing. Thus, the goal of this chapter is to provide a solid conceptual framework for understanding the semantics of concurrency, as well as how concurrent threads are implemented in both the operating system kernel and in user-level libraries. Instructors needing to go more quickly can omit these implementation details. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Synchronization.</B> Chapter 5 discusses the synchronization of multi-threaded programs, a central part of all operating systems and increasingly important in many other contexts. Our approach is to describe one effective method for structuring concurrent programs (based on Mesa monitors), rather than to attempt to cover several different approaches. In our view, it is more important for students to master one methodology. Monitors are a particularly robust and simple one, capable of implementing most concurrent programs efficiently. The implementation of synchronization primitives should be included if there is time, so students see that there is no magic. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multi-Object Synchronization.</B> Chapter 6 discusses advanced topics in concurrency &#8212; specifically, the twin challenges of multiprocessor lock contention and deadlock. This material is increasingly important for students working on multicore systems, but some courses may not have time to cover it in detail. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Scheduling.</B> This chapter covers the concepts of resource allocation in the specific context of processor scheduling. With the advent of data center computing and multicore architectures, the principles and practice of resource allocation have renewed importance. After a quick tour through the tradeoffs between response time and throughput for uniprocessor scheduling, the chapter covers a set of more advanced topics in affinity and multiprocessor scheduling, power-aware and deadline scheduling, as well as basic queueing theory and overload management. We conclude these topics by walking students through a case study of server-side load management. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Address Translation.</B> Chapter 8 explains mechanisms for hardware and software address translation. The first part of the chapter covers how hardware and operating systems cooperate to provide flexible, sparse address spaces through multi-level segmentation and paging. We then describe how to make memory management efficient with translation lookaside buffers (TLBs) and virtually addressed caches. We consider how to keep TLBs consistent when the operating system makes changes to its page tables. We conclude with a discussion of modern software-based protection mechanisms such as those found in the Microsoft Common Language Runtime and Google&#8217;s Native Client. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Caching and Virtual Memory.</B> Caches are central to many different types of computer systems. Most students will have seen the concept of a cache in an earlier class on machine structures. Thus, our goal is to cover the theory and implementation of caches: when they work and when they do not, as well as how they are implemented in hardware and software. We then show how these ideas are applied in the context of memory-mapped files and demand-paged virtual memory. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Advanced Memory Management.</B> Address translation is a powerful tool in system design, and we show how it can be used for zero copy I/O, virtual machines, process checkpointing, and recoverable virtual memory. As this is more advanced material, it can be skipped by those classes pressed for time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>File Systems: Introduction and Overview.</B> Chapter 11 frames the file system portion of the book, starting top down with the challenges of providing a useful file abstraction to users. We then discuss the UNIX file system interface, the major internal elements inside a file system, and how disk device drivers are structured. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Storage Devices.</B> Chapter 12 surveys block storage hardware, specifically magnetic disks and flash memory. The last two decades have seen rapid change in storage technology affecting both application programmers and operating systems designers; this chapter provides a snapshot for students, as a building block for the next two chapters. If students have previously seen this material, this chapter can be skipped. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Files and Directories.</B> Chapter 13 discusses file system layout on disk. Rather than survey all possible file layouts &#8212; something that changes rapidly over time &#8212; we use file systems as a concrete example of mapping complex data structures onto block storage devices. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Reliable Storage.</B> Chapter 14 explains the concept and implementation of reliable storage, using file systems as a concrete example. Starting with the ad hoc techniques used in early file systems, the chapter explains checkpointing and write ahead logging as alternate implementation strategies for building reliable storage, and it discusses how redundancy such as checksums and replication are used to improve reliability and availability.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We welcome and encourage suggestions for how to improve the presentation of the material; please send any comments to the publisher&#8217;s website, suggestions@recursivebooks.com. </FONT><A id=Q1-1-13 name=Q1-1-13></A></P><A id=x1-8000 name=x1-8000>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Acknowledgements</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We have been incredibly fortunate to have the help of a large number of people in the conception, writing, editing, and production of this book. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We started on the journey of writing this book over dinner at the USENIX NSDI conference in 2010. At the time, we thought perhaps it would take us the summer to complete the first version and perhaps a year before we could declare ourselves done. We were very wrong! It is no exaggeration to say that it would have taken us a lot longer without the help we have received from the people we mention below. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Perhaps most important have been our early adopters, who have given us enormously useful feedback as we have put together this edition: </FONT></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Carnegie-Mellon </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">David Eckhardt and Garth Gibson </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Clarkson </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Jeanna Matthews </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Cornell </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Gun Sirer </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">ETH Zurich </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Mothy Roscoe</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">New York University </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Laskshmi Subramanian </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Princeton University </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Kai Li </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Saarland University </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Peter Druschel </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Stanford University </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">John Ousterhout </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">University of California Riverside </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Harsha Madhyastha </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">University of California Santa Barbara </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Ben Zhao </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">University of Maryland </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Neil Spring </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">University of Michigan </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Pete Chen </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">University of Southern California </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Ramesh Govindan </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">University of Texas-Austin </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Lorenzo Alvisi </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Universtiy of Toronto </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Ding Yuan </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">University of Washington </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Gary Kimura and Ed Lazowska </FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In developing our approach to teaching operating systems, both before we started writing and afterwards as we tried to put our thoughts to paper, we made extensive use of lecture notes and slides developed by other faculty. Of particular help were the materials created by Pete Chen, Peter Druschel, Steve Gribble, Eddie Kohler, John Ousterhout, Mothy Roscoe, and Geoff Voelker. We thank them all. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our illustrator for the second edition, Cameron Neat, has been a joy to work with. We would also like to thank Simon Peter for running the multiprocessor experiments introducing Chapter 6. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We are also grateful to Lorenzo Alvisi, Adam Anderson, Pete Chen, Steve Gribble, Sam Hopkins, Ed Lazowska, Harsha Madhyastha, John Ousterhout, Mark Rich, Mothy Roscoe, Will Scott, Gun Sirer, Ion Stoica, Lakshmi Subramanian, and John Zahorjan for their helpful comments and suggestions as to how to improve the book. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We thank Josh Berlin, Marla Dahlin, Rasit Eskicioglu, Sandy Kaplan, John Ousterhout, Whitney Schmidt, and Mike Walfish for helping us identify and correct grammatical or technical bugs in the text. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We thank Jeff Dean, Garth Gibson, Mark Oskin, Simon Peter, Dave Probert, Amin Vahdat, and Mark Zbikowski for their help in explaining the internal workings of some of the commercial systems mentioned in this book. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We would like to thank Dave Wetherall, Dan Weld, Mike Walfish, Dave Patterson, Olav Kvern, Dan Halperin, Armando Fox, Robin Briggs, Katya Anderson, Sandra Anderson, Lorenzo Alvisi, and William Adams for their help and advice on textbook economics and production. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Helen Riaboff Whiteley Center as well as Don and Jeanne Dahlin were kind enough to lend us a place to escape when we needed to get chapters written. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Finally, we thank our families, our colleagues, and our students for supporting us in this larger-than-expected effort. </FONT></P>
<P><A id=Q1-1-15 name=Q1-1-15><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></A><A id=Q1-1-16 name=Q1-1-16></A><A id=Q1-1-17 name=Q1-1-17></A><A id=Q1-1-18 name=Q1-1-18></A></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<H1 class=partHead><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;II<BR></FONT><A id=x1-9000II name=x1-9000II></A><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrency</FONT></H1>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-100004 name=x1-100004>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">4. Concurrency and Threads</FONT></I></H2></A>
<DIV class=chapterQuote>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Many hands make light work. &#8212;<I>John Heywood (1546)</I> </FONT></P>
<DL>
<DT><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DD><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DD></DL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
<BR></FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the real world &#8212; outside of computers &#8212; different activities often proceed at the same time. Five jazz musicians play their instruments while reacting to each other; one car drives north while another drives south; one part of a drug molecule is attracted to a cell&#8217;s receptor, while another part is repelled; a humanoid robot walks, raises its arms, and turns its head; you fetch one article from the <EM>New York Times</EM> website while someone else fetches another; or millions of people make long distance phone calls on Mother&#8217;s Day. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We use the word <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:concurrency"}'>concurrency</A></EM> to refer to multiple activities that can happen at the same time. The real world is concurrent, and internally, modern computers are also concurrent. For example, a high-end server might have more than a dozen processors, 10 disks, and 4 network interfaces; a workstation might have a dozen active I/O devices including a screen, keyboard, mouse, camera, microphone, speaker, wireless network interface, wired network interface, printer, scanner, and disk drive. Today, even mobile phones often have multi-core processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Correctly managing concurrency is a key challenge for operating system developers. To manage hardware resources, to provide responsiveness to users, and to run multiple applications simultaneously, the operating system needs a structured way of keeping track of the various actions it needs to perform. Over the next several chapters, we will present a set of abstractions for expressing and managing concurrency. These abstractions are in widespread use in commercial operating systems because they reduce implementation complexity, improve system reliability, and improve performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrency is also a concern for many application developers. Although the abstractions we discuss were originally developed to make it easier to write correct operating system code, they have become widely used in applications: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Network services need to be able to handle multiple requests from their clients; a Google that could handle only one search request at a time, or an Amazon that could only allow one book to be bought at a time, would be much less useful. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Most applications today have user interfaces; providing good responsiveness to users while simultaneously executing application logic is much easier with a structured approach to concurrency. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Parallel programs need to be able to map work onto multiple processors to get the performance benefits of multicore architectures. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Data management systems need concurrency to mask the latency of disk and network operations.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">From the programmer&#8217;s perspective, it is much easier to think sequentially than to keep track of many simultaneous activities. For example, when reading or writing the code for a procedure, you can identify an initial state and a set of pre-conditions, think through how each successive statement changes the state, and from that determine the post-conditions. How can you write a correct program with dozens of events happening at once? </FONT><A id=x1-100011 name=x1-100011></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00388.gif" data-calibre-src="OEBPS/Images/image00388.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.1: </B>The operating system provides the illusion that programmers can create as many threads as they need, and each thread runs on its own dedicated virtual processor. In reality, of course, a machine only has a finite number of processors, and it is the operating system&#8217;s job to transparently multiplex threads onto the actual processors.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The key idea is to write a concurrent program &#8212; one with many simultaneous activities &#8212; as a set of sequential streams of execution, or <EM>threads</EM>, that interact and share results in very precise ways. Threads let us define a set of tasks that run concurrently while the code for each task is sequential. Each thread behaves as if it has its own dedicated processor, as illustrated in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. As we will see later, using the thread abstraction often requires the programmer to write additional code for coordinating multiple threads accessing shared data structures; we will discuss this topic in much more detail in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The thread abstraction lets the programmer create as many threads as needed without worrying about the exact number of physical processors, or exactly which processor is doing what at each instant. Of course, threads are only an abstraction: the physical hardware has a limited number of processors (and potentially only one!). The operating system&#8217;s job is to provide the illusion of a nearly infinite number of virtual processors even while the physical hardware is more limited. It sustains this illusion by transparently suspending and resuming threads so that at any given time only a subset of the threads are actively running. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This chapter will define the thread abstraction, illustrate how a programmer can use the abstraction, and explain how the operating system can implement threads on top of a limited number of processors. Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> explains how to coordinate threads when they operate on shared data, and Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-780006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> covers advanced issues when programming with threads. Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1070007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> discusses the policy question: how should the operating system choose <EM>which</EM> thread to run next when there are more things to run than processors on which to run them. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Chapter roadmap:</B> The rest of this chapter discusses these topics in detail: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread Use Cases.</B> What are threads useful for? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-110001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread Abstraction.</B> What is the thread abstraction as seen by a programmer? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-130002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Simple Thread API.</B> How can programmers use threads? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-160003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread Data Structures.</B> What data structures does the operating system use to manage threads? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-190004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread Life Cycle.</B> What states does a thread go through between initialization and completion? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-220005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Implementing Kernel Threads.</B> How do we implement the thread abstraction inside the operating system kernel? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-230006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Combining Kernel Threads and Single-Threaded User Processes.</B> How do we extend the implementation of kernel threads to support simple single-threaded processes? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-270007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Implementing Multi-threaded Processes.</B> How do we implement the thread abstraction for multi-threaded applications? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-280008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Alternative Abstractions.</B> What other abstractions can we use to express and implement concurrency? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-320009"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Deja vu all over again?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Threads are widely used, and several modern programming languages directly support writing programs with multiple threads. You may have programmed with threads before or have taken classes that talk about using threads. What is new here? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The discussion in this book is designed to make sense even if you have never seen threads before. If you have seen threads before, great! But we still think you will find the discussion useful. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Beyond describing the basic thread abstraction, we emphasize two points in this chapter and the following ones. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Implementation.</B> We will describe how operating systems implement threads both for their own use and for use by user-level applications. It is important to understand how threads really work so that you can understand their costs and performance characteristics and can use them effectively. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Practice.</B> We will present a methodology for writing correct multi-threaded programs. Concurrency is increasingly important in many programming tasks, but writing correct multi-threaded programs requires much more care and discipline than writing correct single-threaded programs. That said, following a few simple rules that we will describe can greatly simplify the process of writing robust multi-threaded code. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multithreaded programming has a well-deserved reputation for being difficult, but we believe the ideas in this chapter and the subsequent ones can help almost anyone become better at programming with threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-10002r1 name=x1-10002r1></A><A id=x1-110001 name=x1-110001>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.1 Thread Use Cases</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">The intuition behind the thread abstraction is simple: in a program, we can represent each concurrent task as a <EM>thread</EM>. Each thread provides the abstraction of sequential execution similar to the traditional programming model. In fact, we can think of a traditional program as <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:single-threaded program"}'>single-threaded</A></EM> with one logical sequence of steps as each instruction follows the previous one. The program executes statements, iterates through loops, and calls/returns from procedures one after another. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multi-threaded program"}'>multi-threaded program</A></EM> is a generalization of the same basic programming model. Each individual thread follows a single sequence of steps as it executes statements, iterates through loops, calls/returns from procedures, etc. However, a program can now have several such threads executing at the same time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When is it appropriate to use multiple threads within the same program? Threads have become widely used in both operating system and application code, and based on that experience, we can identify several common themes. We illustrate these themes by describing one application in some detail, to show how and why it leverages threads. </FONT><A id=x1-110012 name=x1-110012></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00389.gif" data-calibre-src="OEBPS/Images/image00389.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.2: </B>In the Earth Visualizer example, two threads each draw part of the scene, a third thread manages the user interface widgets, and a fourth thread fetches new data from a remote server. Satellite Image Credit: NASA Earth Observatory.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Consider an Earth Visualizer application similar to Google Earth (</FONT><A href="http://earth.google.com/"><FONT style="BACKGROUND-COLOR: #7be1e1">http://earth.google.com/</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">). This application lets a user virtually fly anywhere in the world, see aerial images at different resolutions, and view other information associated with each location. A key part of the design is that the user&#8217;s controls are always operable: when the user moves the mouse to a new location, the image is redrawn in the background at successively better resolutions while the program continues to let the user adjust the view, select additional information about the location for display, or enter search terms. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To implement this application, as Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-110012"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates, the programmer might write code to draw a portion of the screen, display user interface (UI) widgets, process user inputs, and fetch higher resolution images for newly visible areas. In a sequential program, these functions would run in turn. With threads, they can run concurrently so that the user interface is responsive even while new data is being fetched and the screen being redrawn. </FONT><A id=x1-11002r1 name=x1-11002r1></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.1.1 </FONT><A id=x1-120001 name=x1-120001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Four Reasons to Use Threads</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Using threads to express and manage concurrency has several advantages: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Program structure: expressing logically concurrent tasks.</B> Programs often interact with or simulate real-world applications that have concurrent activities. Threads let you express an application&#8217;s natural concurrency by writing each concurrent task as a separate thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the Earth Visualizer application, threads let different activities &#8212; updating the screen, fetching additional data, and receiving new user inputs &#8212; run at the same time. For example, to get mouse input while also re-drawing the screen and sending and receiving packets off the network, the physical processors need to split their time among these tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although one could imagine manually writing a program that interleaves these activities (e.g., draw a few pixels on the screen, then check to see if the user has moved the mouse, then check to see if new image data have arrived on the network, . . . ), using threads greatly simplifies concurrent code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another example is on the server side of the Earth Visualizer. The server needs to manage the requests of a large number of clients, each focused on a different point on the planet. Since the clients are likely behind a wide variety of access link technologies (e.g., from dialup to gigabit Ethernet), it would slow everyone down if each request needed to be completely handled before the server could start on the next one. By creating a separate thread for each client, the computation and networking needed for that client can be intermixed with other clients, without affecting the logical structure of the program. This design pattern &#8212; one server thread per client &#8212; is common; for example, the popular Apache web server assigns each client its own thread when it first connects to the server. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Responsiveness: shifting work to run in the background.</B> To improve user responsiveness and performance, a common design pattern is to create threads to perform work in the background, without the user waiting for the result. This way, the user interface can remain responsive to further commands, regardless of the complexity of the user request. In a web browser, for example, the cancel button should continue to work even (or especially!) if the downloaded page is gigantic or a script on the page takes a long time to execute. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How does this work? Many applications have a loop: get a user command, then execute the command, then get the next command. If some commands take a long time to perform, however, an application that executes everything sequentially will not be able to check for the next operation until the previous one completes. To keep the interface responsive, we can use threads to split each command into two parts: anything that can be done instantly can be done in the main event loop, and a separate thread can perform the rest of the task in the background. In the Earth Visualizer example, we used threads to move the computationally difficult parts of the application logic &#8212; rendering the display &#8212; out of the main loop. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Operating system kernels make extensive use of threads to preserve responsiveness. Many operating systems are designed so that the common case is fast. For example, when writing a file, the operating system stores the modified data in a kernel buffer, and returns immediately to the application. In the background, the operating system kernel runs a separate thread to flush the modified data out to disk. Another example is on file reads: the kernel can have a thread which attempts to anticipate which blocks are likely to be read next (e.g., if the application is reading a large file from beginning to end), and to bring those blocks from disk before the application asks for them. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance: exploiting multiple processors.</B> Programs can use threads on a multiprocessor to do work in parallel; they can do the same work in less time or more work in the same elapsed time. Today, a server might have more than a dozen processors; a desktop or laptop may include eight processor cores; even most smartphones are multicore machines. Looking forward, Moore&#8217;s law makes it likely that the number of processors per system will continue to increase. An advantage to using threads for parallelism is that the number of threads need not exactly match the number of processors in the hardware on which it is running. The operating system transparently switches which threads run on which processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For an 8-processor machine, you could parallelize the Earth Visualizer application by splitting the demanding job of rendering different portions of the image on the screen across six threads. Then, the operating system could run those six rendering threads on six processors and run the various other threads on the two remaining processors to update the on-screen navigation widgets, construct the network messages needed to fetch additional images from the distant servers, and parse reply messages. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance: managing I/O devices.</B> To do useful work, computers must interact with the outside world via I/O devices. By running tasks as separate threads, when one task is waiting for I/O, the processor can make progress on a different task. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The benefit of concurrency between the processor and the I/O is two-fold: First, processors are often much faster than the I/O systems with which they interact, so keeping the processor idle during I/O would waste much of its capacity. For example, the latency to read from disk can be tens of milliseconds, enough to execute more than 10 million instructions on a modern processor. After requesting a block from disk, the operating system can switch to another program, or another thread within the same program, until the disk completes and the original thread is ready to resume. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Second, I/O provides a way for the computer to interact with external entities, such as users pressing keys on a keyboard or a remote computer sending network packets. The arrival of this type of I/O event is unpredictable, so the processor must be able to work on other tasks while still responding quickly to these external events. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the Earth Visualizer application, a snappy user interface is essential, but much of the imagery is stored on remote servers and fetched by the application only when needed. The application provides a responsive experience when a user changes location by first downloading a small, low-resolution view of the new location. While rendering those images with one thread, another thread simultaneously fetches progressively higher-resolution images, allowing the rendering thread to update the view as the higher-resolution images arrive. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Threads vs. processes</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In Chapter&nbsp;2, we described a process as the execution of a program with restricted rights. A thread is an independent sequence of instructions running within a program. Perhaps the best way to see how these concepts are related, is to see how different operating systems combine them in different ways: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>One thread per process.</B> A simple single-threaded application has one sequence of instructions, executing from beginning to end. The operating system kernel runs those instructions in user mode to restrict access to privileged operations or system memory. The process performs system calls to ask the kernel to perform privileged operations on its behalf. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Many threads per process.</B> Alternately, a program may be structured as several concurrent threads, each executing within the restricted rights of the process. At any given time, a subset of the process&#8217;s threads may be running, while the rest are suspended. Any thread running in a process can make system calls into the kernel, blocking that thread until the call returns but allowing other threads to continue to run. Likewise, when the processor gets an I/O interrupt, it preempts one of the running threads so the kernel can run the interrupt handler; when the handler finishes, the kernel resumes that thread. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Many single-threaded processes.</B> As recently as twenty years ago, many operating systems supported multiple processes but only one thread per process. To the kernel, however, each process looks like a thread: a separate sequence of instructions, executing sometimes in the kernel and sometimes at user level. For example, on a multiprocessor, if multiple processes perform system calls at the same time, the kernel, in effect, has multiple threads executing concurrently in kernel mode. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Many kernel threads.</B> To manage complexity, shift work to the background, exploit parallelism, and hide I/O latency, the operating system kernel itself can benefit from using multiple threads. In this case, each kernel thread runs with the privileges of the kernel: it can execute privileged instructions, access system memory, and issue commands directly to I/O devices. The operating system kernel itself implements the thread abstraction for its own use. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because of the usefulness of threads, almost all modern operating systems support both multiple threads per process and multiple kernel threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-12001r22 name=x1-12001r22></A><A id=x1-130002 name=x1-130002>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.2 Thread Abstraction</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Thus far, we have described what a thread is and why it is useful. Before we go farther, we must define the thread abstraction and its properties more precisely. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:thread"}'>thread</A></EM> is <EM>a single execution sequence</EM> that represents <EM>a separately schedulable task</EM>. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Single execution sequence.</B> Each thread executes a sequence of instructions &#8212; assignments, conditionals, loops, procedures, and so on &#8212; just as in the familiar sequential programming model. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Separately schedulable task.</B> The operating system can run, suspend, or resume a thread at any time. </FONT></P></LI></UL><A id=x1-13001r24 name=x1-13001r24></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.2.1 </FONT><A id=x1-140001 name=x1-140001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Running, Suspending, and Resuming Threads</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Threads provide the illusion of an infinite number of processors. How does the operating system implement this illusion? It must execute instructions from each thread so that each thread makes progress, but the underlying hardware has only a limited number of processors, and perhaps only one! </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To map an arbitrary set of threads to a fixed set of processors, operating systems include a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:thread scheduler"}'>thread scheduler</A></EM> that can switch between threads that are running and those that are ready but not running. For example, in the previous Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, a scheduler might suspend thread 1 from processor 1, move it to the list of ready threads, and then resume thread 5 by moving it from the ready list to run on processor 1. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Switching between threads is transparent to the code being executed within each thread. The abstraction makes each thread appear to be a single stream of execution; this means the programmer can pay attention to the sequence of instruction within a thread and not whether or when that sequence may be (temporarily) suspended to let another thread run. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Threads thus provide an execution model in which <EM>each thread runs on a dedicated virtual processor with unpredictable and variable speed.</EM> From the point of view of a thread&#8217;s code, each instruction appears to execute immediately after the preceding one. However, the scheduler may suspend a thread between one instruction and the next and resume running it later. It is as if the thread were running on a processor that sometimes becomes very slow. </FONT><A id=x1-140013 name=x1-140013></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00390.gif" data-calibre-src="OEBPS/Images/image00390.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.3: </B>Three possible ways that a thread might execute, all of which are equivalent to the programmer.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-140013"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates a programmer&#8217;s view of a simple program and three (of many) possible ways the program might be executed, depending on what the scheduler does. From the thread&#8217;s point of view, other than the speed of execution, the alternatives are equivalent. Indeed, the thread would typically be unaware of which of these (or other) executions actually occurs. </FONT><A id=x1-140024 name=x1-140024></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00391.gif" data-calibre-src="OEBPS/Images/image00391.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.4: </B>Some of the many possible ways that three threads might be interleaved at runtime.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How threads are scheduled affects a thread&#8217;s interleavings with other threads. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-140024"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows some of the many possible interleavings of a program with three threads. Thread programmers should therefore not make any assumptions about the relative speed with which different threads execute. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Cooperative vs. preemptive multi-threading</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although most thread systems include a scheduler that can &#8212; at least in principle &#8212; run any thread at any time, some systems provide the abstraction of <EM>cooperative threads</EM>. In these systems, a thread runs without interruption until it explicitly relinquishes control of the processor to another thread. An advantage of cooperative multi-threading is increased control over the interleavings among threads. For example, in most cooperative multi-threading systems, only one thread runs at a time, so while a thread is running, no other thread can run and affect the system&#8217;s state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, cooperative multi-threading has significant disadvantages. For example, a long-running thread can monopolize the processor, starving other threads and making the system&#8217;s user interface sluggish or non-responsive. Additionally, modern multiprocessor machines run multiple threads at a time, so one would still have to reason about the possible interactions between threads even if cooperative multi-threading were used. Thus, although cooperative multi-threading was used in some significant systems in the past, including early versions of Apple&#8217;s MacOS operating system, it is less often used today. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The alternative we describe in this book is sometimes called <EM>preemptive multi-threading</EM> since running threads can be switched at any time. Whenever the book uses the term &#8220;multi-threading,&#8221; it means preemptive multi-threading unless we explicitly state otherwise. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-14003r26 name=x1-14003r26></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.2.2 </FONT><A id=x1-150002 name=x1-150002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Why &#8220;Unpredictable Speed&#8221;?</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">It may seem strange to require programmers to assume that a thread&#8217;s virtual processor runs at an unpredictable speed and that any interleaving with other threads is possible. Surely, the programmer should be able to take advantage of the fact that some interleavings are more likely than others? </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The thread programming model adopts this assumption as a way to guide programmers when reasoning about correctness. Rather than assuming that one thread runs at the same speed as another (or faster or slower) and trying to write programs that coordinate threads based on their relative speed of execution, multi-threaded programs should make no assumptions about the behavior of the thread scheduler. In turn, the kernel&#8217;s scheduling decisions &#8212; when to assign a thread to a processor, and when to preempt it for a different thread &#8212; can be made without worrying whether they might affect program correctness. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If threads are completely independent of each other, sharing no memory or other resources, then the order of execution will not matter &#8212; any schedule will produce the same output as any other. Most multi-threaded programs share data structures, however. In this case, as Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> describes, the programmer must use explicit synchronization to ensure program correctness regardless of the possible interleaving of instructions of different threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even if we could ignore the issue of scheduling &#8212; e.g., if there are more processors than threads so that each thread is assigned its own physical processor &#8212; the physical reality is that the relative execution speed of different threads can be significantly affected by factors outside their control. An extreme example is that the programmer may be debugging one thread by single-stepping it, while other threads run at full speed on other processors. If the programmer is to have any hope of understanding concurrent program behavior, the program&#8217;s correctness cannot depend on which threads are being observed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Variability in execution speed occurs during normal operation as well. Accessing memory can stall a processor for hundreds or thousands of cycles if a cache miss occurs. Other factors include how frequently the scheduler preempts the thread, how many physical processors are present on a machine, how large the caches are, how fast the memory is, how the energy-saving firmware adjusts the processors&#8217; clock speeds, what network messages arrive, or what input is received from the user. Execution speeds for the different threads of a program are hard to predict, can vary on different hardware, and can even vary from run to run on the same hardware. As a result, we must coordinate thread actions through explicit synchronization rather than by trying to reason about their relative speed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Is a kernel interrupt handler a thread? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>No, an interrupt handler is not a thread.</B> A kernel interrupt handler shares some resemblance to a thread: it is a single sequence of instructions that executes from beginning to end. However, an interrupt handler is not independently schedulable: it is triggered by a hardware I/O event, rather than a decision by the thread scheduler in the kernel. Once started, the interrupt handler runs to completion, unless preempted by another (higher priority) interrupt. &#9633; </FONT><A id=x1-15001r25 name=x1-15001r25></A></P><A id=x1-160003 name=x1-160003>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.3 Simple Thread API</FONT></H3></A><A id=x1-160015 name=x1-160015></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td colSpan=2 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Simple Threads API</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">void thread_create (thread, func, arg)</FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Create a new thread, storing information about it in thread. Concurrently with the calling thread, thread executes the function func with the argument arg. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">void thread_yield ()</FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The calling thread voluntarily gives up the processor to let some other thread(s) run. The scheduler can resume running the calling thread whenever it chooses to do so. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">int thread_join (thread)</FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Wait for thread to finish if it has not already done so; then return the value passed to thread_exit by that thread. Note that thread_join may be called only once for each thread. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">void thread_exit (ret)</FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Finish the current thread. Store the value ret in the current thread&#8217;s data structure. If another thread is already waiting in a call to thread_join, resume it. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.5: </B>Simplified API for using threads.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
Figure </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-160015"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows a simple API for using threads. This simplified API is based on the POSIX standard pthreads API, but it omits some POSIX options and error handling for simplicity. Most other thread packages are quite similar; if you understand how to program with this API, you will find it easy to write code with most standard thread APIs. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A good way to understand the simple threads API is that it provides a way to invoke an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:asynchronous procedure call"}'>asynchronous procedure call</A></EM>. A normal procedure call passes a set of arguments to a function, runs the function immediately on the caller&#8217;s stack, and when the function is completed, returns control back to the caller with the result. An asynchronous procedure call separates the call from the return: with thread_create, the caller starts the function, but unlike a normal procedure call, the caller continues execution concurrently with the called function. Later, the caller can wait for the function completion (with thread_join). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In Chapter&nbsp;3, we saw similar concepts in the UNIX process abstraction. thread_create&nbsp;is analogous to UNIX process fork and exec, while thread_join&nbsp;is analogous to UNIX process wait. UNIX fork creates a new process that runs concurrently with the process calling fork; UNIX exec causes that process to run a specific program. UNIX wait allows the calling process to suspend execution until the completion of the new process. </FONT><A id=x1-16002r29 name=x1-16002r29></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.3.1 </FONT><A id=x1-170001 name=x1-170001></A><FONT style="BACKGROUND-COLOR: #7be1e1">A Multi-Threaded Hello World</FONT></H4><A id=x1-170016 name=x1-170016></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;#include&nbsp;&lt;stdio.h&gt;
&nbsp;#include&nbsp;"thread.h"
&nbsp;
&nbsp;static&nbsp;void&nbsp;go(int&nbsp;n);
&nbsp;
&nbsp;#define&nbsp;NTHREADS&nbsp;10
&nbsp;static&nbsp;thread_t&nbsp;threads[NTHREADS];
&nbsp;
&nbsp;int&nbsp;main(int&nbsp;argc,&nbsp;char&nbsp;**argv)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;i;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;long&nbsp;exitValue;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;NTHREADS;&nbsp;i++){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_create(&amp;(threads[i]),&nbsp;&amp;go,&nbsp;i);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;NTHREADS;&nbsp;i++){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exitValue&nbsp;=&nbsp;thread_join(threads[i]);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("Thread&nbsp;%d&nbsp;returned&nbsp;with&nbsp;%ld\n",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i,&nbsp;exitValue);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("Main&nbsp;thread&nbsp;done.\n");
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;0;
&nbsp;}
&nbsp;
&nbsp;void&nbsp;go(int&nbsp;n)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("Hello&nbsp;from&nbsp;thread&nbsp;%d\n",&nbsp;n);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_exit(100&nbsp;+&nbsp;n);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Not&nbsp;reached
&nbsp;}
</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;%&nbsp;./threadHello
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;0
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;1
&nbsp;Thread&nbsp;0&nbsp;returned&nbsp;100
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;3
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;4
&nbsp;Thread&nbsp;1&nbsp;returned&nbsp;101
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;5
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;2
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;6
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;8
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;7
&nbsp;Hello&nbsp;from&nbsp;thread&nbsp;9
&nbsp;Thread&nbsp;2&nbsp;returned&nbsp;102
&nbsp;Thread&nbsp;3&nbsp;returned&nbsp;103
&nbsp;Thread&nbsp;4&nbsp;returned&nbsp;104
&nbsp;Thread&nbsp;5&nbsp;returned&nbsp;105
&nbsp;Thread&nbsp;6&nbsp;returned&nbsp;106
&nbsp;Thread&nbsp;7&nbsp;returned&nbsp;107
&nbsp;Thread&nbsp;8&nbsp;returned&nbsp;108
&nbsp;Thread&nbsp;9&nbsp;returned&nbsp;109
&nbsp;Main&nbsp;thread&nbsp;done.
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.6: </B>Example multi-threaded program using the simple threads API that prints &#8220;Hello&#8221; ten times. Also shown is the output of one possible run of this program.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
To illustrate how to use the simple threads API, Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-170016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows a very simple multi-threaded program written in &#8217;C&#8217;. The main function uses thread_create&nbsp;to create 10 threads. The interesting arguments are the second and third. </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The second argument, go, is a function pointer &#8212; where the newly created thread should begin execution. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The third argument, i, is passed to that function.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Thus, thread_create&nbsp;initializes the i&#8217;th thread&#8217;s state so that it is prepared to call the function go with the argument i. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When the scheduler runs the i&#8217;th thread, that thread runs the function go with the value i as an argument and prints Hello from thread i. The thread then returns the value (i + 100) by calling thread_exit. This call stores the specified value in a field in the thread_t object so that thread_join&nbsp;can retrieve it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The main function uses thread_join&nbsp;to wait for each of the threads it created. As each thread finishes, code in main reads the thread&#8217;s exit value and prints it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Why might the &#8220;Hello&#8221; message from thread 2 print <EM>after</EM> the &#8220;Hello&#8221; message for thread 5, even though thread 2 was created before thread 5? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>Creating and scheduling threads are separate operations.</B> Although threads are usually scheduled in the order that they are created, there is no guarantee. Further, even if thread 2 started running before thread 5, it might be preempted before it reaches the printf call. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Rather, the only assumption the programmer can make is that each of the threads runs on its own virtual processor with unpredictable speed. Any interleaving is possible. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Why must the &#8220;Thread returned&#8221; message from thread 2 print <EM>before</EM> the Thread returned message from thread 5? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Since the threads run on virtual processors with unpredictable speeds, the order in which the threads finish is indeterminate. However, <B>the main thread checks for thread completion in the order they were created.</B> It calls thread_join&nbsp;for thread i +1 only after thread_join&nbsp;for thread i has returned. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What is the <EM>minimum</EM> and <EM>maximum</EM> number of threads that could exist when thread 5 prints &#8220;Hello?&#8221; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>When the program starts, a main thread begins running main. That thread creates NTHREADS = 10 threads. All of those could run and complete before thread 5 prints &#8220;Hello.&#8221; Thus, <B>the minimum is two threads</B> &#8212; the main thread and thread 5. On the other hand, all 10 threads could have been created, while 5 was the first to run. Thus, <B>the maximum is 11 threads.</B> &#9633; </FONT><A id=x1-17002r32 name=x1-17002r32></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.3.2 </FONT><A id=x1-180002 name=x1-180002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Fork-Join Parallelism</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Although the interface in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-160015"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> is simple, it is remarkably powerful. Many multi-threaded applications can be designed using only these thread operations and no additional synchronization. With <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:fork-join parallelism"}'>fork-join parallelism</A></EM>, a thread can create child threads to perform work (&#8220;fork&#8221;, or thread_create), and it can wait for their results (&#8220;join&#8221;). Data may be safely shared between threads, provided it is (a) written by the parent before the child thread starts or (b) written by the child and read by the parent after the join. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If these sharing restrictions are followed, each thread executes independently and in a deterministic fashion, unaffected by the behavior of any other concurrently executing thread. The multiplexing of threads onto processors has no effect other than performance. </FONT><A id=x1-180017 name=x1-180017></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;To&nbsp;pass&nbsp;two&nbsp;arguments,&nbsp;we&nbsp;need&nbsp;a&nbsp;struct&nbsp;to&nbsp;hold&nbsp;them.
&nbsp;typedef&nbsp;struct&nbsp;bzeroparams&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;unsigned&nbsp;char&nbsp;*buffer;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;length;
&nbsp;};
&nbsp;
&nbsp;#define&nbsp;NTHREADS&nbsp;10
&nbsp;
&nbsp;void&nbsp;go&nbsp;(struct&nbsp;bzeroparams&nbsp;*p)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memset(p-&gt;buffer,&nbsp;0,&nbsp;p-&gt;length);
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Zero&nbsp;a&nbsp;block&nbsp;of&nbsp;memory&nbsp;using&nbsp;multiple&nbsp;threads.
&nbsp;void&nbsp;blockzero&nbsp;(unsigned&nbsp;char&nbsp;*p,&nbsp;int&nbsp;length)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;i;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_t&nbsp;threads[NTHREADS];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct&nbsp;bzeroparams&nbsp;params[NTHREADS];
&nbsp;
&nbsp;//&nbsp;For&nbsp;simplicity,&nbsp;assumes&nbsp;length&nbsp;is&nbsp;divisible&nbsp;by&nbsp;NTHREADS.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert((length&nbsp;%&nbsp;NTHREADS)&nbsp;==&nbsp;0);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;NTHREADS;&nbsp;i++)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;params[i].buffer&nbsp;=&nbsp;p&nbsp;+&nbsp;i&nbsp;*&nbsp;length/NTHREADS;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;params[i].length&nbsp;=&nbsp;length/NTHREADS;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_create_p(&amp;(threads[i]),&nbsp;&amp;go,&nbsp;&amp;params[i]);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;NTHREADS;&nbsp;i++)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_join(threads[i]);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.7: </B>Routine to zero a contiguous region of memory in parallel using multiple threads. To pass two arguments (the pointer to the buffer and the length of the buffer) to the child thread, the program passes a pointer to a struct holding the two parameters.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: Parallel block zero.</B> A simple example of fork-join parallelism in operating systems is the procedure to zero a contiguous block of memory. To prevent unintentional data leakage, whenever a process exits, the operating system must zero the memory that had been allocated to the exiting process. Otherwise, a new process may be re-assigned the memory, enabling it to read potentially sensitive data. For example, an operating system&#8217;s remote login program might temporarily store a user&#8217;s password in memory, but the next process to use the same physical memory might be a memory-scanning program launched by a different, malicious user. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For a large process, parallelizing the zeroing function can make sense. Zeroing 1 GB of memory takes about 50 milliseconds on a modern computer; by contrast, creating and starting a new thread takes a few tens of microseconds. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-180017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the code for a parallel zero function using fork-join parallelism. The multi-threaded blockzero creates a set of threads and assigns each a disjoint portion of the memory region; the region is empty when all threads have completed their work. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In practice, the operating system will often create a thread to run blockzero in the background. The memory of an exiting process does not need to be cleared until the memory is needed &#8212; that is, when the next process is created. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To exploit this flexibility, the operating system can create a set of low priority threads to run blockzero. The kernel can then return immediately and resume running application code. Later on, when the memory is needed, the kernel can call thread_join. If the zero is complete by that point, the join will return immediately; otherwise, it will wait until the memory is safe to use. </FONT><A id=x1-18002r30 name=x1-18002r30></A></P><A id=x1-190004 name=x1-190004>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.4 Thread Data Structures and Life Cycle</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">As we have seen, each thread represents a sequential stream of execution. The operating system provides the illusion that each thread runs on its own virtual processor by transparently suspending and resuming threads. For the illusion to work, the operating system must precisely save and restore the state of a thread. However, because threads run either in a process or in the kernel, there is also <EM>shared</EM> state that is not saved or restored when switching the processor between threads. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Thus, to understand how the operating system implements the thread abstraction, we must define both the per-thread state and the state that is shared among threads. Then we can describe a thread&#8217;s life cycle &#8212; how the operating system can create, start, stop, and delete threads to provide the abstraction. </FONT><A id=x1-190018 name=x1-190018></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00392.gif" data-calibre-src="OEBPS/Images/image00392.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.8: </B>A multi-threaded process or operating system kernel has both <EM>per-thread state</EM> and <EM>shared state</EM>. The thread control block stores the per-thread state: the current state of the thread&#8217;s computation (e.g., saved processor registers and a pointer to the stack) and metadata needed to manage the thread (e.g., the thread&#8217;s ID, scheduling priority, owner, and resource consumption). Shared state includes the program&#8217;s code, global static variables, and the heap.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-19002r34 name=x1-19002r34></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.4.1 </FONT><A id=x1-200001 name=x1-200001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Per-Thread State and Thread Control Block (TCB)</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">The operating system needs a data structure to represent a thread&#8217;s state; a thread is like any other object in this regard. This data structure is called the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:thread control block"}'>thread control block</A></EM> (TCB). For every thread the operating system creates, it creates one TCB. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The thread control block holds two types of per-thread information: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-20002x1 name=x1-20002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The state of the computation being performed by the thread. </FONT></P>
<LI class=enumerate><A id=x1-20004x2 name=x1-20004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Metadata about the thread that is used to manage the thread.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Per-thread Computation State.</B> To create multiple threads and to be able to start and stop each thread as needed, the operating system must allocate space in the TCB for the current state of each thread&#8217;s computation: a pointer to the thread&#8217;s stack and a copy of its processor registers. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Stack.</B> A thread&#8217;s stack is the same as the stack for a single-threaded computation &#8212; it stores information needed by the nested procedures the thread is currently running. For example, if a thread calls foo(), foo() calls bar(), and bar() calls bas(), then the stack would contain a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:stack frame"}'>stack frame</A></EM> for each of these three procedures; each stack frame contains the local variables used by the procedure, the parameters the procedure was called with, and the return address to jump to when the procedure completes. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because at any given time different threads can be in different states in their sequential computations &#8212; each can be in a different place in a different procedure called with different arguments from a different nesting of enclosing procedures &#8212; each thread needs its own stack. When a new thread is created, the operating system allocates it a new stack and stores a pointer to that stack in the thread&#8217;s TCB. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Copy of processor registers.</B> A processor&#8217;s registers include not only its general-purpose registers for storing intermediate values for ongoing computations, but they also include special-purpose registers, such as the instruction pointer and stack pointer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To be able to suspend a thread, run another thread, and later resume the original thread, the operating system needs a place to store a thread&#8217;s registers when that thread is not actively running. In some systems, the general-purpose registers for a stopped thread are stored on the top of the stack, and the TCB contains only a pointer to the stack. In other systems, the TCB contains space for a copy of all processor registers.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>How big a stack?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An implementation question for thread systems is: how large a stack should be allocated for each thread? A stack grows and shrinks as procedure calls are made and those calls return. The size of the stack must be large enough to accommodate the deepest nesting level needed during in the thread&#8217;s lifetime. With hundreds or thousands of threads, it can be wasteful to allocate more than the minimum needed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Most modern operating systems allocate kernel stacks in physical memory, putting space at a premium. However, the maximum procedure nesting depth in the kernel is usually small. Thus, kernels typically allocate a very small fixed sized region for each thread stack, e.g., 8 KB by default in Linux on an Intel x86. The kernel stays within this bound due to an important kernel coding convention: buffers and data structures are always allocated on the heap and never as procedure local variables. Although most programming languages allow arbitrary data structures to be defined as procedure local or &#8220;automatic&#8221; &#8212; allocated when a procedure starts and de-allocated when the procedure exits &#8212; that can cause problems when the stack is of limited size. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">User-level stacks are allocated in virtual memory and so there is less need for a tight space constraint. In a single threaded process, the stack is located at the top end of the address space, where it can grow nearly without bound. To catch program errors, most operating systems will trigger an error if the user program stack grows too large too quickly, as that is usually an indication of unbounded recursion, rather than something that was the programmer&#8217;s intent. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In a multi-threaded user application, it is not possible to have each stack grow without constraint. Although some programming languages, such as Google&#8217;s Go, will automatically grow the stack as needed, this is still uncommon. POSIX allows the default stack size to be library dependent (e.g., larger on a desktop machine, smaller on a smartphone). As one POSIX thread tutorial put it dryly, &#8220;Exceeding the default stack limit is often very easy to do, with the usual results: program termination and/or corrupted data.&#8221;&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Xbarney"}'><FONT style="BACKGROUND-COLOR: #7be1e1">10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. Most implementations try to detect when programs exceed the default stack limit by placing a known value at the very top and bottom of the stack to serve as a guard. The guard values can be checked on every context switch; if the value changes, it is likely the thread exceeded its stack. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To support application portability, the POSIX thread standard allows the user to redefine the default stack size to whatever is needed for the correct execution of a particular program. The thread library provided with the textbook sets the default stack size to 1 MB. This is almost certainly large enough provided you adopt the kernel approach of never putting large data objects on the stack. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Per-thread Metadata.</B> The TCB also includes <EM>per-thread metadata</EM> &#8212; information for managing the thread. For example, each thread might have a thread ID, scheduling priority, and status (e.g., whether the thread is waiting for an event or is ready to be placed onto a processor). </FONT><A id=x1-20005r38 name=x1-20005r38></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.4.2 </FONT><A id=x1-210002 name=x1-210002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Shared State</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">As opposed to per-thread state that is allocated for each thread, some state is <EM>shared</EM> between threads running in the same process or within the operating system kernel (Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-190018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">). In particular, program <EM>code</EM> is shared by all threads in a process, although each thread may be executing at a different place within that code. Additionally, statically allocated <EM>global variables</EM> and dynamically allocated <EM>heap variables</EM> can store information that is accessible to all threads. </FONT>
<P></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Other per-thread state: Thread-local variables</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In addition to the per-thread state that corresponds to execution state in the single-threaded case, some systems include additional <EM>thread-local variables</EM>. These variables are similar to global variables in that their scope spans different procedures, but they differ in that each thread has its own copy of these variables. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider these examples: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Errno.</B> In UNIX, the return value of system calls is intentionally kept simple. For example, the UNIX read system call returns either the number of bytes read (if successful) or -1 (if there was a problem). Often, an application needs additional information about the cause of the error (e.g., permission error, disk offline, etc.). To provide this, the kernel sets a variable in the application memory, the errno, with a diagnostic code for the most recent system call. As UNIX originally had only one thread per process, there was no confusion: the errno referred to the most recent system call of that process. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In a multi-threaded program, however, multiple threads can perform system calls concurrently. Rather than redefine the entire UNIX system call interface for a multi-threaded environment, errno is now a macro that maps to a thread-local variable containing the error code for that thread&#8217;s most recent system call. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Heap internals.</B> Although a program&#8217;s heap is logically shared &#8212; it is acceptable for one thread to allocate an object on the heap and then pass a pointer to that object to another thread &#8212; for performance reasons heaps may internally subdivide their space into per-thread regions. The advantage of subdividing the heap is that multiple threads can each allocate objects at the same time without interfering with one another. Further, by allocating objects used by the same thread from the same memory region, cache hit rates may improve. To implement these optimizations, each subdivision of the heap has thread-local variables that track what parts of the thread-local heap are in use, what parts are free, and so on. Then, the code that allocates new memory (e.g., malloc and new) is written to use these thread-local data structures and only take memory from the shared heap if the local heap is empty.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Thread-local variables are often useful, but, for simplicity, the rest of our discussion focuses only on the TCB, registers, and stack as the core pieces of per-thread state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: Although there is an important logical division between per-thread state and shared state, the operating system typically does not enforce this division. Nothing prevents one buggy thread from accessing another thread&#8217;s (conceptually private) per-thread state. Writing to a bad pointer in one thread can corrupt the stack of another. Or a careless programmer might pass a pointer to a local variable on one thread&#8217;s stack to another thread, giving the second thread a pointer to a stack location whose contents may change as the first thread calls and returns from various procedures. Or the first thread can exit after handing out a pointer to a variable on its stack; the heap will reassign that memory to an unrelated purpose. Because these bugs can depend on the specific interleavings of the threads&#8217; executions, they can be extremely hard to locate and correct. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To avoid unexpected behaviors, it is therefore important when writing multi-threaded programs to know which variables are designed to be shared across threads (global variables, objects on the heap) and which are designed to be private (local/automatic variables). </FONT><A id=x1-21001r36 name=x1-21001r36></A></P><A id=x1-220005 name=x1-220005>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.5 Thread Life Cycle</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">It is useful to consider the progression of states as a thread goes from being created, to being scheduled and de-scheduled onto and off of a processor, and then to exiting. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-220019"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the states of a thread during its lifetime. </FONT><A id=x1-220019 name=x1-220019></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00393.gif" data-calibre-src="OEBPS/Images/image00393.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.9: </B>The states of a thread during its lifetime.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>INIT.</B> Thread creation puts a thread into its INIT&nbsp;state and allocates and initializes per-thread data structures. Once that is done, thread creation code puts the thread into the READY&nbsp;state by adding the thread to the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:ready list"}'>ready list</A></EM>. The ready list is the set of runnable threads that are waiting their turn to use a processor. In practice, as discussed in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1070007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, the ready list is not in fact a &#8220;list&#8221;; the operating system typically uses a more sophisticated data structure to keep track of runnable threads, such as a priority queue. Nevertheless, following convention, we will continue to refer to it as the ready list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>READY.</B> A thread in the <EM>READY</EM> state is available to be run but is not currently running. Its TCB is on the ready list, and the values of its registers are stored in its TCB. At any time, the scheduler can cause a thread to transition from READY&nbsp;to RUNNING&nbsp;by copying its register values from its TCB to a processor&#8217;s registers. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RUNNING.</B> A thread in the <EM>RUNNING</EM> state is running on a processor. At this time, its register values are stored on the processor rather than in the TCB. A RUNNING&nbsp;thread can transition to the READY&nbsp;state in two ways: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The scheduler can preempt a running thread and move it to the READY&nbsp;state by: (1) saving the thread&#8217;s registers to its TCB and (2) switching the processor to run the next thread on the ready list. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A running thread can voluntarily relinquish the processor and go from RUNNING&nbsp;to READY&nbsp;by calling yield (e.g., thread_yield&nbsp;in the thread library). </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice that a thread can transition from READY&nbsp;to RUNNING&nbsp;and back many times. Since the operating system saves and restores the thread&#8217;s registers exactly, only the speed of the thread&#8217;s execution is affected by these transitions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: By convention in this book, a thread that is RUNNING&nbsp;is not on the ready list; the ready list is for READY&nbsp;and not RUNNING&nbsp;threads. However, some operating systems, such as Linux, use a different convention, where the RUNNING&nbsp;thread is whichever thread is at the front of the ready list. Either convention is equivalent as long as it used consistently. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WAITING.</B> A thread in the <EM>WAITING</EM> state is waiting for some event. Whereas the scheduler can move a thread in the READY&nbsp;state to the RUNNING&nbsp;state, a thread in the WAITING&nbsp;state cannot run until some action by another thread moves it from WAITING&nbsp;to READY. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The threadHello program in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-170016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> provides an example of a WAITING&nbsp;thread. After creating its children threads, the main thread must wait for them to complete, by calling thread_join&nbsp;once for each child. If the specific child thread is not yet done at the time of the join, the main thread goes from RUNNING&nbsp;to WAITING&nbsp;until the child thread exits. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While a thread waits for an event, it cannot make progress; therefore, it is not useful to run it. Rather than continuing to run the thread or storing the TCB on the scheduler&#8217;s ready list, the TCB is stored on the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:waiting list"}'>waiting list</A></EM> of some <EM>synchronization variable</EM> associated with the event. When the required event occurs, the operating system moves the TCB from the synchronization variable&#8217;s waiting list to the scheduler&#8217;s ready list, transitioning the thread from WAITING&nbsp;to READY. We describe synchronization variables in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>FINISHED.</B> A thread in the FINISHED&nbsp;state never runs again. The system can free some or all of its state for other uses, though it may keep some remnants of the thread in the FINISHED&nbsp;state for a time by putting the TCB on a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:finished list"}'>finished list</A></EM>. For example, the thread_exit&nbsp;call lets a thread pass its exit value to its parent thread via thread_join. Eventually, when a thread&#8217;s state is no longer needed (e.g., after its exit value has been read by the join call), the system can delete and reclaim the thread&#8217;s state. </FONT><A id=x1-2200210 name=x1-2200210></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>State of Thread</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Location of Thread Control Block (TCB)</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Location of Registers</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">INIT </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Being Created </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">TCB </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">READY </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Ready List </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">TCB </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">RUNNING </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Running List </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Processor </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">WAITING </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Synchronization Variable&#8217;s Waiting List </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">TCB </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">FINISHED </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Finished List then Deleted </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">TCB or Deleted </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.10: </B>Location of thread&#8217;s per-thread state for different life cycle stages.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One way to understand these states is to consider where a thread&#8217;s TCB and registers are stored, as shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2200210"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. For example, all threads in the READY&nbsp;state have their TCBs on the ready list and their registers in the TCB. All threads in the RUNNING&nbsp;state have their TCBs on the running list and their register values in hardware registers. And all threads in the WAITING&nbsp;state have their TCBs on various synchronization variables&#8217; waiting lists. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>The idle thread</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If a system has k processors, most operating systems ensure that there are exactly k RUNNING&nbsp;threads, by keeping a low priority <EM>idle thread</EM> per processor for when there is nothing else to run. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On old machines, the idle thread would spin in a tight loop doing nothing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Today, the idle thread still spins in a loop, but to save power, on each iteration it puts the processor into a low-power sleep mode. In sleep mode, the processor stops executing instructions until a hardware interrupt occurs. Then, the processor wakes up and handles the interrupt in the normal way &#8212; saving the state of the currently running thread (the idle thread) and running the handler. After running the handler, a thread waiting for that I/O event may now be READY. If so, the scheduler runs that thread next; otherwise, the idle thread resumes execution, putting the processor to sleep again. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Having a low-power idle thread also helps when running the operating system inside a virtual machine. Obviously, it would be inefficient for an idle operating system to consume processing cycles that could be better used by another virtual machine on the same system. Putting the processor into sleep mode is a privileged instruction, so if the operating system is running inside a virtual machine, the hardware will trap to the host kernel. The host kernel can then switch to a different virtual machine. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>For the threadHello program in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-170016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, when thread_join&nbsp;returns for thread i, what is thread i&#8217;s thread state? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>When join returns, thread i has finished running and exited. The runtime system saved the exit value in the TCB and moved the TCB to the finished list (so that its exit value can be found by the parent thread). <B>The thread is thus in the FINISHED&nbsp;state.</B> &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>For the threadHello program, what is the minimum and maximum number of times that the main thread enters the READY&nbsp;state on a <EM>uniprocessor</EM>? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The main thread must go into the READY&nbsp;state when it is first created; otherwise, it would never be scheduled. On a uniprocessor, it must also give up the processor (e.g., due to a time slice or in thread_join) in order for its children threads to run. The children threads could then completely run before the main thread is re-scheduled. Once the children have finished, the main thread can run to completion. Thus, <B>the minimum number of times is two.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The maximum number of times is (near) infinite.</B> A running thread can be preempted and re-scheduled many times, without affecting the correctness of the execution. In the limit, the thread could conceivably be preempted after each instruction! &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Where is my TCB?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A remarkably tricky implementation question is how to find the current thread&#8217;s TCB. The thread library needs access to the current TCB for a number of reasons, e.g., to change its priority or to access thread-local variables. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might think finding the TCB would be simple: just store a pointer to the TCB in a global variable. However, recall that every thread running in the same process uses exactly the same code, and therefore each thread would look in exactly the same place for the TCB. On a uniprocessor, this works: the global variable can hold the value of the current TCB, and the library can change the value whenever it switches between threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This does not work on a multiprocessor, however. Some systems, such as the Intel x86, have hardware support for fetching the ID of the current processor. In these systems, the thread library can maintain a global array of pointers, with the i&#8217;th entry pointing to the TCB of the thread running on the i&#8217;th processor. A running thread can then find its TCB by looking up its processor ID and then finding the corresponding entry in the array. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For systems without this feature, however, there is another approach: the stack pointer is always unique to each thread. The thread library can store a pointer to the thread TCB at the very bottom of the stack, underneath the procedure frames. (Some systems take this one step farther, and put the entire TCB at the bottom of the stack.) As long as thread stacks are aligned to start at a fixed block boundary, the low order bits of the current stack pointer can be masked to locate the pointer to the current TCB. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-22003r40 name=x1-22003r40></A><A id=x1-230006 name=x1-230006>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.6 Implementing Kernel Threads</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">So far, we have described the basic data structures and operation of threads. We now describe how to implement them. The specifics of the implementation vary depending on the context: </FONT><A id=x1-2300111 name=x1-2300111></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00394.gif" data-calibre-src="OEBPS/Images/image00394.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.11: </B>A multi-threaded kernel with three kernel threads and two single-threaded user-level processes. Each kernel thread has its own TCB and its own stack. Each user process has a stack at user-level for executing user code and a kernel interrupt stack for executing interrupts and system calls.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-2300212 name=x1-2300212></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00395.gif" data-calibre-src="OEBPS/Images/image00395.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.12: </B>A multi-threaded kernel with three kernel threads and two user-level processes, each with two threads. Each user-level thread has a user-level stack and an interrupt stack in the kernel for executing interrupts and system calls.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Kernel threads.</B> The simplest case is implementing threads inside the operating system kernel, sharing one or more physical processors. A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:kernel thread"}'>kernel thread</A></EM> executes kernel code and modifies kernel data structures. Almost all commercial operating systems today support kernel threads. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Kernel threads and single-threaded processes.</B> An operating system with kernel threads might also run some single-threaded user processes. As shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2300111"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.11</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, these processes can invoke system calls that run concurrently with kernel threads inside the kernel. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multi-threaded processes using kernel threads.</B> Most operating systems provide a set of library routines and system calls to allow applications to use multiple threads within a single user-level process. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2300212"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this case. These threads execute user code and access user-level data structures. They also make system calls into the operating system kernel. For that, they need a kernel interrupt stack just like a normal single-threaded process. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>User-level threads.</B> To avoid having to make a system call for every thread operation, some systems support a model where user-level thread operations &#8212; create, yield, join, exit, and the synchronization routines described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> &#8212; are implemented entirely in a user-level library, without invoking the kernel.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We first describe the implementation for the baseline case of kernel threads. In Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-280008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, we explain how to extend the model to support application multi-threading implemented with kernel threads or with a user-level library. </FONT><A id=x1-23003r39 name=x1-23003r39></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.6.1 </FONT><A id=x1-240001 name=x1-240001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Creating a Thread</FONT></H4><A id=x1-2400113 name=x1-2400113></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;func&nbsp;is&nbsp;a&nbsp;pointer&nbsp;to&nbsp;a&nbsp;procedure&nbsp;the&nbsp;thread&nbsp;will&nbsp;run.
&nbsp;//&nbsp;arg&nbsp;is&nbsp;the&nbsp;argument&nbsp;to&nbsp;be&nbsp;passed&nbsp;to&nbsp;that&nbsp;procedure.
&nbsp;void
&nbsp;thread_create(thread_t&nbsp;*thread,&nbsp;void&nbsp;(*func)(int),&nbsp;int&nbsp;arg)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Allocate&nbsp;TCB&nbsp;and&nbsp;stack
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCB&nbsp;*tcb&nbsp;=&nbsp;new&nbsp;TCB();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread-&gt;tcb&nbsp;=&nbsp;tcb;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;stack_size&nbsp;=&nbsp;INITIAL_STACK_SIZE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;stack&nbsp;=&nbsp;new&nbsp;Stack(INITIAL_STACK_SIZE);
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Initialize&nbsp;registers&nbsp;so&nbsp;that&nbsp;when&nbsp;thread&nbsp;is&nbsp;resumed,&nbsp;it&nbsp;will&nbsp;start&nbsp;running&nbsp;at
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;stub.&nbsp;&nbsp;The&nbsp;stack&nbsp;starts&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;the&nbsp;allocated&nbsp;region&nbsp;and&nbsp;grows&nbsp;down.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;sp&nbsp;=&nbsp;tcb-&gt;stack&nbsp;+&nbsp;INITIAL_STACK_SIZE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;pc&nbsp;=&nbsp;stub;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Create&nbsp;a&nbsp;stack&nbsp;frame&nbsp;by&nbsp;pushing&nbsp;stub&#8217;s&nbsp;arguments&nbsp;and&nbsp;start&nbsp;address
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;onto&nbsp;the&nbsp;stack:&nbsp;func,&nbsp;arg
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*(tcb-&gt;sp)&nbsp;=&nbsp;arg;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;sp--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*(tcb-&gt;sp)&nbsp;=&nbsp;func;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;sp--;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Create&nbsp;another&nbsp;stack&nbsp;frame&nbsp;so&nbsp;that&nbsp;thread_switch&nbsp;works&nbsp;correctly.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;This&nbsp;routine&nbsp;is&nbsp;explained&nbsp;later&nbsp;in&nbsp;the&nbsp;chapter.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_dummySwitchFrame(tcb);
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;state&nbsp;=&nbsp;READY;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readyList.add(tcb);&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Put&nbsp;tcb&nbsp;on&nbsp;ready&nbsp;list
&nbsp;}
&nbsp;
&nbsp;void
&nbsp;stub(void&nbsp;(*func)(int),&nbsp;int&nbsp;arg)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(*func)(arg);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Execute&nbsp;the&nbsp;function&nbsp;func()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_exit(0);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;If&nbsp;func()&nbsp;does&nbsp;not&nbsp;call&nbsp;exit,&nbsp;&nbsp;call&nbsp;it&nbsp;here.
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.13: </B>Pseudo-code for thread creation. The specifics of initializing the stack and the conventions for passing arguments to the initial function are machine-dependent. On the Intel x86 architecture, the stack starts at high addresses and grows down, while arguments are passed on the stack. On other systems, the stack can grow upwards and/or arguments can be passed in registers. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2600114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> provides pseudo-code for thread_dummySwitchFrame.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2400113"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the pseudo-code to allocate a new thread. The goal of thread_create&nbsp;is to perform an asynchronous procedure call to func with arg as the argument to that procedure. When the thread runs, it will execute func(arg) concurrently with the calling thread. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are three steps to creating a thread: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-24003x1 name=x1-24003x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Allocate per-thread state.</B> The first step in the thread constructor is to allocate space for the thread&#8217;s per-thread state: the TCB and stack. As we have mentioned, the TCB is the data structure the thread system uses to manage the thread. The stack is an area of memory for storing data about in-progress procedures; it is allocated in memory like any other data structure. </FONT></P>
<LI class=enumerate><A id=x1-24005x2 name=x1-24005x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Initialize per-thread state.</B> To initialize the TCB, the thread constructor sets the new thread&#8217;s registers to what they need to be when the thread starts RUNNING. When the thread is assigned a processor, we want it to start running func(arg). However, instead of having the thread start in func, the constructor starts the thread in a dummy function, stub, which in turn calls func. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We need this extra step in case the func procedure returns instead of calling thread_exit. Without the stub, func would return to whatever random location is stored at the top of the stack! Instead, func returns to stub and stub calls thread_exit&nbsp;to finish the thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To start at the beginning of stub, the thread constructor sets up the stack as if stub was just called by normal code; the specifics will depend on the calling convention of the machine. In the pseudo-code, we push stub&#8217;s two arguments onto the stack: func and arg. When the thread starts running, the code in stub will access its arguments just like a normal procedure. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In addition, we also push a dummy stack frame for thread_switch&nbsp;onto the stack; we defer an explanation of this detail until we discuss the implementation of thread switching. </FONT></P>
<LI class=enumerate><A id=x1-24007x3 name=x1-24007x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Put TCB on ready list.</B> The last step in creating a thread is to set its state to READY&nbsp;and put the new TCB on the ready list, enabling the thread to be scheduled.</FONT></P></LI></OL><A id=x1-24008r46 name=x1-24008r46></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.6.2 </FONT><A id=x1-250002 name=x1-250002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Deleting a Thread</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">When a thread calls thread_exit, there are two steps to deleting the thread: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Remove the thread from the ready list so that it will never run again. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Free the per-thread state allocated for the thread.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although this seems easy, there is an important subtlety: if a thread removes itself from the ready list and frees its own per-thread state, then the program may break. For example, if a thread removes itself from the ready list but an interrupt occurs before the thread finishes de-allocating its state, there is a memory leak: that thread will never resume to de-allocate its state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Worse, suppose that a thread frees its own state? Can the thread finish running the code in thread_exit&nbsp;if it does not have a stack? What happens if an interrupt occurs just after the running thread&#8217;s stack has been de-allocated? If the context switch code tries to save the current thread&#8217;s state, it will be writing to de-allocated memory, possibly to storage that another processor has re-allocated for some other data structure. The result could be corrupted memory, where the specific behavior depends on the precise sequence of events. Needless to say, such a bug would be very difficult to locate. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, there is a simple fix: a thread never deletes its own state. Instead, some other thread must do it. On exit, the thread transitions to the FINISHED&nbsp;state, moves its TCB from the ready list to a list of <EM>finished</EM> threads the scheduler should never run. The thread can then safely switch to the next thread on the ready list. Once the finished thread is no longer running, it is safe for some <EM>other</EM> thread to free the state of the thread. </FONT><A id=x1-25001r48 name=x1-25001r48></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.6.3 </FONT><A id=x1-260003 name=x1-260003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Context Switch</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">To support multiple threads, we also need a mechanism to switch which threads are RUNNING&nbsp;and which are READY. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:thread context switch"}'>thread context switch</A></EM> suspends execution of a currently running thread and resumes execution of some other thread. The switch saves the currently running thread&#8217;s registers to the thread&#8217;s TCB and stack, and then it restores the new thread&#8217;s registers from that thread&#8217;s TCB and stack into the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We need to answer several questions: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">What triggers a context switch? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How does a voluntary context switch (e.g., a call to thread_yield) work? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How does an involuntary context switch differ from a voluntary one? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">What thread should the scheduler choose to run next?</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We discuss these in turn, but we defer the last question to Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1070007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. The <EM>mechanisms</EM> we discuss in this Chapter work regardless of the <EM>policy</EM> the scheduler uses when choosing threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Separating mechanism from policy</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Separating mechanism from policy is a useful and widely applied principle in operating system design. When mechanism and policy are cleanly separated, it is easier to introduce new policies to optimize a system for a new workload or new technology. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, the thread context switch abstraction cleanly separates mechanism (how to switch between threads) from policy (which thread to run) so that the mechanism works no matter what policy is used. Some systems can elect to do something simple (e.g., FIFO scheduling); other systems can optimize scheduling to meet their goals (e.g., a periodic scheduler to smoothly run real-time multimedia streams for a media device, a round-robin scheduler to balance responsiveness and throughput for a server, or a priority scheduler that devotes most resources to the visible application on a smartphone). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We will see this principle many times in this book. For example, thread synchronization mechanisms work regardless of the scheduling policy; file metadata mechanisms for locating a file&#8217;s blocks work regardless of the policy for where to place the file&#8217;s blocks on disk; and page translation mechanisms for mapping virtual to physical addresses work regardless of which physical pages the operating system assigns to each process. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>What Triggers a Kernel Thread Context Switch?</B> A thread context switch can be triggered by either a voluntary call into the thread library, or an involuntary interrupt or processor exception. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Voluntary.</B> The thread could call a thread library function that triggers a context switch. For example, most thread libraries provide a thread_yield&nbsp;call that lets the currently running thread voluntarily give up the processor to the next thread on the ready list. Similarly, the thread_join&nbsp;and thread_exit&nbsp;calls suspend execution of the current thread and start running a different one. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Involuntary.</B> An <EM>interrupt</EM> or <EM>processor exception</EM> could invoke an interrupt handler. The interrupt hardware saves the state of the running thread and executes the handler&#8217;s code. The handler can decide that some other thread should run, and then switch to it. Alternatively, if the current thread should continue running, the handler restores the state of the interrupted thread and resumes execution. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, many thread systems are designed to ensure that no thread can monopolize the processor. To accomplish this, they set a hardware timer to interrupt the processor periodically (e.g., every few milliseconds). The timer interrupt handler saves the state of the running thread, chooses another thread to run, and runs that thread by restoring its state to the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Other I/O hardware events (e.g., a keyboard key is pressed, a network packet arrives, or a disk operation completes) also invoke interrupt handlers. In these cases as well, the handlers save the state of the currently running thread so that it can be restored later. They then execute the handler code, and when the handler is done, they either restore the state of the current thread, or switch to a new thread. A new thread will be run if the I/O event moves a thread onto the ready list with a higher priority than the previously running thread.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Regardless, the thread system must save the current processor state, so that when the current thread resumes execution, it appears <EM>to the thread</EM> as if the event never occurred except for some time having elapsed. This provides the abstraction of thread execution on a virtual processor with unpredictable and variable speed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To keep things simple, we do not want to do an involuntary context switch while we are in the middle of a voluntary one. When switching between two threads, we need to temporarily defer interrupts until the switch is complete, to avoid confusion. Processors contain privileged instructions to defer and re-enable interrupts; we make use of these in our implementation below. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Why is it necessary to turn off interrupts during thread switch?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our implementation of thread_yield&nbsp;defers any interrupts that might occur during the procedure, until the yield is complete. This might seem unnecessary: after all, even if the thread context switch is interrupted, the state of the switch will be saved onto the stack. Eventually the kernel will re-schedule the thread, restore its state, and complete the thread switch. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, a subtle inconsistency might arise. Suppose a low priority thread (e.g., the idle thread) is about to voluntarily switch to a high priority thread. It pulls the high priority thread off the ready list, and at that precise moment, an interrupt occurs. Supppose the interrupt moves a medium priority thread from WAITING&nbsp;to READY. Since it appears that the processor is still running the low priority thread, the interrupt handler immediately switches to the new thread. The high priority thread is in limbo! It is ready to run, but unable to do so until the low priority thread is re-scheduled. And that may not happen for a long time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, this sequence of events would not occur very often, but when it does, it would be difficult to locate or debug. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Voluntary Kernel Thread Context Switch.</B> Because a voluntary switch is simpler to understand, we start there. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2600114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows pseudo-code for a simple implementation of thread_yield&nbsp;for the Intel x86 hardware architecture. A thread calls thread_yield&nbsp;to voluntarily relinquish the processor to another thread. The calling thread&#8217;s registers are copied to its TCB and stack, and it resumes running later, when the scheduler chooses it. </FONT><A id=x1-2600114 name=x1-2600114></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;We&nbsp;enter&nbsp;as&nbsp;oldThread,&nbsp;but&nbsp;we&nbsp;return&nbsp;as&nbsp;newThread.
&nbsp;//&nbsp;Returns&nbsp;with&nbsp;newThread&#8217;s&nbsp;registers&nbsp;and&nbsp;stack.
&nbsp;void&nbsp;thread_switch(oldThreadTCB,&nbsp;newThreadTCB)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pushad;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Push&nbsp;general&nbsp;register&nbsp;values&nbsp;onto&nbsp;the&nbsp;old&nbsp;stack.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oldThreadTCB-&gt;sp&nbsp;=&nbsp;%esp;&nbsp;//&nbsp;Save&nbsp;the&nbsp;old&nbsp;thread&#8217;s&nbsp;stack&nbsp;pointer.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;%esp&nbsp;=&nbsp;newThreadTCB-&gt;sp;&nbsp;//&nbsp;Switch&nbsp;to&nbsp;the&nbsp;new&nbsp;stack.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;popad;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Pop&nbsp;register&nbsp;values&nbsp;from&nbsp;the&nbsp;new&nbsp;stack.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return;
&nbsp;}
&nbsp;
&nbsp;void&nbsp;thread_yield()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCB&nbsp;*chosenTCB,&nbsp;*finishedTCB;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Prevent&nbsp;an&nbsp;interrupt&nbsp;from&nbsp;stopping&nbsp;us&nbsp;in&nbsp;the&nbsp;middle&nbsp;of&nbsp;a&nbsp;switch.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disableInterrupts();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Choose&nbsp;another&nbsp;TCB&nbsp;from&nbsp;the&nbsp;ready&nbsp;list.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chosenTCB&nbsp;=&nbsp;readyList.getNextThread();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(chosenTCB&nbsp;==&nbsp;NULL)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Nothing&nbsp;else&nbsp;to&nbsp;run,&nbsp;so&nbsp;go&nbsp;back&nbsp;to&nbsp;running&nbsp;the&nbsp;original&nbsp;thread.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Move&nbsp;running&nbsp;thread&nbsp;onto&nbsp;the&nbsp;ready&nbsp;list.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runningThread-&gt;state&nbsp;=&nbsp;ready;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readyList.add(runningThread);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_switch(runningThread,&nbsp;chosenTCB);&nbsp;//&nbsp;Switch&nbsp;to&nbsp;the&nbsp;new&nbsp;thread.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runningThread-&gt;state&nbsp;=&nbsp;running;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Delete&nbsp;any&nbsp;threads&nbsp;on&nbsp;the&nbsp;finished&nbsp;list.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;((finishedTCB&nbsp;=&nbsp;finishedList-&gt;getNextThread())&nbsp;!=&nbsp;NULL)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;delete&nbsp;finishedTCB-&gt;stack;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;delete&nbsp;finishedTCB;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enableInterrupts();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;thread_create&nbsp;must&nbsp;put&nbsp;a&nbsp;dummy&nbsp;frame&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;its&nbsp;stack:
&nbsp;//&nbsp;the&nbsp;return&nbsp;PC&nbsp;and&nbsp;space&nbsp;for&nbsp;pushad&nbsp;to&nbsp;have&nbsp;stored&nbsp;a&nbsp;copy&nbsp;of&nbsp;the&nbsp;registers.
&nbsp;//&nbsp;This&nbsp;way,&nbsp;when&nbsp;someone&nbsp;switches&nbsp;to&nbsp;a&nbsp;newly&nbsp;created&nbsp;thread,
&nbsp;//&nbsp;the&nbsp;last&nbsp;two&nbsp;lines&nbsp;of&nbsp;thread_switch&nbsp;work&nbsp;correctly.
&nbsp;void&nbsp;thread_dummySwitchFrame(newThread)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*(tcb-&gt;sp)&nbsp;=&nbsp;stub;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Return&nbsp;to&nbsp;the&nbsp;beginning&nbsp;of&nbsp;stub.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;sp--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tcb-&gt;sp&nbsp;-=&nbsp;SizeOfPopad;
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.14: </B>Pseudo-code for thread_switch&nbsp;and thread_yield&nbsp;on the Intel x86 architecture. Note that thread_yield&nbsp;is a no-op if there are no other threads to run. Otherwise, it saves the old thread state and restores the new thread state. When the old thread is re-scheduled, it returns from thread_switch&nbsp;as the running thread.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The pseudo-code for thread_yield&nbsp;first turns off interrupts to prevent the thread system from attempting to make two context switches at the same time. The pseudo-code then pulls the next thread to run off the ready list (if any), and switches to it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The thread_switch&nbsp;code may seem tricky, since it is called in the context of the old thread and finishes in the context of the new thread. To make this work, thread_switch&nbsp;saves the state of the registers to the stack and saves the stack pointer to the TCB. It then switches to the stack of the new thread, restores the new thread&#8217;s state from the new thread&#8217;s stack, and returns to whatever program counter is stored on the new stack. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A twist is that the return location may not be to thread_yield! The return is to whatever the new thread was doing beforehand. For example, the new thread might have been WAITING&nbsp;in thread_join&nbsp;and is now READY&nbsp;to run. The thread might have called thread_yield. Or it might be a newly created thread just starting to run. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is essential that any routine that causes the thread to yield or block call thread_switch&nbsp;in the same way. Equally, to create a new thread, thread_create&nbsp;must set up the stack of the new thread to be as if it had suspended execution just before performing its first instruction. Then, if the newly created thread is the next thread to run, a thread can call thread_yield, switch to the newly created thread, switch to its stack pointer, pop the register values off the stack, and &#8220;return&#8221; to the new thread, even though it had never called switch in the first place. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose two threads each loop, calling thread_yield&nbsp;on each iteration. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;go()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while(1)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_yield();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">What is the sequence of steps as seen by the physical processor and by each thread? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>From the processor&#8217;s point of view, one instruction follows the next, but now the instructions from different threads are interleaved (as they must be if they are multiplexed). </FONT></P>
<P><B><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2600215"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.15</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the interleaving:</FONT></B><FONT style="BACKGROUND-COLOR: #7be1e1"> thread_yield&nbsp;is called by one thread but returns in a different thread. thread_yield&nbsp;deliberately violates the procedure call conventions compilers normally follow by manipulating the stack and program counter to switch between threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, the threads themselves can ignore this complexity. From their point of view, they each run this loop on their own (variable-speed) virtual processor. &#9633; </FONT><A id=x1-2600215 name=x1-2600215></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td colSpan=3 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Logical View</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread 1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread 2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">go(){ </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">go(){ </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; while(1){ </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; while(1){ </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; thread_yield(); </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; thread_yield(); </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; } </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; } </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">} </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">} </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR>
<TR class=tr>
<TD class=td colSpan=3 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Physical Reality</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><B><FONT style="BACKGROUND-COLOR: #7be1e1">Thread 1&#8217;s instructions </FONT></B></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread 2&#8217;s instructions</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Processor&#8217;s instructions</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;return&#8221; from thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;return&#8221; from thread_switch</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; into stub </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; into stub </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call go </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call go</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 1 state to TCB </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 1 state to TCB </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 2 state </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 2 state</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;return&#8221; from thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;return&#8221; from thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; into stub </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; into stub </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call go </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call go </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 2 state to TCB </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 2 state to TCB </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 1 state </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 1 state </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_yield </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_yield </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 1 state to TCB </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 1 state to TCB </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 2 state </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 2 state </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_yield </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_yield </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_yield </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">choose another thread </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">call thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 2 state to TCB </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">save thread 2 state to TCB </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 1 state </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load thread 1 state </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_switch </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_switch </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_yield </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">return from thread_yield </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">... </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">... </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">... </FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.15: </B>Interleaving of instructions when two threads loop and call thread_yield().</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>A zero-thread kernel</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Not only can we have a single-threaded kernel or a multi-threaded kernel, it is actually possible to have a kernel with no threads of its own &#8212; a zero-threaded kernel! In fact, this used to be quite common&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XLions:1996:LCU"}'><FONT style="BACKGROUND-COLOR: #7be1e1">107</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the simple picture of the operating system described in Chapter&nbsp;2. Once the system has booted, initialized its device drivers, and started some user-level processes like a login shell, everything else the kernel does is event-driven, i.e., done in response to an interrupt, processor exception, or system call. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In a simple operating system like this, there is no need for a &#8220;kernel thread&#8221; or &#8220;kernel thread control block&#8221; to keep track of an ongoing computation. Instead, when an interrupt, trap, or exception occurs, the stack pointer gets set to the base of the interrupt stack, and the instruction pointer gets set to the address of the handler. Then, the handler executes and either returns immediately to the interrupted user-level process or suspends the user-level process and &#8220;returns&#8221; to some other user-level process. In either case, the next event (interrupt, processor exception, or system call) starts this process anew. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Involuntary Kernel Thread Context Switch.</B> Chapter&nbsp;2 explained what happens when an interrupt, exception, or trap interrupts a running user-level process: hardware and software work together to save the state of the interrupted process, run the kernel&#8217;s handler, and restore the state of the interrupted process. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The mechanism is almost identical when an interrupt or trap triggers a thread switch between threads in the kernel. The three steps described in Chapter&nbsp;2 are slightly modified (<EM>changes are written in italics</EM>): </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-26004x1 name=x1-26004x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Save the state.</B> Save the currently running <EM>thread&#8217;s</EM> registers so that the handler can run code without disrupting the interrupted <EM>thread</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Hardware saves some state when the interrupt or exception occurs, and software saves the rest of the state when the handler runs. </FONT></P>
<LI class=enumerate><A id=x1-26006x2 name=x1-26006x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Run the kernel&#8217;s handler.</B> Run the kernel&#8217;s handler code to handle the interrupt or exception. <EM>Since we are already in kernel mode, we do not need to change from user to kernel mode in this step.</EM> <EM>We also do not need to change the stack pointer to the base of the kernel&#8217;s interrupt stack. Instead, we can just push saved state or handler variables onto the current stack, starting from the current stack pointer.</EM> </FONT></P>
<LI class=enumerate><A id=x1-26008x3 name=x1-26008x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Restore the state.</B> Restore the <EM>next ready thread&#8217;s</EM> registers so that the thread can resume running where it left off. </FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In short, comparing a switch between kernel threads to what happens on a user-mode transfer: (1) there is no need to switch modes (and therefore no need to switch stacks) and (2) the handler can resume any thread on the ready list rather than always resuming the thread or process that was just suspended. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Implementation Details.</B> On most processor architectures, a simple (but inefficient) way to swap to the next thread from within an interrupt handler is to call thread_switch&nbsp;just before the handler returns. As we have already seen, thread_switch&nbsp;saves the state of the current thread (that is, the state of the interrupt handler) and switches to the new kernel thread. When the original thread resumes, it will return from thread_switch, and immediately pop the interrupt context off the stack, resuming execution at the point where it was interrupted. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Most systems, such as Linux, make a small optimization to improve interrupt handling performance. The state of the interrupted thread is already saved on the stack, albeit in the format specified by the interrupt hardware. If we modify thread_switch&nbsp;to save and restore registers exactly as the interrupt hardware does, then returning from an interrupt and resuming a thread are the same action: they both pop the interrupt frame off the stack to resume the next thread to run. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, to be compatible with x86 interrupt hardware, the software implementation of thread_switch&nbsp;would simulate the hardware case, saving the return instruction pointer and eflags register before calling pushad to save the general-purpose registers. After switching to the new stack, it would call iret to resume the new thread, whether the new thread was suspended by a hardware event or a software call. </FONT><A id=x1-26009r43 name=x1-26009r43></A></P><A id=x1-270007 name=x1-270007>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.7 Combining Kernel Threads and Single-Threaded User Processes</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Previously, Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2300111"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.11</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrated a system with both kernel threads and single-threaded user processes. A process is a sequential execution of instructions, so each user-level process includes the process&#8217;s thread. However, a process is more than just a thread because it has its own address space. Process 1 has its own view of memory, its own code, its own heap, and its own global variables that differ from those of process 2 (and from the kernel&#8217;s). </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because a process contains more than just a thread, each process&#8217;s process control block (PCB) needs more information than a thread control block (TCB) for a kernel thread. Like a TCB, a PCB for a single-threaded process must store the processor registers when the process&#8217;s thread is not running. In addition, the PCB has information about the process&#8217;s address space; when a context switch occurs from one process to another, the operating system must change the virtual memory mappings as well as the register state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since the PCB and TCB each represent one thread, the kernel&#8217;s ready list can contain a mix of PCBs for processes and TCBs for kernel threads. When the scheduler chooses the next thread to run, it can pick either kind. A thread switch is nearly identical whether switching between kernel threads or switching between a process&#8217;s thread and a kernel thread. In both cases, the switch saves the state of the currently running thread and restores the state of the next thread to run. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As we mentioned in Chapter&nbsp;2, most operating systems dedicate a kernel interrupt stack for each process. This way, when the process needs to perform a system call, or on an interrupt or processor exception, the hardware traps to the kernel, saves the user-level processor state, and starts running at a specific handler in the kernel. Once inside the kernel, the process thread behaves exactly like a kernel thread &#8212; it can create threads (or other processes), block (e.g., in UNIX process wait or on I/O), and even exit. While inside the kernel, the process can be pre-empted by a timer interrupt or I/O event, and a higher priority process or kernel thread can run in its place. The PCB and kernel stack for the preempted process stores both its current kernel state, as well as the user-level state saved when the process initiated the system call. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can resume a process in the kernel using thread_switch. However, when we resume execution of the user-level process after the completion of a system call or interrupt, we must restore its state precisely as it was beforehand: with the correct value in its registers, executing in user mode, with the appropriate virtual memory mappings, and so forth. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An important detail is that many processor architectures have extra co-processor state, e.g., floating point registers, for user-level code. Typically, the operating system kernel does not make use of floating point operations. Therefore, the kernel does not need to save those registers when switching between kernel threads, but it does save and restore them when switching between processes. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>One small difference</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You may notice that a mode switch in Chapter&nbsp;2 caused the x86 hardware to save not just the instruction pointer and eflags register but also the <EM>stack pointer</EM> of the interrupted process before starting the handler. For mode switching, the hardware changes the stack pointer to the kernel&#8217;s interrupt stack, so it must save the original user-level stack pointer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In contrast, when switching from a kernel thread to a kernel handler, the hardware does not switch stacks. Instead, the handler runs on the current stack, not on a separate interrupt stack. Therefore, the hardware does not need to save the original stack pointer; the handler just saves the stack pointer with the other registers as part of the pushad instruction. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Thus, x86 hardware works slightly differently when switching between a kernel thread and a kernel handler than when doing a mode switch: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Entering the handler.</B> When an interrupt or exception occurs, if the processor detects that it is already in kernel mode (by inspecting the eflags register), it just pushes the instruction pointer and eflags registers (but not the stack pointer) onto the existing stack. On the other hand, if the hardware detects that it is switching from user-mode to kernel-mode, then the processor also changes the stack pointer to the base of the interrupt stack and pushes the original stack pointer along with the instruction pointer and eflags registers onto the new stack. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Returning from the handler.</B> When the iret instruction is called, it inspects both the current eflags register and the value on the stack that it will use to restore the earlier eflags register. If the mode bit is identical, then iret just pops the instruction pointer and eflags register and continues to use the current stack. On the other hand, if the mode bit differs, then the iret instruction pops not only the instruction pointer and eflags register, but also the saved stack pointer, thus switching the processor&#8217;s stack pointer to the saved one. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-27001r52 name=x1-27001r52></A><A id=x1-280008 name=x1-280008>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.8 Implementing Multi-Threaded Processes</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">So far, we have described how to implement multiple threads that run inside the operating system kernel. Of course, we also want to be able to run user programs as well. Since many user programs are single-threaded, we start with the simple case of how to integrate kernel threads and single-threaded processes. We then turn to various ways of implementing <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multi-threaded process"}'>multi-threaded processes</A></EM>, processes with multiple threads. All widely used modern operating systems support both kernel threads and multi-threaded processes. Both programming languages, such as Java, and standard library interfaces such as POSIX and simple threads, use this operating system support to provide the thread abstraction to the programmer. </FONT><A id=x1-28001r49 name=x1-28001r49></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.8.1 </FONT><A id=x1-290001 name=x1-290001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Multi-Threaded Processes Using Kernel Threads</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">The simplest way to support multiple threads per process is to use the kernel thread implementation we have already described. When a kernel thread creates, deletes, suspends, or resumes a thread, it can use a simple procedure call. When a user-level thread accesses the thread library to do the same things, it uses a system call to ask the kernel to do the operation on its behalf. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As shown earlier in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-2300212"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, a thread in a process has: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A user-level stack for executing user code. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A kernel interrupt stack for when this thread makes system calls, causes a processor exception, or is interrupted. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A kernel TCB for saving and restoring the per-thread state.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To create a thread, the user library allocates a user-level stack for the thread and then does a system call into the kernel. The kernel allocates a TCB and interrupt stack, and arranges the state of the thread to start execution on the user-level stack at the beginning of the requested procedure. The kernel needs to store a pointer to the TCB in the process control block; if the process exits, the kernel must terminate any other threads running in the process. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">After creating the thread, the kernel puts the new thread on the ready list, to be scheduled like any other thread, and returns unique identifier for the user program to use when referring to the newly created thread (e.g., for join). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Thread join, yield, and exit work the same way: by calling into the kernel to perform the requested function. </FONT><A id=x1-29001r54 name=x1-29001r54></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.8.2 </FONT><A id=x1-300002 name=x1-300002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing User-Level Threads Without Kernel Support</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">It is also possible to implement threads as a library completely at user level, without any operating system support. Early thread libraries took this pure user-level approach for the simple reason that few operating systems supported multi-threaded processes. Even once operating system support for threads became widespread, pure user-level threads were sometimes used to minimize dependencies on specific operating systems and to maximize portability; for example, the earliest implementations of Sun&#8217;s Java Virtual Machine (JVM) implemented what were called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:green threads"}'>green threads</A></EM>, a pure user-level implementation of threads. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The basic idea is simple. The thread library instantiates all of its data structures within the process: TCBs, the ready list, the finished list, and the waiting lists all are just data structures in the process&#8217;s address space. Then, calls to the thread library are just procedure calls, akin to how the same functions are implemented within a multi-threaded kernel. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To the operating system kernel, a multi-threaded application using green threads appears to be a normal, single-threaded process. The process as a whole can make system calls, be time-sliced, etc. Unlike with kernel threads, when a process using green threads is time-sliced, the entire process, including all of its threads, is suspended. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A limitation of green threads is that the operating system kernel is unaware of the state of the user-level ready list. If the application performs a system call that blocks waiting for I/O, the kernel is unable to run a different user-level thread. Likewise, on a multiprocessor, the kernel is unable to run the different threads running within a single process on different processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Preemptive User-level Threads.</B> However, it is possible on most operating systems to implement preemption among user-level threads executing within a process. As we discussed in Chapter&nbsp;2, most operating systems provide an upcall mechanism to deliver asynchronous event notification to a process; on UNIX these are called signal handlers. Typical events or signals include the user hitting &#8220;Escape&#8221; or on UNIX &#8220;Control-C&#8221;; this informs the application to attempt to cleanly exit. Another common event is a timer interrupt to signal elapsed real time. To deliver an event, the kernel suspends the process execution and then resumes it running at a handler specified by the user code, typically on a separate upcall or signal stack. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To implement preemptive multi-threading for some process P : </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-30002x1 name=x1-30002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The user-level thread library makes a system call to register a timer signal handler and signal stack with the kernel. </FONT></P>
<LI class=enumerate><A id=x1-30004x2 name=x1-30004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When a hardware timer interrupt occurs, the hardware saves P &#8217;s register state and runs the kernel&#8217;s handler. </FONT></P>
<LI class=enumerate><A id=x1-30006x3 name=x1-30006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Instead of restoring P &#8217;s register state and resuming P where it was interrupted, the kernel&#8217;s handler copies P &#8217;s saved registers onto P &#8217;s signal stack. </FONT></P>
<LI class=enumerate><A id=x1-30008x4 name=x1-30008x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The kernel resumes execution in P at the registered signal handler on the signal stack. </FONT></P>
<LI class=enumerate><A id=x1-30010x5 name=x1-30010x5></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The signal handler copies the processor state of the preempted user-level thread from the signal stack to that thread&#8217;s TCB. </FONT></P>
<LI class=enumerate><A id=x1-30012x6 name=x1-30012x6></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The signal handler chooses the next thread to run, re-enables the signal handler (the equivalent of re-enabling interrupts), and restores the new thread&#8217;s state from its TCB into the processor. execution with the state (newly) stored on the signal stack.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This approach virtualizes interrupts and processor exceptions, providing a user-level process with a very similar picture to the one the kernel gets when these events occur. </FONT><A id=x1-30013r55 name=x1-30013r55></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.8.3 </FONT><A id=x1-310003 name=x1-310003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing User-Level Threads With Kernel Support</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Today, most programs use kernel-supported threads rather than pure user-level threads. Major operating systems support threads using standard abstractions, so the issue of portability is less of an issue than it once was. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, various systems take more of a hybrid model, attempting to combine the lightweight performance and application control over scheduling found in user-level threads, while keeping many of the advantages of kernel threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Hybrid Thread Join.</B> Thread libraries can avoid transitioning to the kernel in certain cases. For example, rather than always making a system call for thread_join&nbsp;to wait for the target thread to finish, thread_exit&nbsp;can store its exit value in a data structure in the process&#8217;s address space. Then, if the call to thread_join&nbsp;happens after the targeted thread has exited, it can immediately return the value without having to make a system call. However, if the call to thread_join&nbsp;precedes the call to thread_exit, then a system call is needed to transition to the WAITING&nbsp;state and let some other thread run. As a further optimization, on a multiprocessor it can sometimes make sense for thread_join&nbsp;to spin for a few microseconds before entering the kernel, in the hope that the other thread will finish in the meantime. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Per-Processor Kernel Threads.</B> It is possible to adapt the green threads approach to work on a multiprocessor. For many parallel scientific applications, the cost of creating and synchronizing threads is paramount, and so an approach that requires a kernel call for most thread operations would be prohibitive. Instead, the library multiplexes user-level threads on top of kernel threads, in exactly the same way that the kernel multiplexes kernel threads on top of physical processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When the application starts up, the user-level thread library creates one kernel thread for each processor on the host machine. As long as there is no other activity on the system, the kernel will assign each of these threads a processor. Each kernel thread executes the user-level scheduler in parallel: pull the next thread off the user-level ready list, and run it. Because thread scheduling decisions occur at user level, they can be flexible and application-specific; for example, in a parallel graph algorithm, the programmer might adjust the priority of various threads based on the results of the computation on other parts of the graph. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, most of the downsides of green threads are still present in these systems: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Any time a user-level thread calls into the kernel, its host kernel thread blocks. This prevents the thread library from running a different user-level thread on that processor in the meantime. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Any time the kernel time-slices a kernel thread, the user-level thread it was running is also suspended. The library cannot resume that thread until the kernel thread resumes.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Scheduler Activations.</B> To address these issues, some operating systems have added explicit support for user-level threads. One such model, implemented most recently in Windows, is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:scheduler activations"}'>scheduler activations</A></EM>. In this approach, the user-level thread scheduler is notified (or activated) for every kernel event that might affect the user-level thread system. For example, if one thread blocks in a system call, the activation informs the user-level scheduler that it should choose another thread to run on that processor. Scheduler activations are like upcalls or signals, except that they do not return to the kernel; instead, they directly perform user-level thread suspend and resume. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Various operations trigger a scheduler activation upcall: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-31002x1 name=x1-31002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Increasing the number of virtual processors.</B> When a program starts, it receives an activation to inform the program that it has been assigned a virtual processor: that activation runs the main thread and any other threads that might be created. To assign another virtual processor to the program, the kernel makes another activation upcall on the new processor; the user-level scheduler can pull a waiting thread off the ready list and run it. </FONT></P>
<LI class=enumerate><A id=x1-31004x2 name=x1-31004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Decreasing the number of virtual processors.</B> When the kernel preempts a virtual processor (e.g., to give the processor to a different process), the kernel makes an upcall on one of the other processors assigned to the parallel program. The thread system can then move the preempted user-level thread onto the ready list, so that a different processor can run it. </FONT></P>
<LI class=enumerate><A id=x1-31006x3 name=x1-31006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Transition to WAITING.</B> When a user-level thread blocks in the kernel waiting for I/O, the kernel similarly makes an upcall to notify the user-level scheduler that it needs to take action, e.g., to choose another thread to run while waiting for the I/O to complete. </FONT></P>
<LI class=enumerate><A id=x1-31008x4 name=x1-31008x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Transition from WAITING&nbsp;to READY.</B> When the I/O completes, the kernel makes an upcall to notify the scheduler that the suspended thread can be resumed. </FONT></P>
<LI class=enumerate><A id=x1-31010x5 name=x1-31010x5></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Transition from RUNNING&nbsp;to idle.</B> When a user-level activation finds an empty ready list (i.e., it has no more work to do), it can make a system call into the kernel to return the virtual processor for use by some other process.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As a result, most thread management functions &#8212; thread_create, thread_yield, thread_exit, and thread_join, as well as the synchronization functions described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> &#8212; are implemented as procedure calls within the process. Yet the user-level thread system always knows exactly how many virtual processors it has been assigned and is in complete control of what runs on those processors. </FONT><A id=x1-31011r53 name=x1-31011r53></A></P><A id=x1-320009 name=x1-320009>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.9 Alternative Abstractions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Although threads are a common way to express and manage concurrency, they are not the only way. In this section, we describe two popular alternatives, each targeted at a different application domain: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Asynchronous I/O and event-driven programming.</B> Asynchronous I/O and events allow a single-threaded program to cope with high-latency I/O devices by overlapping I/O with processing and other I/O. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Data parallel programming.</B> With data parallel programming, all processors perform the same instructions in parallel on different parts of a data set.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In each case, the goal is similar: to replace the complexities of multi-threading with a deterministic, sequential model that is easier for the programmer to understand and debug. </FONT><A id=x1-32001r56 name=x1-32001r56></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.9.1 </FONT><A id=x1-330001 name=x1-330001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Asynchronous I/O and Event-Driven Programming</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:asynchronous I/O"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Asynchronous I/O</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> is a way to allow a single-threaded process to issue multiple concurrent I/O requests at the same time. The process makes a system call to issue an I/O request but the call returns immediately, without waiting for the result. At a later time, the operating system provides the result to the process by either: (1) calling a signal handler, (2) placing the result in a queue in the process&#8217;s memory, or (3) storing the result in kernel memory until the process makes another system call to retrieve it. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An example use of asynchronous I/O is to overlap reading from disk with other computation in the same process. Reading from disk can take tens of milliseconds. In Linux, rather than issuing a read system call that blocks until the requested data has been read from disk, a process can issue an aio_read (asynchronous I/O read) system call; this call tells the operating system to initiate the read from disk and then to immediately return. Later, the process can call aio_error to determine if the disk read has finished and aio_return to retrieve the read&#8217;s results, as shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3300116"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-3300116 name=x1-3300116></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00396.gif" data-calibre-src="OEBPS/Images/image00396.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.16: </B>An asynchronous file read on Linux. The application calls aio_read to start the read; this system call returns immediately after the disk read is initialized. The application may then do other processing while the disk is completing the requested operation. The disk interrupts the processor when the operation is complete; this causes the kernel disk interrupt handler to run. The application at any time may ask the kernel if the results of the disk read are available, and then retrieve them with aio_return.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One common design pattern lets a single thread interleave different I/O-bound tasks by waiting for different I/O events. Consider a web server with 10 active clients. Rather than creating one thread per client and having each thread do a blocking read on the network connection, an alternative is for the server to have one thread that processes, in turn, the next message to arrive from <EM>any</EM> client. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For this, the server does a select call that blocks until <EM>any</EM> of the 10 network connections has data available to read. When the select call returns, it provides a list of connection with available data. The thread can then read from those connections, knowing that the read will always return immediately. After processing the data, the thread then calls select again to wait for the next data to arrive. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3300217"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this design pattern. </FONT><A id=x1-3300217 name=x1-3300217></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00397.gif" data-calibre-src="OEBPS/Images/image00397.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.17: </B>A server managing multiple concurrent connections using select. The server calls select to wait for data to arrive on any connection. The server then reads all available data, before returning to select. </FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Asynchronous I/O allows progress by many concurrent operating system requests. This approach gives rise to an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:event-driven programming"}'>event-driven programming</A></EM> pattern where a thread spins in a loop; each iteration gets and processes the next I/O event. To process each event, the thread typically maintains for each task a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:continuation"}'>continuation</A></EM>, a data structure that keeps track of a task&#8217;s current state and next step. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, handling a web request can involve a series of I/O steps: (a) make a network connection, (b) read a request from the network connection, (c) read the requested data from disk, and (d) write the requested data to the network connection. If a single thread is handling requests from multiple different clients at once, it must keep track of where it is in that sequence for each client. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Further, the network may divide a client&#8217;s request into several packets so that the server needs to make several read calls to assemble the full packet. The server may be doing this request assembly for multiple clients at once. Therefore, it needs to keep several per-client variables (e.g., a request buffer, the number of bytes expected, and the number of bytes received so far). When a new message arrives, the thread uses the network connection&#8217;s port number to identify which client sent the request and retrieves the appropriate client&#8217;s variables using this port number/client ID. It can then process the data. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-340001 name=x1-340001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Event-Driven Programming vs. Threads</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Although superficially different, overlapping I/O is fundamentally the same whether using asynchronous I/O and event-driven programming or synchronous I/O and threads. In either case, the program blocks until the next task can proceed, restores the state of that task, executes the next step of that task, and saves the task&#8217;s state until it can take its next step. The differences are: (1) whether the state is stored in a continuation or TCB and (2) whether the state save/restore is done explicitly by the application or automatically by the thread system. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a simple server that collects incoming data from several clients into a set of per-client buffers. The pseudo-code for the event-driven and thread-per-client cases is similar: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Event-driven
&nbsp;Hashtable&lt;Buffer*&gt;&nbsp;*hash;
&nbsp;
&nbsp;while(1)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;connection&nbsp;=&nbsp;use&nbsp;select()&nbsp;to&nbsp;find&nbsp;a
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readable&nbsp;connection&nbsp;ID
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer&nbsp;=&nbsp;hash.remove(connection);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;got&nbsp;=&nbsp;read(connection,&nbsp;tmpBuf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TMP_SIZE);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer-&gt;append(tmpBuf,&nbsp;got);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer&nbsp;=&nbsp;hash.put(connection,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer);
&nbsp;}
 </FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Thread-per-client
&nbsp;Buffer&nbsp;*b;
&nbsp;
&nbsp;while(1)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;got&nbsp;=&nbsp;read(connection,&nbsp;tmpBuf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TMP_SIZE);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer-&gt;append(tmpBuf,&nbsp;got);
&nbsp;}
 </FONT></PRE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When these programs execute, the system performs nearly the same work, as shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3400118"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. With events, the code uses select to determine which connection&#8217;s packet to retrieve next. With threads, the kernel transparently schedules each thread when data has arrived for it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The state in both cases is also similar. In the event-driven case, the application maintains a hash table containing the buffer state for each client. The server must do a lookup to find the buffer each time a packet arrives for a particular client. In the thread-per-client case, each thread has just one buffer, and the operating system keeps track of the different threads&#8217; states. </FONT><A id=x1-3400118 name=x1-3400118></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00398.gif" data-calibre-src="OEBPS/Images/image00398.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.18: </B>Two alternate implementations of a server. In the upper picture, a single thread uses a hash table to keep track of connection state. In the lower picture, each thread keeps a pointer to the state for one connection.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To compare the two approaches, consider again the various use cases for threads from Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-110001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance: Coping with high-latency I/O devices.</B> Either approach &#8212; event-driven or threads &#8212; can overlap I/O and processing. Which provides better performance? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The common wisdom has been that the event-driven approach was significantly faster for two reasons. First, the space and context switch overheads of this approach could be lower because a thread system must use generic code that allocates a stack for each thread&#8217;s state and that saves and restores all registers on each context switch, while the event-driven approach lets programmers allocate and save/restore just the state needed for each task. Second, some past operating systems had inefficient or unscalable implementations of their thread systems, making it important not to create too many threads for each process. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Today, the comparison is less clear cut. Many systems now have large memories, so the cost of allocating a thread stack for each task is less critical. For example, allocating 1000 threads with an 8 KB stack per thread on a machine with 1 GB of memory would consume less than 1% of the machine&#8217;s memory. Also, most operating systems now have efficient and scalable threads libraries. For example, while the Linux 2.4 kernel had poor performance when processes had many threads, Linux 2.6 revamped the thread system, improving its scalability and absolute performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Anecdotal evidence suggests that the performance gap between the two approaches has greatly narrowed. For some applications, highly optimized thread management code and synchronous I/O paths can out-perform less-optimized application code and asynchronous I/O paths. In most cases, the performance difference is small enough that other factors (e.g., code simplicity and ease of maintenance) are more important than raw performance. If performance is crucial for a particular application, then, as is often the case, there is no substitute for careful benchmarking before making your decision. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance: Exploiting multiple processors.</B> By itself, the event-driven approach does not help a program exploit multiple processors. In practice, event-driven and thread approaches are often combined: a program that uses n processors can have n threads, each of which uses the event-driven pattern to multiplex multiple I/O-bound tasks on each processor. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Responsiveness: Shifting work to run in the background.</B> While event-driven programming can be effective when tasks are usually short-lived, threads can be more convenient when there is a mixture of foreground and background tasks. At some cost in coding complexity, the event-driven model can be adapted to this case, e.g., by cutting long tasks into smaller chunks whose state can be explicitly saved when higher priority work is pending. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Program structure: Expressing logically concurrent tasks.</B> Whenever there are two viable programming styles, there are strong advocates for each approach. The situation is no different here, with some advocates of event-driven programming arguing that the synchronization required when threads share data makes threads more complex than events. Advocates for threads argue that they provide a more natural way to express the control flow of a program than having to explicitly store a computation&#8217;s state in a continuation. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In our opinion, there remain cases where both styles are appropriate, and we use both styles in our own programs. That said, for most I/O-intensive programs, threads are preferable: they are often more natural, reasonably efficient, and simpler when running on multiple processors. </FONT><A id=x1-34002r58 name=x1-34002r58></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.9.2 </FONT><A id=x1-350002 name=x1-350002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Data Parallel Programming</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Another important application area is parallel computing, and there is an ongoing debate as to the effectiveness of threads versus other models for expressing and managing parallelism. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One popular model is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:data parallel programming"}'>data parallel programming</A></EM>, also known as SIMD (single instruction multiple data) programming or <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:bulk synchronous parallel programming"}'>bulk synchronous parallel programming</A></EM>. In this model, the programmer describes a computation to apply in parallel across an entire data set at the same time, operating on independent data elements. The work on every data item must complete before moving onto the next step; one processor can use the results of a different processor only in some later step. As a result, the behavior of the program is deterministic. Rather than having programmers divide work among threads, the runtime system decides how to map the parallel work across the hardware&#8217;s processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, taking the earlier example of zeroing a buffer in parallel in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-180017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, a data parallel program to zero an N item array can be as simple as: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;forall&nbsp;i&nbsp;in&nbsp;0:N-1
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;array[i]&nbsp;=&nbsp;0;</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The runtime system would divide the array among processors to execute the computation in parallel. Of course, the runtime system itself might be implemented using threads, but this is invisible to the programmer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Large data-analysis tasks often use data parallel programming. For example, Hadoop is an open source system that can process and analyze terabytes of data spread across hundreds or thousands of servers. It applies an arbitrary computation to each data element, such as to update the popularity of a web page based on a previous estimate of the popularity of the pages that refer to it. Hadoop applies the computation in parallel across all web pages, repeatedly, until the popularity of every page has converged. A search engine can then use the results to decide which pages should be returned in response to a search query. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another example is SQL (Structured Query Language). SQL is a standard language for accessing databases in which programmers specify the database query to perform, and the database maps the query to lower-level thread and disk operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multimedia streams (e.g., audio, video, and graphics) often have large amounts of data on which similar operations are repeatedly performed, so data parallel programming is frequently used for media processing; specialized hardware to support this type of parallel processing is common. Because they are optimized for highly structured data parallel programs, GPUs (Graphical Processing Units) can provide significantly higher rates of data processing. For example, in 2013 a mid-range Radeon 7850 GPU was capable of 1.69 TFLOPS (Trillion FLoating point Operations Per Second (double-precision)); for comparison, an Intel i7 3960 CPU (a high-end, six core general-purpose processor) was capable of 0.19 double-precision TFLOPS. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Considerable effort is currently going towards developing and using General Purpose GPUs (GPGPUs) &#8212; GPUs that better support a wider-range of programs. It is still not clear which classes of programs can work well with GPGPUs and which require more traditional CPU architectures, but for those programs that can be ported to the more restrictive GPGPU programming model, performance gains could be dramatic. </FONT><A id=x1-35001r57 name=x1-35001r57></A></P><A id=x1-3600010 name=x1-3600010>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.10 Summary and Future Directions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrency is ubiquitous &#8212; not only do most smartphones, servers, desktops, laptops, and tablets have multiple cores, but users have come to expect a responsive interface at all times, I/O latencies have become gigantic compared to computer instruction cycle times, and servers must be able to process large numbers of simultaneous requests. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although threads are not the only possible solution to these issues, they are a general-purpose technique that can be applied to a wide range of concurrency issues. In our view, multi-threaded programming is a skill that every professional programmer must master. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this chapter, we have discussed: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The thread abstraction.</B> Threads are a set of concurrent activities, each of which executes sequentially at unpredictable speed. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>A simple thread API.</B> Thread libraries, whether for use in the operating system kernel or in application code, provide the ability to perform an asynchronous procedure call. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread implementations.</B> The core of any implementation of preemptive multi-threading is the ability to save one thread&#8217;s state and restore another&#8217;s. The thread system keeps track of the saved state of all threads not currently running; it switches threads between READY&nbsp;and RUNNING&nbsp;as needed. The implementation of multi-threading can be in the kernel or at user-level, depending on the goals of the system. In our view, most systems in the future will have both a kernel-level thread system for managing concurrency in the operating system, and a lightweight thread system for expressing parallelism at the application level. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Alternative abstractions.</B> Practical alternatives to threads exist for two important domains: event-driven programming for servers as well as data parallel programming for multiprocessors.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Technology trends suggest that concurrent programming will only increase in importance over time. After several decades in which computer architects were able to make individual processor cores run faster and faster, we have reached a point where the performance of individual cores is leveling off and where further speedups will have to come from parallel processing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The best programming model for expressing and managing parallelism is still an active area of research, but it seems likely that threads will remain an important option for decades to come. </FONT><A id=x1-36001r63 name=x1-36001r63></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.10.1 </FONT><A id=x1-370001 name=x1-370001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Historical Notes</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">The extreme engineering complexity and bugginess of commercial operating systems in the 1960&#8217;s led researchers to investigate alternatives. One direct result of this experience was modern software engineering: the systematic management of complex implementation tasks through the careful control of feature lists, module testing, assertions, and so forth. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another consequence was the use of threads for managing concurrency. One of the most influential papers in computer science history is Dijkstra&#8217;s description of his THE system&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XDijkstra:1968:SLS:363095.363143"}'><FONT style="BACKGROUND-COLOR: #7be1e1">48</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. Dijkstra argued for constructing operating systems as a series of layered abstractions, with communicating threads implementing each layer. Within a decade, the research community was convinced. When Xerox PARC built the Alto in the late 1970&#8217;s, the Alto&#8217;s operating system was built from the ground up using threads. The Alto demonstrated most of the technology we now take for granted with personal computers: bit-mapped display, menus, windowing, mice, Ethernet, and email. We base much of our description of thread programming on the experiences from that project&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XLampson:1980:EPM:358818.358824"}'><FONT style="BACKGROUND-COLOR: #7be1e1">98</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Widespread commercial adoption of threads took much longer, however. By the early 1990&#8217;s, the widespread adoption of client-server computing led to several commercially important operating systems written from scratch using threads, including Microsoft&#8217;s Windows NT, SUN Microsystems Solaris, and Linux. Client operating systems followed, and by the late 1990&#8217;s, with Apple&#8217;s introduction of OS X, all major commercial operating systems were based on threads. At about the same time, the interface to thread libraries became standardized, starting with POSIX in 1995. Likewise, modern programming languages such as Java were designed with constructs for creating and synchronizing threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The increasing importance of parallel processing led to the development of very lightweight user-level thread implementations, as there is little point to parallelizing an application unless it improves performance. By the early 90&#8217;s, scheduler activations were developed to integrate user-level and kernel threads&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XAnderson:1992:SAE:146941.146944"}'><FONT style="BACKGROUND-COLOR: #7be1e1">2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even so, the topic of whether threads are a better programming model than the alternatives remains an active one&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Xtyma"}'><FONT style="BACKGROUND-COLOR: #7be1e1">159</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. Several prominent operating systems researchers have argued that normal programmers should almost never use threads because (a) it is just too hard to write multi-threaded programs that are correct and (b) most things that threads are commonly used for can be accomplished in other, safer ways&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Xousterhoutbadthreads"}'><FONT style="BACKGROUND-COLOR: #7be1e1">129</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">,&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XvanRenesse:1998:GPC:319195.319208"}'><FONT style="BACKGROUND-COLOR: #7be1e1">160</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. These are important arguments to understand &#8212; even if you disagree with them, they point out pitfalls with using threads that are important to avoid. </FONT><A id=Q1-1-66 name=Q1-1-66></A><A id=Q1-1-67 name=Q1-1-67></A></P><A id=x1-380001 name=x1-380001>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=problems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For some of the following problems, you will need to download the thread library from </FONT><A href="http://ospp.cs.washington.edu/instructor.html"><FONT style="BACKGROUND-COLOR: #7be1e1">http://ospp.cs.washington.edu/instructor.html</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. The comment at the top of threadHello.c explains how to compile and run a program that uses this library. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Download threadHello.c, compile it, and run it several times. What happens when you run it? Do you get the same result if you run it multiple times? What if you are also running some other demanding processes (e.g., compiling a big program, playing a Flash game on a website, or watching streaming video) when you run this program? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For the threadHello program in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-170016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, suppose that we delete the second for loop so that the main routine simply creates NTHREADS threads and then prints &#8220;Main thread done.&#8221; What are the possible outputs of the program now. <B>Hint:</B> Fewer than NTHREADS+1 lines may be printed in some runs. Why? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">How expensive are threads? Write a program that times how long it takes to create and then join 1000 threads, where each thread simply calls thread_exit(0) as soon as it starts running. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Write a program that has two threads. Make the first thread a simple loop that continuously increments a counter and prints a period (&#8220;.&#8221;) whenever the value of that counter is divisible by 10,000,000. Make the second thread repeatedly wait for the user to input a line of text and then print &#8220;Thank you for your input.&#8221; On your system, does the first thread makes rapid progress? Does the second thread respond quickly? </FONT><A id=x1-3800119 name=x1-3800119></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00399.gif" data-calibre-src="OEBPS/Images/image00399.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.19: </B>Matrix multiplication.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Write a program that uses threads to perform a parallel matrix multiply. To multiply two matrices, C = A * B, the result entry C<SUB>(i,j)</SUB> is computed by taking the dot product of the ith row of A and the jth column of B: C<SUB>i,j</SUB> = &#931;<SUB>k=0</SUB><SUP>N-1</SUP>A<SUB>(i,k)</SUB>B<SUB>(k,j)</SUB>. We can divide the work by creating one thread to compute each value (or each row) in C, and then executing those threads on different processors in parallel, as shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3800119"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.19</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Write a program that uses threads to perform a parallel merge sort. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For the threadHello program in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-170016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, the procedure go() has the parameter np and the local variable n. Are these variables <EM>per-thread</EM> or <EM>shared</EM> state? Where does the compiler store these variables&#8217; states? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For the threadHello program in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-170016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, the procedure main() has local variables such as i and exitValue. Are these variables <EM>per-thread</EM> or <EM>shared</EM> state? Where does the compiler store these variables? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">In the <EM>thread-local variables</EM> sidebar, we described how many thread systems have this type of per-thread state. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Describe how you would implement thread-local variables. Each thread should have an array of 1024 pointers to its thread-local variables. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">What would you add to the TCB? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">How would you change the thread creation procedure? (For simplicity, assume that when a thread is created, all 1024 entries should be initialized to NULL.) </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">How would a running thread allocate a new thread-local variable? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">In your design, how would a running thread access a particular thread-local variable? </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For the threadHello program, what is the minimum and maximum number of times that the main thread enters the WAITING&nbsp;state? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Using simple threads, write a program that creates several threads and then determines whether the threads package on your system allocates a fixed-size stack for each thread or whether each thread&#8217;s stack starts at some small size and dynamically grows as needed. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Hints:</B> You probably want to write a recursive procedure that you can use to consume a large amount of stack memory. You may also want to examine the addresses of variables allocated to different threads&#8217; stacks. Finally, you may want to be able to determine how much memory has been allocated to your process; most operating systems have a command or utility that can show the resource consumption of currently running processes (e.g., top in Linux, Activity Monitor in OSX, or Task Manager in Windows). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-390005 name=x1-390005>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">5. Synchronizing Access to Shared Objects</FONT></I></H2></A>
<DIV class=chapterQuote>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is not enough to be industrious. So are the ants. The question is: What are we industrious about? &#8212;<I>Henry David Thoreau</I> </FONT></P>
<DL>
<DT><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DD><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DD></DL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
<BR></FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multi-threaded programs extend the traditional, single-threaded programming model so that each thread provides a single sequential stream of execution composed of familiar instructions. If a program has <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:independent threads"}'>independent threads</A></EM> that operate on completely separate subsets of memory, we can reason about each thread separately. In this case, reasoning about independent threads differs little from reasoning about a series of independent, single-threaded programs. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, most multi-threaded programs have both <EM>per-thread state</EM> (e.g., a thread&#8217;s stack and registers) and <EM>shared state</EM> (e.g., shared variables on the heap). <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:cooperating threads"}'>Cooperating threads</A></EM> read and write shared state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Sharing state is useful because it lets threads communicate, coordinate work, and share information. For example, in the </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-110001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Earth Visualizer</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> example in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, once one thread finishes downloading a detailed image from the network, it shares that image data with a rendering thread that draws the new image on the screen. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, when cooperating threads share state, writing correct multi-threaded programs becomes much more difficult. Most programmers are used to thinking &#8220;sequentially&#8221; when reasoning about programs. For example, we often reason about the series of states traversed by a program as a sequence of instructions is executed. However, this sequential model of reasoning does not work in programs with cooperating threads, for three reasons: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-39002x1 name=x1-39002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Program execution depends on the possible interleavings of threads&#8217; access to shared state.</B> For example, if two threads write a shared variable, one thread with the value 1 and the other with the value 2, the final value of the variable depends on which of the threads&#8217; writes finishes last. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although this example is simple, the problem is severe because programs need to work for <EM>any possible interleaving</EM>. In particular, recall that thread programmers </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-150002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">should not make any assumptions about the relative speed at which their threads operate.</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Worse, as programs grow, there is a combinatorial explosion in the number of possible interleavings. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center><EM><FONT style="BACKGROUND-COLOR: #7be1e1">How can we reason about all possible interleavings of threads&#8217; actions in a multi-million line program?</FONT></EM></DIV></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<LI class=enumerate><A id=x1-39004x2 name=x1-39004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Program execution can be nondeterministic.</B> Different runs of the same program may produce different results. For example, the scheduler may make different scheduling decisions, the processor may run at a different frequency, or another concurrently running program may affect the cache hit rate. Even common debugging techniques &#8212; such as running a program under a debugger, recompiling with the -g option instead of -O, or adding a printf &#8212; can change how a program behaves. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Jim Gray, the 1998 ACM Turing Award winner, coined the term <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Heisenbugs"}'>Heisenbugs</A></EM> for bugs that disappear or change behavior when you try to examine them. Multi-threaded programming is a common source of Heisenbugs. In contrast, <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Bohrbugs"}'>Bohrbugs</A></EM> are deterministic and generally much easier to diagnose. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center><EM><FONT style="BACKGROUND-COLOR: #7be1e1">How can we debug programs with behaviors that change across runs?</FONT></EM></DIV></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<LI class=enumerate><A id=x1-39006x3 name=x1-39006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Compilers and processor hardware can reorder instructions.</B> Modern compilers and hardware reorder instructions to improve performance. This reordering is generally invisible to single-threaded programs; compilers and processors take care to ensure that dependencies within a single sequence of instructions &#8212; that is, within a thread &#8212; are preserved. However, reordering can become visible when multiple threads interact through accessing shared variables. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, consider the following code to compute q as a function of p: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Thread&nbsp;1
&nbsp;
&nbsp;p&nbsp;=&nbsp;someComputation();
&nbsp;pInitialized&nbsp;=&nbsp;true;
&nbsp;</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Thread&nbsp;2
&nbsp;
&nbsp;while(!pInitialized)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;
&nbsp;q&nbsp;=&nbsp;anotherComputation(p);
&nbsp;</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although it seems that p is always initialized before anotherComputation(p) is called, this is not the case. To maximize instruction level parallelism, the hardware or compiler may set pInitialized = true before the computation to compute p has completed, and anotherComputation(p) may be computed using an unexpected value. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center><EM><FONT style="BACKGROUND-COLOR: #7be1e1">How can we reason about thread interleavings when compilers and processor hardware may reorder a thread&#8217;s operations?</FONT></EM></DIV></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Why do compilers and processor hardware reorder operations?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We often find that students are puzzled by the notion that a compiler might produce code, or a processor might execute code, in a way that is correct for a single thread but unpredictable for a multi-threaded program without synchronization. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For compilers, the issue is simple. Modern processors have deep pipelines; they execute many instructions simultaneously by overlapping the instruction fetch, instruction decode, data fetch, arithmetic operation, and conditional branch of a sequence of instructions. The processor stalls when necessary &#8212; e.g., if the result of one instruction is needed by the next. Modern compilers will reorder instructions to reduce these stalls as much as possible, provided the reordering does not change the behavior of the program. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The difficulty arises in what assumptions the compiler can make about the code. If the code is single-threaded, it is much easier to analyze possible dependencies between adjacent instructions, allowing more optimization. By contrast, variables in (unsynchronized) multi-threaded code can potentially be read or written by another thread at any point. As the example in the text demonstrated, the precise sequence of seemingly unrelated instructions can potentially affect the behavior of the program. To preserve semantics, instruction re-ordering may no longer be feasible, resulting in more processor stalls and slower code execution. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As long as the programmer uses structured synchronization for protecting shared data, the compiler can reorder instructions as needed without changing program behavior, provided that the compiler does not reorder across synchronization operations. A compiler making the more conservative assumption that all memory is shared would produce slow code even when it was not necessary. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For processor architectures, the issue is also performance. Certain optimizations are possible if the programmer is using structured synchronization but not otherwise. For example, modern processors buffer memory writes to allow instruction execution to continue while the memory is written in the background. If two adjacent instructions issue memory writes to different memory locations, they can occur in parallel and complete out of order. This optimization is safe on a single processor, but potentially unsafe if multiple processors are simultaneously reading and writing the same locations without intervening synchronization. Some processor architectures make the conservative assumption that optimizations should never change program behavior regardless of the programming style &#8212; in this case, they stall to prevent reordering. Others make a more optimistic assumption that the programmer is using structured synchronization. For your code to be portable, you should assume that the compiler and the hardware can reorder instructions except across synchronization operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Given these challenges, multi-threaded code can introduce subtle, non-deterministic, and non-reproducible bugs. This chapter describes a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:structured synchronization"}'>structured synchronization</A></EM> approach to sharing state in multi-threaded programs. Rather than scattering access to shared state throughout the program and attempting <EM>ad hoc</EM> reasoning about what happens when the threads&#8217; accesses are interleaved in various ways, a better approach is to: (1) structure the program to facilitate reasoning about concurrency, and (2) use a set of standard synchronization primitives to control access to shared state. This approach gives up some freedom, but if you consistently follow the rules we describe in this chapter, then reasoning about programs with shared state becomes much simpler. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The first part of this chapter elaborates on the challenges faced by multi-threaded programmers and on why it is dangerous to try to reason about all possible thread interleavings in the general, unstructured case. The rest of the chapter describes how to structure shared objects in multi-threaded programs so that we can reason about them. First, we structure a multi-threaded program&#8217;s shared state as a set of <EM>shared objects</EM> that encapsulate the shared state as well as define and limit how the state can be accessed. Second, to avoid <EM>ad hoc</EM> reasoning about the possible interleavings of access to the state variables within a shared object, we describe how shared objects can use a small set of <EM>synchronization primitives</EM> &#8212; locks and condition variables &#8212; to coordinate access to their state by different threads. Third, to simplify reasoning about the code in shared objects, we describe a set of <EM>best practices</EM> for writing the code that implements each shared object. Finally, we dive into the details of how to implement synchronization primitives. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multi-threaded programming has a reputation for being difficult. We agree that it takes care, but this chapter provides a set of simple rules that anyone can follow to implement objects that can be safely shared by multiple threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Chapter roadmap:</B> </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Challenges.</B> Why is it difficult to reason about multi-threaded programs with unstructured use of shared state? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-400001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Structuring Shared Objects.</B> How should we structure access to shared state by multiple threads? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-460002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Locks: Mutual Exclusion.</B> How can we enforce a logical sequence of operations on shared state? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-490003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Condition Variables: Waiting for a Change.</B> How does a thread wait for a change in shared state? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-540004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Designing and Implementing Shared Objects.</B> Given locks and condition variables, what is a good way to write and reason about the code for shared objects? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-580005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Three Case Studies.</B> We illustrate our methodology by using it to develop solutions to three concurrent programming challenges. (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-620006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Implementing Synchronization Primitives.</B> How are locks and condition variables implemented? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-660007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Semaphores Considered Harmful.</B> What other synchronization primitives are possible, and how do they relate to locks and condition variables? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-740008"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">)</FONT></P></LI></UL><A id=x1-39007r64 name=x1-39007r64></A><A id=x1-400001 name=x1-400001>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.1 Challenges</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">We began this chapter with the core challenge of multi-threaded programming: a multi-threaded program&#8217;s execution depends on the interleavings of different threads&#8217; access to shared memory, which can make it difficult to reason about or debug these programs. In particular, cooperating threads&#8217; execution may be affected by <EM>race conditions.</EM> </FONT><A id=x1-40001r65 name=x1-40001r65></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.1.1 </FONT><A id=x1-410001 name=x1-410001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Race Conditions</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:race condition"}'>race condition</A></EM> occurs when the behavior of a program depends on the interleaving of operations of different threads. In effect, the threads run a race between their operations, and the results of the program execution depends on who wins the race. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Reasoning about even simple programs with race conditions can be difficult. To appreciate this, we now look at three extremely simple multi-threaded programs. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The world&#8217;s simplest cooperating-threads program.</B> Suppose we run a program with two threads that do the following: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread A</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread B</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">x = 1; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; x = 2;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What are the possible final values of x? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The result can be <B>x = 1 or x = 2</B> depending on which thread wins or loses the &#8220;race&#8221; to set x. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">That was easy, so let&#8217;s try one that is a bit more interesting. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The world&#8217;s second-simplest cooperating-threads program.</B> Suppose that initially y = 12, and we run a program with two threads that do the following: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread A</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread B</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">x = y + 1; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; y = y * 2;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What are the possible final values of x? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The result is <B>x = 13 if Thread A executes first or x = 25 if Thread B executes first.</B> More precisely, we get x = 13 if Thread A reads y before Thread B updates y, or we get x = 25 if Thread B updates y before Thread A reads y. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The world&#8217;s third-simplest cooperating-threads program.</B> Suppose that initially x = 0 and we run a program with two threads that do the following: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread A</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread B</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">x = x + 1; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; x = x + 2;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What are the possible final values of x? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Obviously, <B>one possible outcome is x = 3.</B> For example, Thread A runs to completion and then Thread B starts and runs to completion. However, <B>we can also get x = 2 or x = 1.</B> In particular, when we write a single statement like x = x + 1, compilers on many processors produce multiple instructions, such as: (1) load memory location x into a register, (2) add 1 to that register, and (3) store the result to memory location x. If we disassemble the above program into simple pseudo-assembly-code, we can see some of the possibilities. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center><FONT style="BACKGROUND-COLOR: #7be1e1"><B>One Interleaving</B> </FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread A</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread B</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load r1, x </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">add r2, r1, 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">store x, r2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; load r1, x </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; add r2, r1, 2</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; store x, r2 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><B><FONT style="BACKGROUND-COLOR: #7be1e1">final: x == 3</FONT></B></DIV></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Another Interleaving</B> </FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread A</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread B</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load r1, x </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; load r1, x </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">add r2, r1, 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; add r2, r1, 2 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">store x, r2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; store x, r2 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><B><FONT style="BACKGROUND-COLOR: #7be1e1">final: x == 2</FONT></B></DIV></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Yet Another Interleaving</B><BR></FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread A</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread B</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">load r1, x </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; load r1, x </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">add r2, r1, 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; add r2, r1, 2 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; store x, r2 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">store x, r2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD></TR></TBODY></TABLE></DIV><B><FONT style="BACKGROUND-COLOR: #7be1e1">final: x == 1</FONT></B></DIV></TD></TR></TBODY></TABLE><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even for this 2-line program, the complexity of reasoning about race conditions and interleavings is beginning to grow. Not only would one have to reason about all possible interleavings of statements, but one would also have to disassemble the program and reason about all possible interleavings of assembly instructions. (And if the compiler and hardware can reorder instructions, there are even more possibilities to consider.) </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>The Case of the Therac-25</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Therac-25 was a cancer therapy device, designed to deliver very high doses of radiation to a targeted region of the body in an attempt to eliminate cancer cells before they had a chance to spread. Over a several year period in the mid-1980&#8217;s, a computer malfunction caused six separate patients to receive an estimated 100 times the intended dose of radiation. Three of the patients later died as a result; the others sustained serious but non-fatal injuries. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although there were many contributing factors to the malfunction, a race condition was at the heart of both the overdose and the delay in recognizing and repairing the problem. The Therac-25 was designed to check in software that the entered dosage was medically safe before using it to configure the radiation beam. However, the software was also concurrent: the operator interface code could run at the same time that the dosage was being checked and used, with no locking or other synchronization. In rare cases, the dosage could be changed after the check and before the use, and due to a separate user interface bug, the operator could enter an overdose without either intending or realizing it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because the problem required a rare sequence of events, the machine appeared to work successfully for almost all patients. Years elapsed between the first incident and the final one, and during this period, the manufacturer repeatedly insisted that no overdose was possible and that the patient injuries must be due to some other factor. It took the second occurrence of the race condition at the same hospital to help reveal the system&#8217;s design flaw. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-41001r72 name=x1-41001r72></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.1.2 </FONT><A id=x1-420002 name=x1-420002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Atomic Operations</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">When we disassembled the code in last example, we could reason about <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:atomic operations"}'>atomic operations</A></EM>, indivisible operations that cannot be interleaved with or split by other operations. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On most modern architectures, a load or store of a 32-bit word from or to memory is an atomic operation. So, the previous analysis reasoned about interleaving of atomic loads and stores to memory. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Conversely, a load or store is not always an atomic operation. Depending on the hardware implementation, if two threads store the value of a 64-bit floating point register to a memory address, the final result might be the first value, the second value, or a mix of the two. </FONT><A id=x1-42001r73 name=x1-42001r73></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.1.3 </FONT><A id=x1-430003 name=x1-430003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Too Much Milk</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Although one could, in principle, reason carefully about the possible interleavings of different threads&#8217; atomic loads and stores, doing so is tricky and error-prone. Later, we present a higher level abstraction for synchronizing threads, but first we illustrate the problems with using atomic loads and stores using a simple problem called, &#8220;Too Much Milk.&#8221; The example is intentionally simple; real-world concurrent programs are often much more complex. Even so, the example shows the difficulty of reasoning about interleaved access to shared state. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Too Much Milk problem models two roommates who share a refrigerator and who &#8212; as good roommates &#8212; make sure the refrigerator is always well stocked with milk. With such responsible roommates, the following scenario is possible: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>Roommate 1&#8217;s actions</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>Roommate 2&#8217;s actions</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:00 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Look in fridge; out of milk. </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:05 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Leave for store. </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:10 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Arrive at store. </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Look in fridge; out of milk.</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:15 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Buy milk. </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Leave for store. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:20 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Arrive home; put milk away. </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Arrive at store. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:25 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Buy milk. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:30 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Arrive home; put milk away. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3:35 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; Oh no! </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can model each roommate as a thread and the number of bottles of milk in the fridge with a variable in memory. If the only atomic operations on shared state are atomic loads and stores to memory, is there a solution to the Too Much Milk problem that ensures both <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:safety property"}'>safety</A></EM> (the program never enters a bad state) and <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:liveness property"}'>liveness</A></EM> (the program eventually enters a good state)? Here, we strive for the following properties: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Safety:</B> Never more than one person buys milk. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Liveness:</B> If milk is needed, someone eventually buys it.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: <B>Simplifying Assumption.</B> Throughout the analysis in this section, we assume that the instructions are executed in exactly the order written, i.e., neither the compiler nor the architecture reorders instructions. This assumption is crucial for reasoning about the order of atomic load and store operations, but many modern compilers and architectures violate it, so be extremely careful applying the style of analysis we present here to your own programs. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Solution 1.</B> The basic idea is for a roommate to leave a note on the fridge before going to the store. The simplest way to leave this note &#8212; given our programming model that we have shared memory on which we can perform atomic loads and stores &#8212; is to set a flag when going to buy milk and to check this flag before going to buy milk. Each thread might run the following code: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;if&nbsp;(milk==0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;milk
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(note==0)&nbsp;{&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;note
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;note&nbsp;=&nbsp;1;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;leave&nbsp;note
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;buy&nbsp;milk
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;note&nbsp;=&nbsp;0;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;remove&nbsp;note
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, this implementation can violate safety. For example, the first thread could execute everything up to and including the check of the milk value and then get context switched. Then, the second thread could run through all of this code and buy milk. Finally, the first thread could be re-scheduled, see that note is zero, leave the note, buy more milk, and remove the note, leaving the system with milk == 2. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Thread&nbsp;B
   &nbsp;if&nbsp;(milk==0)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(milk==0)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(note==0)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;note&nbsp;=&nbsp;1;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;note&nbsp;=&nbsp;0;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(note==0)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;note&nbsp;=&nbsp;1;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;note&nbsp;=&nbsp;0;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;}
   &nbsp;
   &nbsp;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Oh&nbsp;no!</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This &#8220;solution&#8221; makes the problem worse! The preceding code usually works, but it may fail occasionally when the scheduler does just the right (or wrong) thing. We have created a Heisenbug that causes the program to occasionally fail in ways that may be very difficult to reproduce (e.g., probably only when the grader is looking at it or when the CEO is demonstrating a new product at a trade show). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Solution 2.</B> In solution 1, the roommate checks the note before setting it. This opens up the possibility that one roommate has already made a decision to buy milk before notifying the other roommate of that decision. If we use two variables for the notes, a roommate can create a note before checking the other note and the milk and making a decision to buy. For example, we can do the following: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Path A </FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;noteA&nbsp;=&nbsp;1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;leave&nbsp;note
&nbsp;if&nbsp;(noteB==0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;note&nbsp;&nbsp;A1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(milk==0)&nbsp;{&nbsp;//&nbsp;if&nbsp;no&nbsp;milk&nbsp;&nbsp;A2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;buy&nbsp;milk&nbsp;&nbsp;&nbsp;&nbsp;A3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;noteA&nbsp;=&nbsp;0;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;remove&nbsp;note&nbsp;A
 </FONT></PRE><FONT style="BACKGROUND-COLOR: #7be1e1">Path B </FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;noteB&nbsp;=&nbsp;1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;leave&nbsp;note
&nbsp;if&nbsp;(noteA==0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;note&nbsp;&nbsp;B1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(milk==0)&nbsp;{&nbsp;//&nbsp;if&nbsp;no&nbsp;milk&nbsp;&nbsp;B2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;buy&nbsp;milk&nbsp;&nbsp;&nbsp;&nbsp;B3
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B4
&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B5
&nbsp;noteB&nbsp;=&nbsp;0;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;remove&nbsp;note
 </FONT></PRE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the first thread executes the Path A code and the second thread executes the Path B code, this protocol is safe; by having each thread write a note (&#8220;I might buy milk&#8221;) before deciding to buy milk, we ensure the safety property: at most one thread buys milk. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although this intuition is solid, proving the safety property without enumerating all possible interleavings requires care. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Safety Proof.</B> Assume for the sake of contradiction that the algorithm is <EM>not</EM> safe &#8212; both A and B buy milk. Consider the state of the two variables (noteB, milk) when thread A is at the line marked <B>A1</B>, at the precise moment when the atomic load of noteB from shared memory to A&#8217;s register occurs. There are three cases to consider: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Case 1:</B> (noteB = 1, milk = any value). This state contradicts the assumption that thread A buys milk and reaches <B>A3</B>. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Case 2:</B> (noteB = 0, milk &gt; 0). In this simple program, the property milk &gt; 0 is a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:stable property"}'>stable property</A></EM> &#8212; once it becomes true, it remains true forever. Thus, if milk &gt; 0 is true when A is at <B>A1</B>, A&#8217;s test at line <B>A2</B> will fail, and A will not buy milk, contradicting our assumption. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Case 3:</B> (noteB = 0, milk = 0). We know that thread B must not currently be executing any of the lines marked <B>B1-B5</B>. We also know that either noteA == 1 or milk &gt; 0 will be true from this time forward (noteA OR milk is also a stable property). This means that B cannot buy milk in the future (either the test at B1 or B2 must fail), which contradicts our assumption that both A and B buy milk. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since every case contradicts the assumption, the algorithm is safe. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Liveness.</B> Unfortunately, Solution 2 does not ensure liveness. In particular, it is possible for both threads to set their respective notes, for each thread to check the other thread&#8217;s note, and for both threads to decide not to buy milk. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This brings us to Solution 3. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Solution 3.</B> Solution 2 was safe because a thread would avoid buying milk if there were any chance that the other thread <EM>might</EM> buy milk. For Solution 3, we ensure that at least one of the threads determines whether the other thread has bought milk or not before deciding whether or not to buy. In particular, we do the following: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Path A </FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;noteA&nbsp;=&nbsp;1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;leave&nbsp;note&nbsp;A
&nbsp;while&nbsp;(noteB==1)&nbsp;{&nbsp;//&nbsp;wait&nbsp;for&nbsp;no&nbsp;note&nbsp;B
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;spin
&nbsp;}
&nbsp;if&nbsp;(milk==0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;milk&nbsp;M
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;buy&nbsp;milk
&nbsp;}
&nbsp;noteA&nbsp;=&nbsp;0;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;remove&nbsp;note&nbsp;A
 </FONT></PRE><FONT style="BACKGROUND-COLOR: #7be1e1">Path B </FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;noteB&nbsp;=&nbsp;1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;leave&nbsp;note&nbsp;B
&nbsp;if&nbsp;(noteA==0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;note&nbsp;A
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(milk==0)&nbsp;{&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;milk
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;buy&nbsp;milk
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//
&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//
&nbsp;noteB&nbsp;=&nbsp;0;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;remove&nbsp;note&nbsp;B
 </FONT></PRE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can show that Solution 3 is safe using an argument similar to the one we used for Solution 2. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To show that Solution 3 is live, observe that code path B has no loops, so eventually thread B must finish executing the listed code. Eventually, noteB == 0 becomes true and remains true. Therefore, thread A must eventually reach line <B>M</B> and decide whether to buy milk. If it finds M == 1, then milk has been bought. If it finds M == 0, then it will buy milk. Either way, the liveness property &#8212; that if needed, some milk is bought &#8212; is met. </FONT><A id=x1-43001r74 name=x1-43001r74></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.1.4 </FONT><A id=x1-440004 name=x1-440004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Discussion</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming that the compiler and processor execute instructions in program order, the preceding proof shows that it is possible to devise a solution to Too Much Milk that is both safe and live using nothing but atomic load and store operations on shared memory. Although the solution we presented only works for two roommates, there is a generalization, called Peterson&#8217;s algorithm, which works with any fixed number of n threads. More details on Peterson&#8217;s algorithm can be found elsewhere (e.g., </FONT><A href="http://en.wikipedia.org/wiki/Peterson's_algorithm"><FONT style="BACKGROUND-COLOR: #7be1e1">http://en.wikipedia.org/wiki/Peterson&#8217;s_algorithm</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">). </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, our solution for Too Much Milk (and likewise Peterson&#8217;s algorithm) is not terribly satisfying: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The solution is <EM>complex</EM> and requires careful reasoning to be convinced that it works. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The solution is <EM>inefficient</EM>. In Too Much Milk, while thread A is waiting, it is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:busy-waiting"}'>busy-waiting</A></EM> and consuming CPU resources. In Peterson&#8217;s generalized solution, <EM>all</EM> n threads can busy-wait. Busy-waiting is particularly problematic on modern systems with preemptive multi-threading, as the spinning thread may be holding the processor waiting for an event that cannot occur until some preempted thread is re-scheduled to run. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The solution <EM>may fail</EM> if the compiler or hardware reorders instructions. This limitation can be addressed by using <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:memory barrier"}'>memory barriers</A></EM> (see sidebar). Adding memory barriers would further increase the implementation complexity of the algorithm; barriers do not address the other limitations just mentioned. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Memory barriers</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you are writing low-level code that must reason about the ordering of memory operations. How can this be done on modern hardware and with modern compilers? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM>memory barrier</EM> instruction prevents the compiler and hardware from reordering memory accesses across the barrier &#8212; no accesses before the barrier are moved after the barrier and no accesses after the barrier are moved before the barrier. One can add memory barriers to the Too Much Milk solution or to Peterson&#8217;s algorithm to get code that works on modern machines with modern compilers. Of course, this makes the code even more complex. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Details of how to issue a memory barrier instruction depend on hardware and compiler details. However, a good example is gcc&#8217;s __sync_synchronize() builtin, which tells the compiler not to reorder memory accesses across the barrier and to issue processor-specific instructions that the underlying hardware treats as a memory barrier. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-44001r75 name=x1-44001r75></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.1.5 </FONT><A id=x1-450005 name=x1-450005></A><FONT style="BACKGROUND-COLOR: #7be1e1">A Better Solution</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">The next section describes a better approach to writing programs in which multiple threads access shared state. We write <EM>shared objects</EM> that use <EM>synchronization objects</EM> to coordinate different threads&#8217; access to shared state. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose, for example, we had a primitive called a <EM>lock</EM> that only one thread at a time can own. Then, we can solve the Too Much Milk problem by defining the class for a Kitchen object with the following method: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Kitchen::buyIfNeeded()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(milk&nbsp;==&nbsp;0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;milk
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;buy&nbsp;milk
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">After outlining a strategy for managing synchronization in the next section, we define locks and condition variables (another type of synchronization object) in Sections&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-490003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-540004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-45001r71 name=x1-45001r71></A></P><A id=x1-460002 name=x1-460002>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.2 Structuring Shared Objects</FONT></H3></A><A id=x1-460011 name=x1-460011></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00400.gif" data-calibre-src="OEBPS/Images/image00400.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.1: </B>In a multi-threaded program, threads are separate from and operate concurrently on shared objects. Shared objects contain both shared state and synchronization variables, used for controlling concurrent access to shared state.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
Decades of work have developed a much simpler approach to writing multi-threaded programs than using just atomic loads and stores. This approach extends the modularity of object-oriented programming to multi-threaded programs. As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-460011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates, a multi-threaded program is built using <EM>shared objects</EM> and a set of threads that operate on them. </FONT>
<P><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:shared object"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Shared objects</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> are objects that can be accessed safely by multiple threads. All shared state in a program &#8212; including variables allocated on the heap (e.g., objects allocated with malloc or new) and static, global variables &#8212; should be encapsulated in one or more shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Programming with shared objects extends traditional object-oriented programming, in which objects hide their implementation details behind a clean interface. In the same way, shared objects hide the details of synchronizing the actions of multiple threads behind a clean interface. The threads using shared objects need only understand the interface; they do not need to know how the shared object internally handles synchronization. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Like regular objects, programmers can design shared objects for whatever modules, interfaces, and semantics an application needs. Each shared object&#8217;s class defines a set of public methods on which threads operate. To assemble the overall program from these shared objects, each thread executes a &#8220;main loop&#8221; written in terms of actions on public methods of shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since shared objects encapsulate the program&#8217;s shared state, the main loop code that defines a thread&#8217;s high-level actions need not concern itself with synchronization details. The programming model thus looks very similar to that for single-threaded code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Shared objects, monitors, and syntactic sugar</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We focus on <EM>shared objects</EM> because object-oriented programming provides a good way to think about shared state: hide shared state behind public methods that provide a clean interface to threads and that handle the details of synchronization. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although we use object-oriented terminology in our discussion, the ideas are equally applicable to non-object-oriented languages. For example, where a C++ program might define a class of shared objects with public methods, a C program might define a struct with synchronization variables and state variables as fields. Rather than scattering the code that accesses the struct&#8217;s fields, a well-designed C program will have a fixed set of functions that operate on the struct&#8217;s fields. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Conversely, some programming languages build in even more support for shared objects than we describe here. When a programming language includes support for shared objects, a shared object is often called a <EM>monitor</EM>. Early languages with monitors include Brinch Hansen&#8217;s Concurrent Pascal and Xerox PARC&#8217;s Mesa; today, Java supports monitors via the synchronized keyword. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We regard the distinctions between procedural languages, object-oriented languages, and languages with built-in support for monitors as relatively unimportant syntactic sugar &#8212; they are just a different way of writing the same thing. We use the terms &#8220;shared objects&#8221; or &#8220;monitors&#8221; broadly to refer to a conceptual approach that can and should be used to manage concurrency regardless of the particular programming language. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this book, our code and pseudo-code are based on C++&#8217;s syntax. We believe provides the right level of detail for teaching the shared objects or monitors approach. We prefer teaching with C++ to Java because we want to explicitly show where locks and condition variables are allocated and accessed rather than relying on operations hidden by a language&#8217;s built in monitor syntax. Conversely, we prefer C++ to C because we think C++&#8217;s support for object-oriented programming may help you internalize the underlying philosophy of the shared object approach. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-46002r76 name=x1-46002r76></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.2.1 </FONT><A id=x1-470001 name=x1-470001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Shared Objects</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, internally the shared objects must handle the details of synchronization. As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-470012"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows, shared objects are implemented in layers. </FONT><A id=x1-470012 name=x1-470012></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00401.gif" data-calibre-src="OEBPS/Images/image00401.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.2: </B>Multi-threaded programs are built with shared objects. Shared objects are built using synchronization variables and state variables. Synchronization variables are implemented using specialized processor instructions to manage interrupt delivery and to atomically read-modify-write memory locations.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Shared object layer.</B> As in standard object-oriented programming, shared objects define application-specific logic and hide internal implementation details. Externally, they appear to have the same interface as you would define for a single-threaded program. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Synchronization variable layer.</B> Rather than implementing shared objects directly with carefully interleaved atomic loads and stores, shared objects include <EM>synchronization variables</EM> as member variables. Synchronization variables, stored in memory just like any other object, can be included in any data structure. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:synchronization variable"}'>synchronization variable</A></EM> is a data structure used for coordinating concurrent access to shared state. Both the interface and the implementation of synchronization variables must be carefully designed. In particular, we build shared objects using two types of synchronization variables: <EM>locks</EM> and <EM>condition variables.</EM> We define these and describe how to construct them in Sections&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-490003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-540004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Synchronization variables coordinate access to <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:state variable"}'>state variables</A></EM>, which are just the normal member variables of an object that you are familiar with from single-threaded programming (e.g., integers, strings, arrays, and pointers). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using synchronization variables simplifies implementing shared objects. In fact, not only do shared objects externally resemble traditional single-threaded objects, but, by implementing them with synchronization variables, their internal implementations are quite similar to those of single-threaded programs. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Atomic instruction layer.</B> Although the layers above benefit from a simpler programming model, it is not turtles all the way down. Internally, synchronization variables must manage the interleavings of different threads&#8217; actions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Rather than implementing synchronization variables, such as locks and condition variables, using atomic loads and stores as we tried to do for the Too Much Milk problem, modern implementations build synchronization variables using <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:atomic read-modify-write instruction"}'>atomic read-modify-write instructions</A></EM>. These processor-specific instructions let one thread have temporarily exclusive and atomic access to a memory location while the instruction executes. Typically, the instruction atomically reads a memory location, does some simple arithmetic operation to the value, and stores the result. The hardware guarantees that any other thread&#8217;s instructions accessing the same memory location will occur either entirely before, or entirely after, the atomic read-modify-write instruction. </FONT></P></LI></UL><A id=x1-47002r79 name=x1-47002r79></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.2.2 </FONT><A id=x1-480002 name=x1-480002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scope and Roadmap</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-470012"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> indicates, concurrent programs are built on top of shared objects. The rest of this chapter focuses on the middle layers of the figure &#8212; how to build shared objects using synchronization objects and how to build synchronization objects out of atomic read-modify-write instructions. Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-780006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> discusses issues that arise when composing multiple shared objects into a larger program. </FONT><A id=x1-48001r77 name=x1-48001r77></A><A id=x1-490003 name=x1-490003>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.3 Locks: Mutual Exclusion</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:lock"}'>lock</A></EM> is a synchronization variable that provides <EM>mutual exclusion</EM> &#8212; when one thread holds a lock, no other thread can hold it (i.e., other threads are <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:mutual exclusion"}'>excluded</A></EM>). A program associates each lock with some subset of shared state and requires a thread to hold the lock when accessing that state. Then, only one thread can access the shared state at a time. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Mutual exclusion greatly simplifies reasoning about programs because a thread can perform an arbitrary set of operations while holding a lock, and those operations <EM>appear to be atomic</EM> to other threads. In particular, because a lock enforces mutual exclusion and threads must hold the lock to access shared state, no other thread can observe an intermediate state. Other threads can only observe the state left after the lock release. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: Locking to group multiple operations.</B> Consider, for example, a bank account object that includes a list of transactions and a total balance. To add a new transaction, we acquire the account&#8217;s lock, append the new transaction to the list, read the old balance, modify it, write the new balance, and release the lock. To query the balance and list of recent transactions, we acquire the account&#8217;s lock, read the recent transactions from the list, read the balance, and release the lock. Using locks in this way guarantees that one update or query completes before the next one starts. Every query always reflects the complete set of recent transactions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another example of grouping is when printing output. Without locking, if two threads called printf at the same time, the individual characters of the two messages could be interleaved, garbling their meaning. Instead, on modern multi-threaded operating systems, printf uses a lock to ensure that the group of characters in each message prints as a unit. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is much easier to reason about interleavings of atomic groups of operations rather than interleavings of individual operations for two reasons. First, there are (obviously) fewer interleavings to consider. Reasoning about interleavings on a coarser-grained basis reduces the sheer number of cases to consider. Second, and more important, we can make each atomic group of operations correspond to the logical structure of the program, which allows us to reason about <EM>invariants</EM> not specific <EM>interleavings</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In particular, shared objects usually have one lock guarding all of an object&#8217;s state. Each public method acquires the lock on entry and releases the lock on exit. Thus, reasoning about a shared class&#8217;s code is similar to reasoning about a traditional class&#8217;s code: we assume a set of invariants when a public method is called and re-establish those invariants before a public method returns. If we define our invariants well, we can then reason about each method independently. </FONT><A id=x1-49001r81 name=x1-49001r81></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.3.1 </FONT><A id=x1-500001 name=x1-500001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Locks: API and Properties</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A lock enables mutual exclusion by providing two methods: Lock::acquire() and Lock::release(). These methods are defined as follows: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A lock can be in one of two states: BUSY&nbsp;or FREE. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A lock is initially in the FREE&nbsp;state. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Lock::acquire waits until the lock is FREE&nbsp;and then atomically makes the lock BUSY. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Checking the state to see if it is FREE&nbsp;and setting the state to BUSY&nbsp;are together an <EM>atomic operation</EM>. Even if multiple threads try to acquire the lock, at most one thread will succeed. One thread observes that the lock is FREE&nbsp;and sets it to BUSY; the other threads just see that the lock is BUSY&nbsp;and wait. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Lock::release makes the lock FREE. If there are pending <TT>acquire</TT>&nbsp;operations, this state change causes one of them to proceed. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We describe how to implement locks with these properties in Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-660007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Using locks makes solving the Too Much Milk problem trivial. Both threads run the following code: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;lock.acquire();
   &nbsp;if&nbsp;(milk&nbsp;==&nbsp;0)&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;no&nbsp;milk
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;milk++;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;buy&nbsp;milk
   &nbsp;}
   &nbsp;lock.release();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Many routines in an operating system kernel need to allocate and de-allocate memory blocks. Assuming you are given the code for a single-threaded kernel memory allocator, explain how to implement a thread-safe memory allocator. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Using C malloc and free as an example, we can convert them to be thread-safe by acquiring a lock before accessing the heap, and releasing it after the block has been allocated or freed. Since malloc and free read and modify the same data structures, it is essential to use the <EM>same</EM> lock in both procedures, heaplock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;char&nbsp;*malloc&nbsp;(int&nbsp;n)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;char&nbsp;*p;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;heaplock.acquire();
&nbsp;&nbsp;//&nbsp;Code&nbsp;for&nbsp;single-threaded&nbsp;malloc()
&nbsp;&nbsp;//&nbsp;p&nbsp;=&nbsp;allocate&nbsp;block&nbsp;of&nbsp;memory
&nbsp;&nbsp;//&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;size&nbsp;n.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;heaplock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;p;
&nbsp;}
 </FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;void&nbsp;free&nbsp;(char&nbsp;*p)&nbsp;{
&nbsp;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;heaplock.acquire();
&nbsp;&nbsp;//&nbsp;Code&nbsp;for&nbsp;single-threaded&nbsp;free()
&nbsp;&nbsp;//&nbsp;Put&nbsp;p&nbsp;back&nbsp;on&nbsp;free&nbsp;list.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;heaplock.release();
&nbsp;
&nbsp;}
 </FONT></PRE>
<P><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Formal properties.</B> A lock can be defined more precisely as follows. A thread <EM>holds a lock</EM> if it has returned from a lock&#8217;s <TT>acquire</TT>&nbsp;method more often than it has returned from a lock&#8217;s <TT>release</TT>&nbsp;method. A thread <EM>is attempting to acquire</EM> a lock if it has called but not yet returned from a call to <TT>acquire</TT>&nbsp;on the lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A lock should ensure the following three properties: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-50002x1 name=x1-50002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Mutual Exclusion.</B> At most one thread holds the lock. </FONT></P>
<LI class=enumerate><A id=x1-50004x2 name=x1-50004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Progress.</B> If no thread holds the lock and any thread attempts to acquire the lock, then eventually some thread succeeds in acquiring the lock. </FONT></P>
<LI class=enumerate><A id=x1-50006x3 name=x1-50006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bounded waiting.</B> If thread T attempts to acquire a lock, then there exists a bound on the number of times other threads can successfully acquire the lock before T does. </FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Mutual exclusion is a safety property because locks prevent more than one thread from accessing shared state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Progress and bounded waiting are liveness properties. If a lock is FREE, <EM>some</EM> thread must be able to acquire it. Further, any <EM>particular</EM> thread that wants to acquire the lock must eventually succeed in doing so. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If these definitions sound stilted, it is because we have carefully crafted them to avoid introducing subtle corner cases. For example, if a thread holding a lock never releases it, other threads cannot make progress, so we define the <EM>bounded waiting</EM> condition in terms of successful <TT>acquire</TT>&nbsp;operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: <B>Non-property: Thread ordering.</B> The <EM>bounded waiting</EM> property defined above guarantees that a thread will eventually get a chance to acquire the lock. However, it does not promise that waiting threads acquire the lock in FIFO order. Most implementations of locks that you will encounter &#8212; for example with POSIX threads &#8212; do not provide FIFO ordering. </FONT><A id=x1-50007r83 name=x1-50007r83></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.3.2 </FONT><A id=x1-510002 name=x1-510002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Thread-Safe Bounded Queue</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">As in standard object-oriented programming, each shared object is an instance of a class that defines the class&#8217;s state and the methods that operate on that state. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The class&#8217;s state includes both state variables (e.g., ints, floats, strings, arrays, and pointers) and synchronization variables (e.g., locks). Every time a class constructor produces another instance of a shared object, it allocates both a new lock and new instances of the state protected by that lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:bounded queue"}'>bounded queue</A></EM> is a queue with a fixed size limit on the number of items stored in the queue. Operating system kernels use bounded queues for managing interprocess communication, TCP and UDP sockets, and I/O requests. Because the kernel runs in a finite physical memory, the kernel must be designed to work properly with finite resources. For example, instead of a simple, infinite buffer between a producer and a consumer thread, the kernel will instead use a limited size buffer, or bounded queue. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:thread-safe bounded queue"}'>thread-safe bounded queue</A></EM> is a type of a bounded queue that is safe to call from multiple concurrent threads. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-510013"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> gives an implementation; it lets any number of threads safely insert and remove items from the queue. As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-510024"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates, a program can allocate multiple such queues (e.g., queue1, queue2, and queue3), each of which includes its own lock and state variables. </FONT><A id=x1-510013 name=x1-510013></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Thread-safe&nbsp;queue&nbsp;interface
&nbsp;
&nbsp;const&nbsp;int&nbsp;MAX&nbsp;=&nbsp;10;
&nbsp;
&nbsp;class&nbsp;TSQueue&nbsp;{
&nbsp;&nbsp;&nbsp;//&nbsp;Synchronization&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;
&nbsp;&nbsp;&nbsp;//&nbsp;State&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;items[MAX];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;front;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;nextEmpty;
&nbsp;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TSQueue();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~TSQueue(){};
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;tryInsert(int&nbsp;item);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;tryRemove(int&nbsp;*item);
&nbsp;};
</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Initialize&nbsp;the&nbsp;queue&nbsp;to&nbsp;empty
&nbsp;//&nbsp;and&nbsp;the&nbsp;lock&nbsp;to&nbsp;free.
&nbsp;TSQueue::TSQueue()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;front&nbsp;=&nbsp;nextEmpty&nbsp;=&nbsp;0;
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Try&nbsp;to&nbsp;insert&nbsp;an&nbsp;item.&nbsp;If&nbsp;the&nbsp;queue&nbsp;is
&nbsp;//&nbsp;full,&nbsp;return&nbsp;false;&nbsp;otherwise&nbsp;return&nbsp;true.
&nbsp;bool
&nbsp;TSQueue::tryInsert(int&nbsp;item)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;success&nbsp;=&nbsp;false;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;((nextEmpty&nbsp;-&nbsp;front)&nbsp;&lt;&nbsp;MAX)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;items[nextEmpty&nbsp;%&nbsp;MAX]&nbsp;=&nbsp;item;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nextEmpty++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;success&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;success;
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Try&nbsp;to&nbsp;remove&nbsp;an&nbsp;item.&nbsp;If&nbsp;the&nbsp;queue&nbsp;is
&nbsp;//&nbsp;empty,&nbsp;return&nbsp;false;&nbsp;otherwise&nbsp;return&nbsp;true.
&nbsp;bool
&nbsp;TSQueue::tryRemove(int&nbsp;*item)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;success&nbsp;=&nbsp;false;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(front&nbsp;&lt;&nbsp;nextEmpty)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*item&nbsp;=&nbsp;items[front&nbsp;%&nbsp;MAX];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;front++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;success&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;success;
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.3: </B>A thread-safe bounded queue. For implementation simplicity, we assume the queue stores integers (rather than arbitrary objects) and the total number of items stored is modest.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-510024 name=x1-510024></A>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00402.gif" data-calibre-src="OEBPS/Images/image00402.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.4: </B>Three shared objects, each an instance of class TSQueue.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The queue stores only a fixed number, MAX, of items. When the queue is full, an insert request returns an error. Similarly, when the queue is empty, a remove request returns an error. Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-540004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows how <EM>condition variables</EM> let the calling thread <EM>wait</EM> instead of returning an error. On insert, the thread waits until the queue has space to store the item and, on remove, it waits until the queue has at least one item queued before returning it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The TSQueue implementation defines a circular queue that stores data in a fixed size array, items[MAX]. The state variable, front is the next item in the queue to be removed, if any; nextEmpty is the next location for a new item, if any. To keep the example as simple as possible, only items of type int can be stored in and removed from the queue, and we assume the total number of items stored fits within a 64 bit integer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">All of these variables are as they would be for a single-threaded version of this object. The lock allows tryInsert and tryRemove to atomically read and write multiple variables just as a single-threaded version would. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What constraints are true of TSQueue at the moment immediately after the lock is acquired? What constraints hold immediately before the lock is released? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Because the lock enforces mutual exclusion and is always held whenever a thread modifies a state variable, when the lock is acquired the object&#8217;s state variables must be either: (i) in the initial state or (ii) in the state left by a previous thread when it released the lock. These constraints are the same as for single-threaded code using a bounded queue: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The total number of items ever inserted in the queue is nextEmpty. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The total number of items ever removed from the queue is front. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">front &lt;= nextEmpty </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The current number of items in the queue is nextEmpty - front. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">nextEmpty - front &lt;= MAX</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The lock holder always re-establishes these constraints before releasing the lock. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Are these constraints also true if the lock is not held? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>No.</B> It seems intuitive that if the constraints hold immediately before the lock is released, then they must also hold immediately after the lock is released. However, this is not the case. In the meantime, some other thread may have acquired the lock and may be in the process of modifying the state variables. In general, if the lock is not held, one cannot say <EM>anything</EM> about the object&#8217;s state variables. &#9633; </FONT></P>
<H5 class=subsubsectionHead><A id=x1-520002 name=x1-520002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Critical Sections</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:critical section"}'>critical section</A></EM> is a sequence of code that atomically accesses shared state. By ensuring that a thread holds the object&#8217;s lock while executing any of its critical sections, we ensure that each critical section appears to execute atomically on its shared state. There is a critical section in each of the methods tryInsert and tryRemove. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice two things: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Each class can define multiple methods that operate on the shared state defined by the class, so there may be <EM>multiple critical sections per class</EM>. However, for each instance of the class (i.e., for each object), only one thread holds the object&#8217;s lock at a time, so <EM>only one thread actively executes any of the critical sections per shared object instance.</EM> For the TSQueue class, if one thread calls queue1.tryInsert and another calls queue1.tryRemove, the insert occurs either before the remove or vice versa. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A program can create <EM>multiple instances of a class</EM>. Each instance is a shared object, and each shared object has its own lock. Thus, different threads may be active in the critical sections for different shared object instances. For the TSQueue class, if one thread calls queue1.tryInsert, another thread calls queue2.tryRemove, and a third thread calls queue3.tryInsert, all three threads may be simultaneously executing critical section code operating on <EM>different instances</EM> of the TSQueue class. </FONT></P></LI></UL>
<H5 class=subsubsectionHead><A id=x1-530002 name=x1-530002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Using Shared Objects</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Shared objects are allocated in the same way as other objects. They can be dynamically allocated from the heap using malloc and new, or they can be statically allocated in global memory by declaring static variables in the program. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multiple threads must be able to access shared objects. If shared objects are global variables, then a thread&#8217;s code can refer to an object&#8217;s global name to reference it; the compiler computes the corresponding address. If shared objects are dynamically allocated, then each thread that uses an object needs a pointer or reference to it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Two common ways to provide a thread a pointer to a shared object are: (1) provide a pointer to the shared object when the thread is created, and (2) store references to shared objects in other shared objects (e.g., containers). For example, a program might have a global, shared (and synchronized!) hash table that threads can use to store and retrieve references to other shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-530015"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows a simple program that creates three queues and then creates some threads that insert into these queues. It then removes 20 items from each queue and prints the values it removes. The initial main thread allocates the shared queues on the heap using new, and provides each worker thread a pointer to one of the shared queues. </FONT><A id=x1-530015 name=x1-530015></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;TSQueueMain.cc
&nbsp;//&nbsp;&nbsp;&nbsp;Test&nbsp;code&nbsp;for&nbsp;TSQueue.
&nbsp;
&nbsp;int&nbsp;main(int&nbsp;argc,&nbsp;char&nbsp;**argv)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TSQueue&nbsp;*queues[3];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sthread_t&nbsp;workers[3];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;i,&nbsp;j;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Start&nbsp;worker&nbsp;threads&nbsp;to&nbsp;insert.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;3;&nbsp;i++)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;queues[i]&nbsp;=&nbsp;new&nbsp;TSQueue();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_create_p(&amp;workers[i],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;putSome,&nbsp;queues[i]);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Wait&nbsp;for&nbsp;some&nbsp;items&nbsp;to&nbsp;be&nbsp;put.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_join(workers[0]);
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Remove&nbsp;20&nbsp;items&nbsp;from&nbsp;each&nbsp;queue.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;3;&nbsp;i++)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("Queue&nbsp;%d:\n",&nbsp;i);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;testRemoval(&amp;queues[i]);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Insert&nbsp;50&nbsp;items&nbsp;into&nbsp;a&nbsp;queue.
&nbsp;void&nbsp;*putSome(void&nbsp;*p)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TSQueue&nbsp;*queue&nbsp;=&nbsp;(TSQueue&nbsp;*)p;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;i;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;50;&nbsp;i++)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;queue-&gt;tryInsert(i);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;NULL;
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Remove&nbsp;20&nbsp;items&nbsp;from&nbsp;a&nbsp;queue.
&nbsp;void&nbsp;testRemoval(TSQueue&nbsp;*queue)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;i,&nbsp;item;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(i&nbsp;=&nbsp;0;&nbsp;i&nbsp;&lt;&nbsp;20;&nbsp;j++)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(queue-&gt;tryRemove(&amp;item))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("Removed&nbsp;%d\n",&nbsp;item);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;printf("Nothing&nbsp;there.\n");
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.5: </B>This code creates three TSQueue objects and then adds and removes some items from these queues. We use thread_create_p instead of thread_create so that we can pass to the newly created thread a pointer to the queue it should use.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: <B>Put shared objects on the heap, not the stack.</B> While nothing prevents you from writing a program that allocates a shared object as an automatic variable in a procedure or method, you should not write programs that do this. The compiler allocates automatic variables (sometimes called &#8220;local variables&#8221;, with good reason) on the stack during procedure invocation. If one thread passes a pointer or reference to one of its automatic variables to another thread and later returns from the procedure where the automatic variable was allocated, then that second thread now has a pointer into a region of the first thread&#8217;s stack that may be used for other purposes. To prevent this error, a few garbage-collected languages, such as Google&#8217;s Go, automatically convert all automatic data to being heap-allocated if the data can be referenced outside of the procedure. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You might be tempted to argue that, for a particular program, you know that the procedure will never return until all of the threads with which it is sharing an object are done using that object, and that therefore sharing one of the procedure&#8217;s local variables is safe. The problem with this argument is that the code may change over time, introducing a dangerous and subtle bug. When sharing dynamically allocated variables, it is best to stay in the habit of sharing variables only from the heap &#8212; and never sharing variables from the stack &#8212; across threads. </FONT><A id=x1-53002r82 name=x1-53002r82></A></P><A id=x1-540004 name=x1-540004>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.4 Condition Variables: Waiting for a Change</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Condition variables provide a way for one thread to wait for another thread to take some action. For example, in the thread-safe queue example in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-510013"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, rather than returning an error when we try to remove an item from an empty queue, we might wait until the queue is non-empty, and then always return an item. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Similarly, a web server might wait until a new request arrives; a word processor might wait for a key to be pressed; a weather simulator&#8217;s coordinator thread might wait for the worker threads calculating temperatures in each region to finish; or, in our </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-110001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Earth Visualizer</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> example, a thread in charge of rendering part of the screen might wait for a user command or for new data to update the view. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In all of these cases, we want a thread to wait for some action to change the system state so that the thread can make progress. </FONT><A id=x1-540016 name=x1-540016></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;int
&nbsp;TSQueue::remove()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;item;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;success;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;success&nbsp;=&nbsp;tryRemove(&amp;item);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;until(success);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;item;
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.6: </B>A polling-based implementation of TSQueue::remove. The code retries in a loop until it succeeds in removing an item.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One way for a thread to wait would be to poll &#8212; to repeatedly check the shared state to see if it has changed. As shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-540016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, a polling implementation of remove would have a simple wrapper that repeatedly calls tryRemove until it returns success. Unfortunately, this approach is inefficient: the waiting thread continually loops, or busy-waits, consuming processor cycles without making useful progress. Worse, busy-waiting can delay the scheduling of other threads &#8212; perhaps exactly the thread for which the looping thread is waiting. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>The sleep fix?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We often find that students want to &#8220;fix&#8221; the polling-based approach by adding a delay. For example, in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-540016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, we could add a call to sleep to yield the processor for (say) 100 ms after each unsuccessful tryRemove call. This would allow some other thread to run while the waiting thread is waiting. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This approach has two problems. First, although it reduces the inefficiency of polling, it does not eliminate it. Suspending and scheduling a thread imposes non-trivial overheads, and a program with many polling threads would still waste significant resources. Second, periodic polling adds latency. In our Earth Visualizer example, if the thread waiting for keyboard input waited 100 ms between each check, the application might become noticeably more sluggish. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As an extreme example, one of the authors once had an employee implement a network server that provided several layers of processing, where each layer had a thread that received work from the layer above and sent the work to the layer below. Measurements of the server showed surprisingly bad performance; we expected each request to take a few milliseconds, but instead each took just over half a second. Fortunately, the performance was so poor that it was easy to track down the problem: layers passed work to each other through bounded queues much like TSQueue, but the queue remove method was implemented as a polling loop with a 100 ms delay. With five such layers of processing, the server became unusable. Fortunately, the fix was simple: use condition variables instead. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-54002r84 name=x1-54002r84></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.4.1 </FONT><A id=x1-550001 name=x1-550001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Condition Variable Definition</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:condition variable"}'>condition variable</A></EM> is a synchronization object that lets a thread efficiently wait for a change to shared state that is protected by a lock. A condition variable has three methods: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>CV::wait(Lock *lock).</B> This call atomically <EM>releases the lock</EM> and <EM>suspends execution of the calling thread</EM>, placing the calling thread on the condition variable&#8217;s waiting list. Later, when the calling thread is re-enabled, it <EM>re-acquires the lock</EM> before returning from the <TT>wait</TT>&nbsp;call. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>CV::signal().</B> This call takes one thread off the condition variable&#8217;s waiting list and marks it as eligible to run (i.e., it puts the thread on the scheduler&#8217;s ready list). If no threads are on the waiting list, <TT>signal</TT>&nbsp;has no effect. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>CV::broadcast().</B> This call takes all threads off the condition variable&#8217;s waiting list and marks them as eligible to run. If no threads are on the waiting list, <TT>broadcast</TT>&nbsp;has no effect. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: Note that condition variable <TT>wait</TT>&nbsp;and <TT>signal</TT>&nbsp;are different from the UNIX system calls wait and signal. The nomenclature is unfortunate but longstanding. In this book, we always use the terms, UNIX wait and UNIX signal, to refer to the UNIX variants, and simple <TT>wait</TT>&nbsp;and <TT>signal</TT>&nbsp;to refer to condition variable operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A condition variable is used to wait for a change to shared state, and a lock must always protect updates to shared state. Thus, the condition variable API is designed to work in concert with locks. All three methods (<TT>wait</TT>, <TT>signal</TT>, and <TT>broadcast</TT>) should only be called while the associated lock is held. </FONT><A id=x1-550017 name=x1-550017></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;SharedObject::someMethodThatWaits()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;
&nbsp;//&nbsp;Read&nbsp;and/or&nbsp;write&nbsp;shared&nbsp;state&nbsp;here.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(!testOnSharedState())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv.wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert(testOnSharedState());
&nbsp;
&nbsp;//&nbsp;Read&nbsp;and/or&nbsp;write&nbsp;shared&nbsp;state&nbsp;here.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;SharedObject::someMethodThatSignals()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;
&nbsp;//&nbsp;Read&nbsp;and/or&nbsp;write&nbsp;shared&nbsp;state&nbsp;here.
&nbsp;
&nbsp;//&nbsp;If&nbsp;state&nbsp;has&nbsp;changed&nbsp;in&nbsp;a&nbsp;way&nbsp;that
&nbsp;//&nbsp;could&nbsp;allow&nbsp;another&nbsp;thread&nbsp;to&nbsp;make
&nbsp;//&nbsp;progress,&nbsp;signal&nbsp;(or&nbsp;broadcast).
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv.signal();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.7: </B>Design patterns for waiting using a condition variable (top) and for waking up a waiter (bottom). Since many critical sections need to both <TT>wait</TT>&nbsp;and <TT>signal</TT>, these two design patterns are often combined in one method. </FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The standard design pattern for a shared object is a lock and zero or more condition variables. A method that waits using a condition variable works as shown on the top in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-550017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. In this code, the calling thread first acquires the lock and can then read and write the shared object&#8217;s state variables. To wait until testOnSharedState succeeds, the thread calls <TT>wait</TT>&nbsp;on the shared object&#8217;s condition variable cv. This atomically puts the thread on the waiting list and releases the lock, allowing other threads to enter the critical section. Once the waiting thread is signaled, it re-acquires the lock and returns from <TT>wait</TT>. The monitor can then safely test the state variables to see if testOnSharedState succeeds. If so, the monitor performs its tasks, releases the lock, and returns. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The bottom of Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-550017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the complementary code that causes a waiting thread to wake up. Whenever a thread changes the shared object&#8217;s state in a way that enables a waiting thread to make progress, the thread must signal the waiting thread using the condition variable. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A thread waiting on a condition variable must inspect the object&#8217;s state in a loop. The condition variable&#8217;s <TT>wait</TT>&nbsp;method releases the lock (to let other threads change the state of interest) and then re-acquires the lock (to check that state again). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Similarly, the only reason for a thread to <TT>signal</TT>&nbsp;(or <TT>broadcast</TT>) is that it has just changed the shared state in a way that may be of interest to a waiting thread. To make a change to shared state, the thread must hold the lock on the state variables, so <TT>signal</TT>&nbsp;and <TT>broadcast</TT>&nbsp;are also always called while holding a lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Discussion.</B> Condition variables have been carefully designed to work in tandem with locks and shared state. The precise definition of condition variables includes three properties worth additional comment: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A condition variable is <EM>memoryless</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The condition variable, itself, has no internal state other than a queue of waiting threads. Condition variables do not need their own state because they are always used inside shared objects that have their own state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If no threads are currently on the condition variable&#8217;s waiting list, a <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;has no effect. No thread calls <TT>wait</TT>&nbsp;unless it holds the lock, checks the state variables, and finds that it needs to wait. Thus, the condition variable has no &#8220;memory&#8221; of earlier calls to <TT>signal</TT>&nbsp;or <TT>broadcast</TT>. After <TT>signal</TT>&nbsp;is called, if sometime later another thread calls <TT>wait</TT>, it will block until the <EM>next</EM> <TT>signal</TT>&nbsp;(or <TT>broadcast</TT>) is called, regardless of how many times <TT>signal</TT>&nbsp;has been called in the past. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><EM>CV::wait atomically releases the lock.</EM> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A thread always calls <TT>wait</TT>&nbsp;while holding a lock. The call to <TT>wait</TT>&nbsp;<EM>atomically</EM> releases the lock and puts the thread on the condition variable&#8217;s waiting list. Atomicity ensures that there is no separation between checking the shared object&#8217;s state, deciding to wait, adding the waiting thread to the condition variable&#8217;s queue, and releasing the lock so that some other thread can access the shared object. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If threads released the lock before calling <TT>wait</TT>, they could miss a signal or broadcast and wait forever. Consider the case where thread T<SUB>1</SUB> checks an object&#8217;s state and decides to wait, so it releases the lock in anticipation of putting itself on the condition variable&#8217;s waiting list. At that precise moment, T<SUB>2</SUB> preempts T<SUB>1</SUB>. T<SUB>2</SUB> acquires the lock, changes the object&#8217;s state to what T<SUB>1</SUB> wants, and calls <TT>signal</TT>, but the waiting list is empty so the call to <TT>signal</TT>&nbsp;has no effect. Finally, T<SUB>1</SUB> runs again, puts itself on the waiting list, and suspends execution. The lack of atomicity means that T<SUB>1</SUB> missed the signal and is now waiting, potentially forever. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Once <TT>wait</TT>&nbsp;releases the lock, any number of threads might run before <TT>wait</TT>&nbsp;re-acquires the lock after a <TT>signal</TT>. In the meantime, the state variables might have changed &#8212; in fact, they are almost <EM>certain</EM> to have changed. Code must not assume just because something was true before <TT>wait</TT>&nbsp;was called, it remains true when <TT>wait</TT>&nbsp;returns. The only assumption you should make on return from <TT>wait</TT>&nbsp;is that the lock is held, and the normal invariants that hold at the start of the critical section are true. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When a waiting thread is re-enabled via <TT>signal</TT>&nbsp;or <TT>broadcast</TT>, <EM>it may not run immediately</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When a waiting thread is re-enabled, it is moved to the scheduler&#8217;s ready queue with no special priority, and the scheduler may run it at some later time. Furthermore, when the thread finally does run, it must re-acquire the lock, which means that other threads may have acquired and released the lock in the meantime, between when the signal occurs and when the waiter re-acquires the lock. Therefore, even if the desired predicate were true when <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;was called, it may no longer be true when <TT>wait</TT>&nbsp;returns. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This may seem like a small window of vulnerability, but concurrent programs must work with all possible schedules. Otherwise, programs may fail sometimes, but not always, making debugging very difficult. See the sidebar on Mesa vs. Hoare semantics for a discussion of the history behind this property. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: The points above have an important implication for programmers: <TT>wait</TT>&nbsp;<EM>must always be called from within a loop</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because <TT>wait</TT>&nbsp;releases the lock, and because there is no guarantee of atomicity between <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;and the return of a call to <TT>wait</TT>, there is no guarantee that the checked-for state still holds. Therefore, a waiting thread must always wait in a loop, rechecking the state until the desired predicate holds. Thus, the design pattern is: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;...
   &nbsp;while&nbsp;(predicateOnStateVariables(...))&nbsp;{
   &nbsp;&nbsp;&nbsp;wait(&amp;lock);
   &nbsp;}
   &nbsp;...</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">and not: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;...
   &nbsp;if&nbsp;(predicateOnStateVariables(...))&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wait(&amp;lock);
   &nbsp;}
   &nbsp;...</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are two fundamental reasons why condition variables impose this requirement: to simplify the implementation and to improve modularity. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Simplifying the implementation.</B> When a waiting thread is re-enabled, it may not run immediately. Other threads may access the shared state before it runs, and the desired predicate on the shared state may no longer hold when <TT>wait</TT>&nbsp;finally does return. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This behavior simplifies the implementation of condition variables without increasing the complexity of the code that uses them. No special code is needed for scheduing; <TT>signal</TT>&nbsp;puts the signaled thread onto the ready list and lets the scheduler choose when to run it. Similarly, no special code is needed to re-acquire the lock at the end of <TT>wait</TT>. The woken thread calls <TT>acquire</TT>&nbsp;when it is re-scheduled. As with any attempt to acquire a lock, it may succeed immediately, or it may wait if some other thread acquired the lock first. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some implementations go even further and warn that a call to <TT>wait</TT>&nbsp;may return even if no thread has called <TT>signal</TT>&nbsp;or <TT>broadcast</TT>. So, not only is it possible that the desired predicate on the state <EM>is no longer true</EM>, it is possible that the desired predicate on the state <EM>was never true.</EM> For example, the Java definition of condition variables allows for &#8220;spurious wakeups&#8221;: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=makequote><FONT style="BACKGROUND-COLOR: #7be1e1">When waiting upon a Condition, a &#8220;spurious wakeup&#8221; is permitted to occur, in general, as a concession to the underlying platform semantics. This has little practical impact on most application programs as a Condition should always be waited upon in a loop, testing the state predicate that is being waited for. An implementation is free to remove the possibility of spurious wakeups but it is recommended that applications programmers always assume that they can occur and so always wait in a loop.<BR>(From </FONT><A href="https://docs.oracle.com/javase/8/docs/api/"><FONT style="BACKGROUND-COLOR: #7be1e1">https://docs.oracle.com/javase/8/docs/api/</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">)</FONT></DIV>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Improving modularity.</B> Waiting in a loop that checks the shared state makes shared objects&#8217; code more modular because we can reason about when the thread will continue by looking only at the <TT>wait</TT>&nbsp;loop. In particular, we do not need to examine the rest of the shared object&#8217;s code to understand where and why calls to <TT>signal</TT>&nbsp;and <TT>broadcast</TT>&nbsp;are made to know the post-condition for the <TT>wait</TT>&nbsp;loop. For example, in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-550017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, we know the assert call will never fail without having to look at any other code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Not only does waiting in a loop simplify writing and reasoning about the code that waits, it simplifies writing and reasoning about the code that signals or broadcasts. Signaling at the wrong time will never cause a waiting thread to proceed when it should not. Signal and <TT>broadcast</TT>&nbsp;can be regarded as <EM>hints</EM> that it <EM>might</EM> be a good time to proceed; if the hints prove to be wrong, no damage is done. You can always convert a <TT>signal</TT>&nbsp;to a <TT>broadcast</TT>, or add any number of <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;calls, without changing the semantics of a shared object. Avoiding extra <TT>signal</TT>&nbsp;and <TT>broadcast</TT>&nbsp;calls may matter for performance, but not for correctness.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bottom line:</B> Given the range of possible implementations and the modularity benefits, <TT>wait</TT>&nbsp;must always be done from within a loop that tests the desired predicate. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Mesa vs. Hoare semantics</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In modern condition variables, <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;calls take waiting threads from a condition variable&#8217;s waiting list and put them on the ready list. Later, when these threads are scheduled, they may block for some time while they try to re-acquire the lock. Thus, modern condition variables implement what are often called <EM>Mesa Semantics</EM> (for Mesa, an early programming language at Xerox PARC that implemented these semantics). Despite the name, Mesa was not the first system to use &#8220;Mesa&#8221; semantics; Brinch Hansen had proposed their use five years earlier. However, PARC was the first to use Mesa semantics extensively in a very large operating system, and the name stuck. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">C.A.R. &#8220;Tony&#8221; Hoare proposed a different definition for condition variables. Under <EM>Hoare semantics</EM>, when a thread calls <TT>signal</TT>, execution of the signaling thread is suspended, the ownership of the lock is immediately transferred to one of the waiting threads, and execution of that thread is immediately resumed. Later, when the resumed thread releases the lock, ownership of the lock reverts to the signaling thread, whose execution continues. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Under Hoare semantics, signaling is atomic with the resumption of a waiting thread, and a signaled thread may assume that the state has not changed since the signal that woke it up was issued. Under Mesa semantics, waiting is always done in a loop: while (predicate()) {cv.wait(&amp;lock);}. Under Hoare semantics, waiting can be done with a simple conditional: if (predicate()) {cv.wait(&amp;lock);}. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Mesa semantics are much more widely used, but some argue that the atomicity of signaling and resuming a waiting process makes it easier to prove liveness properties of programs under Hoare semantics. If we know that one thread is waiting on a condition, and we do a signal, we know that the waiting thread (and not some other late-arriving thread) will resume and make progress. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The authors of this book come down strongly on the side of Mesa semantics. The modularity advantages of Mesa greatly simplify reasoning about an object&#8217;s core safety properties. For the properties we care most about (i.e., the safety properties that threads proceed only when they are supposed to) and for large programs where modularity matters, Mesa semantics seem vastly preferable. Later in this chapter, we will explain how to implement FIFO queueing with Mesa semantics, for where liveness concerns are paramount. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As a practical matter the debate has been settled: essentially all systems, including both Java and POSIX, use Mesa semantics. We know of no widely used system that implements Hoare semantics. Programmers that assume the weaker Mesa semantics &#8212; always writing while (predicate()) &#8212; will write programs that work under either definition. The overhead of the &#8220;extra&#8221; check of the predicate upon return from wait in a while loop is unlikely to be significant compared to the signaling and scheduling overheads. As a programmer, you will not go wrong if you write your code assuming Mesa semantics. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-55002r92 name=x1-55002r92></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.4.2 </FONT><A id=x1-560002 name=x1-560002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Thread Life Cycle Revisited</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> discussed how a thread can switch between the READY, WAITING, and RUNNING&nbsp;states. We now explain the WAITING&nbsp;state in more detail. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A RUNNING&nbsp;thread that calls <TT>wait</TT>&nbsp;is put in the WAITING&nbsp;state. This is typically implemented by moving the thread control block (TCB) from the ready list to the condition variable&#8217;s list of waiting threads. Later, when some RUNNING&nbsp;thread calls <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;on that condition variable, one (if <TT>signal</TT>) or all (if <TT>broadcast</TT>) of the TCBs on that condition variable&#8217;s waiting list are moved to the ready list. This changes those threads from the WAITING&nbsp;state to the READY&nbsp;state. At some later time, the scheduler selects a READY&nbsp;thread and runs it by moving it to the RUNNING&nbsp;state. Eventually, the signaled thread runs. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Locks are similar. A lock <TT>acquire</TT>&nbsp;on a busy lock puts the caller into the WAITING&nbsp;state, with the caller&#8217;s TCB on a list of waiting TCBs associated with the lock. Later, when the lock owner calls <TT>release</TT>, one waiting TCB is moved to the ready list, and that thread transitions to the READY&nbsp;state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice that threads that are RUNNING&nbsp;or READY&nbsp;have their state located at a pre-defined, &#8220;global&#8221; location: the CPU (for a RUNNING&nbsp;thread) or the scheduler&#8217;s list of ready threads (for a READY&nbsp;thread). However, threads that are WAITING&nbsp;typically have their state located on some per-lock or per-condition-variable queue of waiting threads. Then, a <TT>signal</TT>, <TT>broadcast</TT>, or <TT>release</TT>&nbsp;call can easily find and re-enable a waiting thread for that particular condition variable or lock. </FONT><A id=x1-560018 name=x1-560018></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Thread-safe&nbsp;blocking&nbsp;queue.
&nbsp;
&nbsp;const&nbsp;int&nbsp;MAX&nbsp;=&nbsp;10;
&nbsp;
&nbsp;class&nbsp;BBQ{
&nbsp;&nbsp;&nbsp;//&nbsp;Synchronization&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;itemAdded;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;itemRemoved;
&nbsp;
&nbsp;&nbsp;&nbsp;//&nbsp;State&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;items[MAX];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;front;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;nextEmpty;
&nbsp;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BBQ();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~BBQ()&nbsp;{};
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;insert(int&nbsp;item);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;remove();
&nbsp;};
</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Initialize&nbsp;the&nbsp;queue&nbsp;to&nbsp;empty,
&nbsp;//&nbsp;the&nbsp;lock&nbsp;to&nbsp;free,&nbsp;and&nbsp;the
&nbsp;//&nbsp;condition&nbsp;variables&nbsp;to&nbsp;empty.
&nbsp;BBQ::BBQ()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;front&nbsp;=&nbsp;nextEmpty&nbsp;=&nbsp;0;
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Wait&nbsp;until&nbsp;there&nbsp;is&nbsp;room&nbsp;and
&nbsp;//&nbsp;then&nbsp;insert&nbsp;an&nbsp;item.
&nbsp;void
&nbsp;BBQ::insert(int&nbsp;item)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;((nextEmpty&nbsp;-&nbsp;front)&nbsp;==&nbsp;MAX)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itemRemoved.wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;items[nextEmpty&nbsp;%&nbsp;MAX]&nbsp;=&nbsp;item;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nextEmpty++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itemAdded.signal();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Wait&nbsp;until&nbsp;there&nbsp;is&nbsp;an&nbsp;item&nbsp;and
&nbsp;//&nbsp;then&nbsp;remove&nbsp;an&nbsp;item.
&nbsp;int
&nbsp;BBQ::remove()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;item;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(front&nbsp;==&nbsp;nextEmpty)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itemAdded.wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item&nbsp;=&nbsp;items[front&nbsp;%&nbsp;MAX];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;front++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;itemRemoved.signal();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;item;
&nbsp;}
&nbsp;
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.8: </B>A thread-safe blocking bounded queue using Mesa-style condition variables.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-56002r94 name=x1-56002r94></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.4.3 </FONT><A id=x1-570003 name=x1-570003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Blocking Bounded Queue</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">We can use condition variables to implement a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:blocking bounded queue"}'>blocking bounded queue</A></EM>, one where a thread trying to remove an item from an empty queue will wait until an item is available, and a thread trying to put an item into a full queue will wait until there is room. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-560018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> defines the blocking bounded queue&#8217;s interface and implementation. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As in TSQueue, we acquire and release the lock at the beginning and end of the public methods (e.g., insert and remove). Now, however, we can atomically release the lock and wait if there is no room in insert or no item in remove. Before returning, insert signals on itemAdded since a thread waiting in remove may now be able to proceed; similarly, remove signals on itemRemoved before it returns. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We <TT>signal</TT>&nbsp;rather than <TT>broadcast</TT>&nbsp;because each insert allows at most one remove to proceed, and vice versa. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What invariants hold when <TT>wait</TT>&nbsp;returns in BBQ:remove? Is an item guaranteed to be in the queue? Why or why not? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>Exactly the same invariants hold when <TT>wait</TT>&nbsp;returns as when the thread first acquired the lock.</B> These are the same constraints as listed earlier for the thread-safe (non-blocking) bounded queue TSQueue. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In particular, although there is always an item in the queue when insert calls <TT>signal</TT>, there is <EM>no</EM> guarantee that the item is still in the queue when <TT>wait</TT>&nbsp;returns. Even if the language runtime avoids spurious wakeups, some other thread may have run between the <TT>signal</TT>&nbsp;and the return from <TT>wait</TT>. That thread may perform a remove, acquire the BBQ::lock, find the item, and empty the queue, all before <TT>wait</TT>&nbsp;returns. &#9633; </FONT><A id=x1-57001r90 name=x1-57001r90></A></P><A id=x1-580005 name=x1-580005>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.5 Designing and Implementing Shared Objects</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Although multi-threaded programming has a reputation for being difficult, shared objects provide a basis for writing simple, safe code for multi-threaded programs. In this section, we provide a methodology for writing correct multi-threaded code using shared objects. </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We first define a high-level approach to designing shared objects. Given a concurrent problem, where do you start? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-590001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.5.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We provide six specific rules, or best practices, that you should always follow when writing multi-threaded shared objects. (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-600002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.5.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We describe three common pitfalls to multi-threading in C, C++, and Java code. (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-610003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.5.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">)</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our experience is that following this approach and these rules makes it much more likely that you will write code that is not only correct but also easy for others to read, understand, and maintain. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>On simplicity</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One of the themes running through this textbook is the importance of simple abstractions in building robust, reliable operating systems. Operating systems place a premium on reliability; if the operating system breaks, the computer becomes temporarily unusable, or worse. And yet, it is nearly impossible to fully test whether some piece of multi-threaded operating system code works under all possible conditions and all possible schedule interleavings. This places a premium on designing solutions that work the first time they are run, by keeping code simple. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Particularly with concurrent code, it is not enough for the code to work. It also needs to be simple enough to understand. We often find students write intricate concurrent code in solutions to our homework assignments and exams. Perhaps the difficulty of the topic suggests to students that their solutions must also be difficult to understand! Sometimes these solutions work; more often the complexity hides a design flaw. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even if your code is literally correct, we would like to encourage you to not stop there. Is it easy to understand <EM>why</EM> your code works? If not, try again. Even if you can get the code to work this time, someone else may need to come along later and change it. For concurrent code to be maintainable over time, it is essential that the next developer to work on the code be able to understand it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Yet, often in technology circles, simplicity is considered an insult. Someone might say, &#8220;Anyone could have done that!&#8221;, meaning it as a put down. We take the other side: a simple design should be seen as a complement. Complexity should be introduced only where it is absolutely necessary. Consider three possible states for one of your designs (hat tip to John Ousterhout for this list): </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The code is simple enough that anyone can understand it. If someone says this to you, the appropriate response is to take it as a complement and reply, &#8220;Thank you.&#8221; </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The code is so complicated that only the author can understand it. While this might be useful in the short-term as a strategy to keep the author employed (after all, no one else can fix or improve code without understanding it first), it is not such a good idea over the long term. Eventually, you will want to work on something new! </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The code is so complicated not even the author can understand it. Concurrent code often lands in this category, unnecessarily in our view. Using the rules we introduce in this section will help put your code in the first and not the last bucket.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, writing individual shared objects is not enough. Most programs have multiple shared objects, and new issues arise when combining them. But, before trying to compose multiple shared objects, we must make sure that each individual object works. Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-780006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> discusses the issues that arise when programs use multiple shared objects. </FONT><A id=x1-58001r96 name=x1-58001r96></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.5.1 </FONT><A id=x1-590001 name=x1-590001></A><FONT style="BACKGROUND-COLOR: #7be1e1">High Level Methodology</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A shared object has public methods, private methods, state variables, and synchronization variables; its synchronization variables include a lock and one or more condition variables. At this level, shared object programming resembles standard object-oriented programming, except that we have added synchronization variables to each shared object. This similarity is deliberate: the interfaces to locks and condition variables have been carefully defined so that we can continue to apply familiar techniques for programming and reasoning about objects. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Therefore, most high-level design challenges for a shared object&#8217;s class are the same as for class design in single-threaded programming: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Decompose the problem into objects. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For each object: </FONT></P>
<UL class=itemize2>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Define a clean interface. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Identify the right internal state and invariants to support that interface. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Implement methods with appropriate algorithms to manipulate that state.</FONT></P></LI></UL></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These steps require creativity and sound engineering judgment and intuition. Going from single-threaded to multi-threaded programming does not make these steps much more difficult. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Compared to how you implement a class in a single-threaded program, the new steps needed for the multi-threaded case for shared objects are straightforward: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-59002x1 name=x1-59002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Add a lock. </FONT></P>
<LI class=enumerate><A id=x1-59004x2 name=x1-59004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Add code to <TT>acquire</TT>&nbsp;and <TT>release</TT>&nbsp;the lock. </FONT></P>
<LI class=enumerate><A id=x1-59006x3 name=x1-59006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Identify and add condition variables. </FONT></P>
<LI class=enumerate><A id=x1-59008x4 name=x1-59008x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Add loops to wait using the condition variables. </FONT></P>
<LI class=enumerate><A id=x1-59010x5 name=x1-59010x5></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Add <TT>signal</TT>&nbsp;and <TT>broadcast</TT>&nbsp;calls. </FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We discuss each of these steps in turn. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Other than these fairly mechanical changes, writing the rest of your code proceeds as in the single-threaded case. </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-59012x1 name=x1-59012x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Add a lock.</B> Each shared object needs a lock as a member variable to enforce mutually exclusive access to the object&#8217;s shared state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This chapter focuses on the simple case where each shared object includes exactly one lock. In Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-780006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, we will talk about more advanced variations, such as an </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-830003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">ownership design pattern</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> where higher-level program structure enforces mutual exclusion by ensuring that at most one thread at a time owns and can access an object. </FONT></P>
<LI class=enumerate><A id=x1-59014x2 name=x1-59014x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Add code to acquire and release the lock.</B> All code accessing the object&#8217;s shared state &#8212; any state shared across more than one thread &#8212; must hold the object&#8217;s lock. Typically, all of an object&#8217;s member variables are shared state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The simplest and most common approach is to acquire the lock at the start of each public method and release it at the end of each public method. Doing so makes it easy to inspect your code to verify that a lock is always held when needed. It also means that the lock is already held when each private method is called, and you do not need to re-acquire it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: You may be tempted to try to avoid acquiring the lock in some methods or parts of some methods. Do not be tempted by this &#8220;optimization&#8221; until you are an experienced programmer and have done sufficient profiling of the code to verify that the optimization will significantly speed up your program, <EM>and</EM> you fully understand the hazards posed by compiler and architecture instruction re-ordering. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Acquiring an uncontended lock is a relatively inexpensive operation. By contrast, reasoning about memory interleavings can be quite difficult &#8212; and the instruction reordering done by modern compilers and processors makes it even harder. Later in this section, we discuss one commonly used (and abused) &#8220;optimization,&#8221; double-checked locking, that is outright dangerous to use. </FONT></P>
<LI class=enumerate><A id=x1-59016x3 name=x1-59016x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Identify and add condition variables.</B> How do you decide what condition variables a shared object needs? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A systematic way to approach this problem is to consider each method and ask, <EM>&#8220;When can this method wait?"</EM> Then, you can map each situation in which a method can wait to a condition variable. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You have considerable freedom in deciding how many condition variables a class should have and what each should represent. A good option is to add a condition variable for each situation in which the method must wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: Blocking bounded queue with two condition variables.</B> In our blocking bounded queue example, if the queue is full, <TT>insert</TT>&nbsp;must wait until another thread removes an item, so we created a condition variable itemRemoved. Similarly, if the queue is empty, <TT>remove</TT>&nbsp;must wait until another thread inserts an item, so we created a condition variable itemAdded. It is natural in this case to create two condition variables, itemAdded to wait until the queue has items, and itemRemoved to wait until the queue has space. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Alternatively, a single condition variable can often suffice. In fact, early versions of Java defined a single condition variable per object and did not let programmers allocate additional ones. Using this approach, any thread that waits for any reason uses that condition variable; if the condition variable is used by different threads waiting for different reasons, then any thread that wakes up a thread must <TT>broadcast</TT>&nbsp;on the condition variable. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: Blocking bounded queue with one condition variable.</B> It is also possible to implement the blocking bounded queue with a single condition variable, i.e., somethingChanged, on which threads in both <TT>insert</TT>&nbsp;or threads in <TT>remove</TT>&nbsp;can wait. With this approach, both <TT>insert</TT>&nbsp;and <TT>remove</TT>&nbsp;need to <TT>broadcast</TT>&nbsp;rather than <TT>signal</TT>&nbsp;to ensure that the right threads get a chance to run. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Programs that are more complex make these trade-offs more interesting. For example, imagine a ResourceManager class that allows a calling thread to request exclusive access to any subset of n distinct resources. One could imagine creating 2<SUP>n</SUP> condition variables; this would let a requesting thread wait on a condition variable representing exactly its desired combination. However, it would be simpler to have a single condition variable on which requesting threads wait and to broadcast on that condition whenever a resource is freed. Depending on the number of resources and the expected number of waiting threads, this simpler approach may even be more efficient. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The bottom line is that there is no hard and fast rule for how many condition variables to use in a shared object. Selecting condition variables requires thought, and different designers may use different numbers of condition variables for a given class. Like many other design decisions, this is a matter of programmer taste, judgment, and experience. Asking &#8220;When can this method wait?&#8221; will help you identify what is for you a natural way of thinking about a shared object&#8217;s condition variables. </FONT></P>
<LI class=enumerate><A id=x1-59018x4 name=x1-59018x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Add loops to wait using the condition variables.</B> Add a while(...) {cv.wait()} loop into each method that you identified as potentially needing to wait before returning. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Remember that every call to <TT>wait</TT>&nbsp;must be enclosed in a while loop that tests an appropriate predicate. Modern implementations almost invariably provide Mesa semantics and often allow for spurious wakeups (i.e., a thread can return from <TT>wait</TT>&nbsp;even if no thread called <TT>signal</TT>&nbsp;or <TT>broadcast</TT>). Therefore, a thread must always check the condition before proceeding. Even if the condition was true when the <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;call occurred, it may no longer be true when the waiting thread resumes execution. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Modularity benefits.</B> If you always wait in a while loop, your code becomes highly modular. You can look at the code that waits, and when it proceeds, know <EM>without</EM> examining any other code that the condition holds. Even erroneous calls to <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;will not change how the waiting code behaves. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, consider the assertion in the following code: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;...
&nbsp;while&nbsp;(!workAvailable())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cond.wait(&amp;lock);
&nbsp;}
&nbsp;assert(workAvailable());
&nbsp;...</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We know that the assertion holds by local inspection <EM>without knowing anything about the code that calls <TT>signal</TT>&nbsp;or <TT>broadcast</TT>.</EM> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Waiting in a while loop also makes the signal and broadcast code more robust. Adding an extra <TT>signal</TT>, or changing a <TT>signal</TT>&nbsp;to a <TT>broadcast</TT>, will not introduce bugs. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>HINT</B>: <B>Top-down design.</B> As you start writing your code, you may know that a method needs to include a wait loop, but you may not know exactly what the predicate should be. In this situation, it is often useful to name a private method function that will perform the test (e.g., workAvailable in the preceding example) and write the code that defines the function later. </FONT></P>
<LI class=enumerate><A id=x1-59020x5 name=x1-59020x5></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Add <TT>signal</TT>&nbsp;and <TT>broadcast</TT>&nbsp;calls.</B> Just as you must decide when methods can wait, you must decide when methods can let other waiting threads proceed. It is usually easy to ask, &#8220;Can a call to this method allow another thread to proceed?&#8221; and then add a <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;call if the answer is yes. But which call should you use? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">CV::signal is appropriate when: (1) at most one waiting thread can make progress, and (2) any thread waiting on the condition variable can make progress. In contrast, <TT>broadcast</TT>&nbsp;is needed when: (1) multiple waiting threads may all be able to make progress, or (2) different threads are using the same condition variable to wait for different predicates, so some of the waiting threads can make progress but others cannot. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Consider the n-resource ResourceManager problem described earlier. For the solution with a single condition variable, we must <TT>broadcast</TT>&nbsp;on the condition variable whenever a resource is freed. We do not know which thread(s) can make progress, so we tell them all to check. If, instead, we used <TT>signal</TT>, then the &#8220;wrong&#8221; thread might receive the <TT>signal</TT>, and a thread that could make progress might remain blocked. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is always safe to use <TT>broadcast</TT>. Even in cases where <TT>signal</TT>&nbsp;would suffice, at worst, all of the waiting threads would run and check the condition in the while loop, but only one would continue out of the loop. Compared to <TT>signal</TT>, this would consume some additional resources, but it would not introduce any bugs.</FONT></P></LI></OL><A id=x1-59021r98 name=x1-59021r98></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.5.2 </FONT><A id=x1-600002 name=x1-600002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementation Best Practices</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Above, we described the basic thought process you should follow when designing a shared object. To make things more concrete, we next give a set of six simple rules that we strongly advocate you follow; these are a set of &#8220;best practices&#8221; for writing code for shared objects. </FONT>
<P></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Coding standards, soapboxes, and preaching</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some programmers rebel against coding standards. We do not understand their logic. For concurrent programming in particular, a few good design patterns have stood the test of time (and many unhappy people who have departed from those patterns). For concurrent programming, <EM>debugging does not work</EM>. You must rely on: (a) writing correct code, and (b) writing code that you and others can read and understand &#8212; not just for now, but also over time as the code changes. Following the rules we provide will help you write correct, readable code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When we teach multi-threaded programming, we treat the six rules described in this section as <EM>required coding standards</EM> for all multi-threaded code that students write in our course. We say, &#8220;We cannot control what you do when you leave this class, but while you are in this class, any solution that violates these standards is, by definition, <EM>wrong</EM>.&#8221; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In fact, we feel so strongly about these rules that one of us actually presents them in class by standing on a table and pronouncing them as the Six Commandments of multi-threaded programming: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">1. Thou shalt always do things the same way. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">and so on. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The particular formulation (and presentation) of these rules evolved from our experience teaching multi-threaded programming dozens of times to hundreds of students and identifying common mistakes. We have found that when we insist that students follow these rules, the vast majority find it easy to write clear and correct code for shared objects. Conversely, in earlier versions of the course, when we phrased these items as &#8220;strong suggestions,&#8221; many students found themselves adrift, unable to write code for even the simplest shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our advice to those learning multi-threaded programming is to treat these rules as a given and follow them strictly for a semester or so, until writing shared objects is easy. At that point, you most likely will understand concurrent programming well enough to decide whether to continue to follow the rules. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We also believe that experienced programmers benefit from adhering closely to these rules. Since we began teaching them, we have also disciplined ourselves to follow them unless there is a very good reason not to. We have found exceptions to be rare. Conversely, when we catch ourselves being tempted to deviate from the rules, the vast majority of the time our code improves if we force ourselves to rewrite the code to follow the rules. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although the rules may come across as opinionated (and they are), they are far from novel. Over three decades ago, Lampson and Redell&#8217;s paper, &#8220;Experience with Processes and Monitors in Mesa,&#8221; provided similar advice (in a more measured tone). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-60002x1 name=x1-60002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Consistent structure.</B> The first rule is a meta-rule that underlies the other five rules: <EM>follow a consistent structure.</EM> Although programming with a clean, consistent structure is always useful, it is particularly important to strictly follow tried-and-true design patterns for shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">At a minimum, even if one way is not inherently better than another, following the same strategy every time: (1) frees you to focus on the core problem because the details of the standard approach become a habit, and (2) makes it easier for those who follow to review, maintain, and debug your code. (And it will make it easier for <EM>you</EM> to maintain and debug your code.) </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As an analogy, electricians follow standards for the colors of wire they use for different tasks. White is neutral. Black or red is hot. Copper is ground. An electrician does not have to decide &#8220;Hm. I have a bit more white on my belt today than black, should I use white or black for my grounds?&#8221; When an electrician walks into a room she wired last month, she does not have to spend time trying to remember which color is which. If an electrician walks into a room she has never seen before, she can immediately determine what the wiring is doing, without having to trace it back into the switchboard. Similar advantages apply to coding standards. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, for concurrent programs, the evidence is that the abstractions we describe <EM>are</EM> better than almost all others. Until you become a <EM>very</EM> experienced concurrent programmer, take advantage of the hard-won experience of those that have come before you. Once you are a concurrency guru, you are welcome to invent a better mousetrap. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Sure, you can cut corners and occasionally save a line or two of typing by departing from the standards. However, you will have to spend a few minutes thinking to convince yourself that you are right on a case-by-case basis (and another few minutes typing comments to convince the next person to look at the code that you are right), and a few hours or weeks tracking down bugs when you are wrong. It is just not worth it. </FONT></P>
<LI class=enumerate><A id=x1-60004x2 name=x1-60004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Always synchronize with locks and condition variables.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Many operating systems, such as Linux, Windows, and MacOS, provide a diversity of synchronization primitives. At the end of this chapter, we will describe one such primitive, semaphores, which is particularly widely used in operating system kernel implementations. Compared to locks and condition variables, semaphores are equally powerful: you can build condition variables using semaphores and vice versa. If so, why pick one over the other? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We recommend that you be able to read and understand semaphores so you can understand legacy code, but that you only write new code using locks and condition variables. Almost always, code using locks and condition variables is clearer than the equivalent code using semaphores because it is more &#8220;self-documenting.&#8221; If the code is well structured, what each synchronization action is doing should be obvious. Admittedly, semaphores sometimes seem to fit what you are doing perfectly because you can map the object&#8217;s invariants exactly onto the internal state of the semaphore; for example, you can write an extremely concise version of our blocking bounded queue using semaphores. But what happens when the code changes next month? Will the fit remain as good? For consistency and simplicity, choose one of the two styles and stick with it. In our opinion, the right one is to use locks and condition variables. </FONT></P>
<LI class=enumerate><A id=x1-60006x3 name=x1-60006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Always acquire the lock at the beginning of a method and release it right before the return.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This extends the principle of consistent structure: pick one way to do things and always follow it. The benefit here is that it is easy to read code and see where the lock is or is not held because synchronization is structured on a method-by-method basis. Conversely, if <TT>acquire</TT>&nbsp;and <TT>release</TT>&nbsp;calls are buried in the middle of a method, it is harder to quickly inspect and understand the code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Taking a step back, if there is a logical chunk of code that you can identify as a set of actions that require a lock, then that section should probably be its own procedure: it is a set of logically related actions. If you find yourself wanting to acquire a lock in the middle of a procedure, that is usually a red flag that you should break the piece you are considering into a separate procedure. We are all sometimes lazy about creating new procedures when we should. Take advantage of this signal, and the result will be clearer code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are two corollaries to this rule. First, if your code is well structured, all shared data will be encapsulated in an object, and therefore all accesses to shared data will be protected by a lock. Since compilers and processors <EM>never</EM> re-order instructions across lock operations, this rule guarantees instruction re-ordering is not a concern for your code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Second, from time to time, we see students attempting to acquire a lock in one procedure, and release it in another procedure, or worse, in a completely different thread. (One popular idea is to acquire a lock in a parent thread, pass it in thread_fork to a child, and have the child release the lock after it has started.) <EM>Do not do this.</EM> For one, it can make it very difficult for someone reading your code to determine which shared variables are protected by which lock; by acquiring at the beginning of the procedure and releasing at the end, which variables go with which locks is obvious. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While some early thread systems allowed lock passing, most recently designed systems prohibit it. For example, in POSIX, lock release is &#8220;undefined&#8221; when called by a different thread than the thread that acquired the lock. In other words, it might work on some systems, but it is not portable. In Java, it is completely prohibited. </FONT></P>
<LI class=enumerate><A id=x1-60008x4 name=x1-60008x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Always hold the lock when operating on a condition variable.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The reason you signal on a condition variable &#8212; after manipulating shared state &#8212; is that another thread is waiting in a loop for some test on shared state to become true. Condition variables are useless without shared state, and shared state should only be accessed while holding a lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Many libraries enforce this rule &#8212; that you cannot call condition variable methods unless you hold the corresponding lock. However, some run-time systems and libraries allow sloppiness, so take care. </FONT></P>
<LI class=enumerate><A id=x1-60010x5 name=x1-60010x5></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Always wait in a while() loop</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The pattern should always be: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;while&nbsp;(predicateOnStateVariables(...))&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;condition-&gt;wait(&amp;lock);
&nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">and never: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;...
&nbsp;if&nbsp;(predicateOnStateVariables(...))&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wait(&amp;lock);
&nbsp;}
&nbsp;...</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Here, predicateOnStateVariables(...) is code that looks at the state variables of the current object to decide if the thread should proceed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You may be tempted to guard a <TT>wait</TT>&nbsp;call with an if conditional rather than a while loop when you can deduce from the global structure of the program that, despite Mesa semantics, any time a thread returns from wait, it can proceed. Avoid this temptation. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While works any time if does, and it works in situations when if does not. By the principle of consistent structure, do things the same way every time. But there are three additional issues. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using if breaks modularity. In the preceding example, to know whether using if will work, you must consider the global structure of the program: what threads there are, where <TT>signal</TT>&nbsp;is called, etc. The problem is that a change in code in one method (say, adding a <TT>signal</TT>) can then cause a bug in another method (where the <TT>wait</TT>&nbsp;is). Using while is self-documenting; anyone can look at the <TT>wait</TT>&nbsp;and see exactly when a thread may proceed. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Always using while gives you incredible freedom about where to put a <TT>signal</TT>. In fact, <TT>signal</TT>&nbsp;becomes a hint &#8212; you can add a signal to an arbitrary place in a correct program and have it remain correct. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using if breaks portability. Some implementations of condition variables allow spurious wakeups, while others do not. For example, implementations of condition variables in both Java and the POSIX pthreads library are allowed to return from <TT>wait</TT>&nbsp;even though no thread called <TT>signal</TT>&nbsp;or <TT>broadcast</TT>.</FONT></P></LI></UL>
<LI class=enumerate><A id=x1-60012x6 name=x1-60012x6></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>(Almost) never use <TT>thread_sleep</TT>.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Many thread libraries have a <TT>thread_sleep</TT>&nbsp;function that suspends execution of the calling thread for some period of wall clock time. Once that time passes, the thread is returned to the scheduler&#8217;s ready queue and can run again. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Never use <TT>thread_sleep</TT>&nbsp;to have one thread wait for another thread to perform a task. The correct way to wait for a condition to become true is to <TT>wait</TT>&nbsp;on a condition variable. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In general, <TT>thread_sleep</TT>&nbsp;is appropriate only when there is a particular real-time moment when you want to perform some action, such as a timeout for when to declare a remote server non-responsive. If you catch yourself writing while(testOnObjectState()) {thread_sleep();}, treat this as a red flag that you are probably making a mistake. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Similarly, if a thread must wait for an object&#8217;s state to change, it should <TT>wait</TT>&nbsp;on a condition variable, and not just call thread_yield. Use thread_yield&nbsp;only when a low-priority thread <EM>that can still make progress</EM> wants to let a higher-priority thread to run. </FONT></P></LI></OL><A id=x1-60013r99 name=x1-60013r99></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.5.3 </FONT><A id=x1-610003 name=x1-610003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Three Pitfalls</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">We next describe three common pitfalls. The first, double-checked locking, is a problem in many different programming languages, including C, C++ and Java. The second and third pitfalls are specific to Java. Java is a modern type-safe language that included support for threads from its inception. This built-in support makes multi-threaded programming in Java convenient. However, some aspects of the language are <EM>too</EM> flexible and can encourage bad practices. We highlight those pitfalls here. </FONT>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-61002x1 name=x1-61002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Double-Checked Locking.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We strongly advise holding a shared object&#8217;s lock across any method that accesses the object&#8217;s member variables. Programmers are often tempted to avoid some of these lock acquire and release operations. Unfortunately, such efforts often result in code that is complex, wrong, or both. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To illustrate the challenges, consider the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:double-checked locking"}'>double-checked locking</A></EM> design pattern. The canonical example is an object that is allocated and initialized lazily the first time it is needed by any thread. (This example and analysis is taken from Meyers and Alexandrescu, &#8220;C++ and the Perils of Double-Checked Locking.&#8221; </FONT><A href="http://www.aristeia.com/Papers/DDJ_Jul_Aug_2004_revised.pdf"><FONT style="BACKGROUND-COLOR: #7be1e1">http://www.aristeia.com/Papers/DDJ_Jul_ Aug_2004_revised.pdf</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) Being good programmers, we can hide the lazy allocation inside an object, Singleton, which returns a pointer to the object, creating it if needed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The &#8220;optimization&#8221; is to acquire the lock if the object has not already been allocated, but to avoid acquiring the lock if the object already exists. Because there can be a race condition between the first check and acquiring the lock, the check must be made again inside the lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;Singleton&nbsp;{
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;static&nbsp;Singleton*&nbsp;instance();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;static&nbsp;Singleton*&nbsp;pInstance;
&nbsp;};
&nbsp;
&nbsp;Singleton*&nbsp;Singleton::pInstance&nbsp;=&nbsp;NULL;
 </FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;BUG!&nbsp;&nbsp;DON&#8217;T&nbsp;DO&nbsp;THIS!
&nbsp;Singleton*
&nbsp;Singleton::instance()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(pInstance&nbsp;==&nbsp;NULL)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(pInstance&nbsp;==&nbsp;NULL){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pInstance&nbsp;=&nbsp;new&nbsp;Instance();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;pInstance;
&nbsp;}
 </FONT></PRE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although the intuition is appealing, <B>this code does not work.</B> The problem is that the statement pInstance = new Instance() is not an atomic operation; in fact, it comprises at least three steps: </FONT></P>
<OL class=enumerate2>
<LI class=enumerate><A id=x1-61004x1 name=x1-61004x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Allocate memory for a Singleton object. </FONT></P>
<LI class=enumerate><A id=x1-61006x2 name=x1-61006x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Initialize the Singleton object&#8217;s memory by running the constructor. </FONT></P>
<LI class=enumerate><A id=x1-61008x3 name=x1-61008x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Make pInstance point to this newly constructed object.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The problem is that modern compilers and hardware architectures can reorder these events. Thus, it is possible for thread 1 to execute the first step and then the third step; then thread 2 can call instance, see that pInstance is non-null, return it, and begin using this object before thread 1 finishes initializing it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Discussion.</B> This is just an example of dangers that lurk when you try to elide locks; the lesson applies more broadly. This example is extremely simple &#8212; fewer than 10 lines of code with very simple logic &#8212; yet a number of published solutions have been wrong. As Meyers and Alexandrescu note, some tempting solutions using temporary variables and the volatile keyword do not work. Bacon et al.&#8217;s &#8220;The &#8217;Double-Checked Locking is Broken&#8217; Declaration&#8221; discusses a range of non-solutions in Java. </FONT><A href="http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html"><FONT style="BACKGROUND-COLOR: #7be1e1">http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This type of optimization is risky and often does not provide significant performance gains in practice. Most programmers should not consider them. Even expert programmers should habitually stick to simpler programming patterns, like the ones we have discussed, and only consider optimizations like double-checked locking when performance measurements and profiling indicate that the optimizations would significantly improve overall performance. </FONT></P>
<LI class=enumerate><A id=x1-61010x2 name=x1-61010x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Avoid defining a synchronized block in the middle of a method.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Java provides built in language support for shared objects. The base Object class, from which all classes inherit, includes a lock and a condition variable as members. Any method declaration can include the keyword synchronized to indicate that the object&#8217;s lock is to be automatically acquired on entry to the method and automatically released on any return from the method. For example: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;public&nbsp;synchronized&nbsp;foo()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Do&nbsp;something;&nbsp;lock&nbsp;is&nbsp;automatically&nbsp;acquired/released.
&nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This syntax is useful &#8212; it follows rule #2 above, and it frees the programmer from having to worry about details, like making sure the lock is released before every possible return point including exceptions. The pitfall is that Java also allows a <EM>synchronized block</EM> in the middle of a method. For example: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;public&nbsp;bar()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Do&nbsp;something&nbsp;without&nbsp;holding&nbsp;the&nbsp;lock
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;synchronized{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Do&nbsp;something&nbsp;while&nbsp;holding&nbsp;the&nbsp;lock
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Do&nbsp;something&nbsp;without&nbsp;holding&nbsp;the&nbsp;lock
&nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This construct violates rule #3 from Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-600002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.5.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and suffers from the disadvantages listed there. The solution is the same as discussed above: when you find yourself tempted to write a synchronized block in the middle of a Java method, treat that as a strong hint that you should define a separate method to more clearly encapsulate the logical chunk you have identified. </FONT></P>
<LI class=enumerate><A id=x1-61012x3 name=x1-61012x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Keep shared state classes separate from thread classes.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Java defines a class called Thread that implements an interface called Runnable that other classes can implement in order to be treated as threads by the runtime system. To write the code that represents a thread&#8217;s &#8220;main loop,&#8221; you typically extend the Thread class or implement a class that implements Runnable. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The pitfall is that, when extending the Thread class (or writing a new class that implements Runnable), you may be tempted to include not only the thread&#8217;s main loop but also state to be shared across multiple threads, blurring the lines between the threads and the shared objects. This is almost always confusing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, for a blocking bounded queue, rather than defining two classes, BBQ for the shared queue and WorkerThread for the threads, you may be tempted to combine the two into a single class &#8212; for example, a queue with an associated worker thread. If this sounds confusing, it is, but it is a pitfall that we frequently see in student code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The solution is simple. Always make sure threads and shared objects are defined in separate classes. State that can be accessed by multiple threads, locks, and condition variables should never appear in any Java class that extends Thread or implements Runnable.</FONT></P></LI></OL><A id=x1-61013r97 name=x1-61013r97></A><A id=x1-620006 name=x1-620006>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.6 Three Case Studies</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">The best way to learn how to program concurrently is to practice. Multithreaded programming is an important skill, and we anticipate that almost everyone reading this book will over time need to write many multi-threaded programs. To help get you started, this section walks through several examples. </FONT><A id=x1-62001r100 name=x1-62001r100></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.6.1 </FONT><A id=x1-630001 name=x1-630001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Readers/Writers Lock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">First, we implement a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:readers/writers lock"}'>readers/writers lock</A></EM>. Like a normal mutual exclusion lock, a readers/writers lock (RWLock) protects shared data. However, it makes the following optimization. To maximize performance, an RWLock allows multiple &#8220;reader&#8221; threads to simultaneously access the shared data. Any number of threads can safely read shared data at the same time, as long as no thread is modifying the data. However, only one &#8220;writer&#8221; thread may hold the RWLock at any one time. (While a &#8220;reader&#8221; thread is restricted to only read access, a &#8220;writer&#8221; thread may read <EM>and</EM> write the data structure.) When a writer thread holds the RWLock, it may safely modify the data, as the lock guarantees that no other thread (whether reader or writer) may simultaneously hold the lock. The mutual exclusion is thus between any writer and any other writer, and between any writer and <EM>the set</EM> of readers. </FONT>
<P></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Optimizing for the common case</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Reader/writer locks are an example of an important principle in the design of computer systems: optimizing for the common case. Performance optimizations often have the side effect of making the code more complex to understand and reason about. Code that is more complex is more likely to be buggy, and more likely to have new bugs introduced as features are added. How do we decide when an optimization is worth the cost? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One approach is to profile your code. Then, <EM>and only then</EM>, optimize the code paths that are frequently used. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the case of locks, it is obviously simpler to use a regular mutual exclusion lock. Replacing a mutual exclusion lock with a reader-writer lock is appropriate when both of the following are true: (i) there is substantial contention for the mutual exclusion lock and (ii) a substantial majority of the accesses are read-only. In other words, it is only appropriate to use if it would make a significant difference. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Reader-writer locks are very commonly used in databases, where they are used to support faster search queries over the database, while also supporting less frequent database updates. Another common use is inside the operating system kernel, where core data structures are often read by many threads and only infrequently updated. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To generalize our mutual exclusion lock into a readers/writers lock, we implement a new kind of shared object, RWLock, to guard access to the shared data and to enforce these rules. The RWLock is implemented using our standard synchronization building blocks: mutual exclusion locks and condition variables. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A thread that wants to (atomically) read the shared data proceeds as follows: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;rwLock-&gt;startRead();
   &nbsp;//&nbsp;Read&nbsp;shared&nbsp;data
   &nbsp;rwLock-&gt;doneRead();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Similarly, a thread that wants to (atomically) write the shared data does the following: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;rwLock-&gt;startWrite();
   &nbsp;//&nbsp;Read&nbsp;and&nbsp;write&nbsp;shared&nbsp;data
   &nbsp;rwLock-&gt;doneWrite();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To design the RWLock class, we begin by defining its interface (already done in this case) and its shared state. For the state, it is useful to keep enough data to allow a precise characterization of the object; especially when debugging, having too much state is better than having too little. Here, the object&#8217;s behavior is fully characterized by the number of threads reading or writing and the number of threads waiting to read or write, so we have chosen to keep four integers to track these values. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-630019"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the members of and interface to the RWLock class. </FONT><A id=x1-630019 name=x1-630019></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;RWLock{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Synchronization&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;readGo;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;writeGo;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;State&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;activeReaders;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;activeWriters;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;waitingReaders;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;waitingWriters;
&nbsp;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RWLock();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~RWLock()&nbsp;{};
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;startRead();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;doneRead();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;startWrite();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;doneWrite();
&nbsp;
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;readShouldWait();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;writeShouldWait();
&nbsp;};
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.9: </B>The interface and member variables for our readers/writers lock.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Next, we add synchronization variables by asking, &#8220;When can methods wait?&#8221; First, we add a mutual exclusion lock: the RWLock methods must wait whenever another thread is accessing the RWLock state variables. Next, we observe that startRead or startWrite may have to wait, so we add a condition variable for each case: readGo and writeGo. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">RWLock::doneRead and doneWrite do not wait (other than to acquire the mutual exclusion lock). Therefore, these methods do not need any additional condition variables. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can now implement RWLock. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6300210"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the complete solution, which we develop in a few simple steps. Much of what we need to do is almost automatic. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since we always acquire/release mutual exclusion locks at the beginning/end of a method (and never in the middle), we can write calls to acquire and release the mutual exclusion lock at the start and end of each public method before even thinking in detail about what these methods do. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">At this point, startRead and doneRead look like this:</FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;void&nbsp;RWLock::startRead()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RWLock::doneRead()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
 </FONT></PRE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">RWLock::startWrite and RWLock::doneWrite are similar. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since we know startRead and startWrite may have to wait, we can write a while(...){wait(...);} loop in the middle of each. In fact, we can defer thinking about the details by inserting a private method to be defined later, as the predicate for the while loop (e.g., readShouldWait and writeShouldWait). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">At this point, startRead looks like this: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;void&nbsp;RWLock::startRead()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(readShouldWait())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readGo.Wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">RWLock::StartWrite() looks similar. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Now things get a bit more complex. We can add code to track activeReaders, activeWriters, waitingReaders, and waitingWriters. Since we hold mutual exclusion locks in all of the public methods, this is easy to do. For example, a call to startRead initially increments the number of waiting readers; when the thread gets past the while loop, the number of waiting readers is decremented, but the number of active readers is incremented. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When reads or writes finish, it may become possible for waiting threads to proceed. We therefore need to add <TT>signal</TT>&nbsp;or <TT>broadcast</TT>&nbsp;calls to doneRead and doneWrite. The simplest solution would be to <TT>broadcast</TT>&nbsp;on both readGo and writeGo in each method, but that would be both inefficient and (to our taste) less clear about how the class actually works. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Instead, we observe that in doneRead, when a read completes, there are two interesting cases: (a) no writes are pending, and nothing needs to be done since this read cannot prevent other reads from proceeding, or (b) a write is pending, and this is the last active read, so one write can proceed. In case (b), we use <TT>signal</TT>&nbsp;since at most one write can proceed, and any write waiting on the condition variable can proceed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our code for startRead and doneRead is now done: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Wait&nbsp;until&nbsp;no&nbsp;active&nbsp;or&nbsp;waiting
&nbsp;//&nbsp;writes,&nbsp;then&nbsp;proceed.
&nbsp;void&nbsp;RWLock::startRead()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitingReaders++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(readShouldWait())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readGo.Wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitingReaders--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;activeReaders++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Done&nbsp;reading.&nbsp;If&nbsp;no&nbsp;other&nbsp;active
&nbsp;//&nbsp;reads,&nbsp;a&nbsp;write&nbsp;may&nbsp;proceed.
&nbsp;void&nbsp;RWLock::doneRead()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;activeReaders--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(activeReaders&nbsp;==&nbsp;0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&amp;&amp;&nbsp;waitingWriters&nbsp;&gt;&nbsp;0)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writeGo.signal();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
 </FONT></PRE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Code for startWrite and doneWrite is similar. For doneWrite, if there are any pending writes, we signal on writeGo. Otherwise, we broadcast on readGo. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Finally, we need to define the readShouldWait and writeShouldWait predicates. Here, we implement a <EM>writers preferred</EM> solution: reads should wait if there are any active or pending writers, while writes wait only while there are active readers or active writers. Otherwise, a continuous stream of new readers could starve a write request and prevent if from ever being serviced. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;bool
   &nbsp;RWLock::readShouldWait()&nbsp;{
   &nbsp;&nbsp;&nbsp;return&nbsp;(activeWriters&nbsp;&gt;&nbsp;0&nbsp;||&nbsp;waitingWriters&nbsp;&gt;&nbsp;0);
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The code for writeShouldWait is similar. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since readShouldWait and writeShouldWait are private methods that are always called from public methods that hold the mutual exclusion lock, they do not need to acquire the lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6300210"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> gives the full code. This solution may not be to your taste. You may decide to use more or fewer condition variables, use different state variables to implement different invariants, or change when to call <TT>signal</TT>&nbsp;or <TT>broadcast</TT>. The shared object approach allows designers freedom in these dimensions. </FONT><A id=x1-6300210 name=x1-6300210></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Wait&nbsp;until&nbsp;no&nbsp;active&nbsp;or&nbsp;waiting
&nbsp;//&nbsp;writes,&nbsp;then&nbsp;proceed.
&nbsp;void&nbsp;RWLock::startRead()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitingReaders++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(readShouldWait())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readGo.Wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitingReaders--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;activeReaders++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Done&nbsp;reading.&nbsp;If&nbsp;no&nbsp;other&nbsp;active
&nbsp;//&nbsp;reads,&nbsp;a&nbsp;write&nbsp;may&nbsp;proceed.
&nbsp;void&nbsp;RWLock::doneRead()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;activeReaders--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(activeReaders&nbsp;==&nbsp;0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&amp;&amp;&nbsp;waitingWriters&nbsp;&gt;&nbsp;0)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writeGo.signal();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Read&nbsp;waits&nbsp;if&nbsp;any&nbsp;active&nbsp;or&nbsp;waiting
&nbsp;//&nbsp;write&nbsp;("writers&nbsp;preferred").
&nbsp;bool
&nbsp;RWLock::readShouldWait()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;(activeWriters&nbsp;&gt;&nbsp;0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;||&nbsp;waitingWriters&nbsp;&gt;&nbsp;0);
&nbsp;}
&nbsp;
&nbsp;
&nbsp;//&nbsp;Wait&nbsp;until&nbsp;no&nbsp;active&nbsp;read&nbsp;or
&nbsp;//&nbsp;write&nbsp;then&nbsp;proceed.
&nbsp;void&nbsp;RWLock::startWrite()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitingWriters++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(writeShouldWait())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writeGo.Wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitingWriters--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;activeWriters++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Done&nbsp;writing.&nbsp;A&nbsp;waiting&nbsp;write&nbsp;or
&nbsp;//&nbsp;read&nbsp;may&nbsp;proceed.
&nbsp;void
&nbsp;RWLock::doneWrite()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;activeWriters--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert(activeWriters&nbsp;==&nbsp;0);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(waitingWriters&nbsp;&gt;&nbsp;0)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writeGo.signal();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readGo.broadcast();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Write&nbsp;waits&nbsp;for&nbsp;active&nbsp;read&nbsp;or&nbsp;write.
&nbsp;bool
&nbsp;RWLock::writeShouldWait()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;(activeWriters&nbsp;&gt;&nbsp;0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;||&nbsp;activeReaders&nbsp;&gt;&nbsp;0);
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.10: </B>An implementation of a readers/writers lock.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Single stepping and model checking your code</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you have written some concurrent code, and you would like to verify that the solution behaves as you expect. One thing you should <EM>always</EM> do &#8212; whether for sequential or concurrent code &#8212; is to use a debugger to single step through the code on various inputs, to verify that the program logic is doing what you expect it to do, and do the variables have the values you expect. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This is especially useful for concurrent programs. Since the program must work for any possible thread schedule, you can use the debugger to consider what happens when threads are interleaved in different ways. Does your program logic still do what you expect? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, for the RWLock class, you can: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Start a single reader. Does it go all the way through? Obviously, it should not wait, since no one has the lock and there are no writers. When it finishes readDone, are the state variables back to their initial state? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Start a writer, and after it acquires the mutual exclusion lock, start a reader. Does it wait for the lock? When the writer finishes startWrite, does the reader proceed and then wait for the writer to call doneWrite? Does the reader proceed after that? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Start a reader, followed by a writer, followed by another reader. And so forth. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We encourage you to do this for the examples in this section. The examples are short enough that you can execute them by hand, but we also provide code if you want to try this in a debugger. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A more systematic approach is called model checking. To fully verify that a concurrent program does what it was designed to do, a model checker enumerates all possible sequences of operations, and tries each one in turn. Since this could result in a nearly infinite number of possible tests even for a fairly simple program, to be practical model checking needs to reduce the search space. For code that follows our guidelines &#8212; with locks to protect shared data &#8212; the exact ordering of instructions is no longer important. For example, preempting a thread that holds a lock is immaterial to the behavior of the program. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Rather, the behavior of the program depends on the sequence of synchronization instructions: which thread is first to acquire the lock, which thread waits on a condition variable, and so forth. Thus, a model checker can proceed in two steps: first verify that there are no unlocked accesses to shared data, and then enumerate various sequences of synchronization operations. Even with this, the number of possibilities can be prohibitively large, and so typically the model checker will verify however many different interleavings it can within some time limit. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-63003r102 name=x1-63003r102></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.6.2 </FONT><A id=x1-640002 name=x1-640002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Synchronization Barriers</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">With data parallel programming, as we explained in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, the computation executes in parallel across a data set, with each thread operating on a different partition of the data. Once all threads have completed their work, they can safely use each other&#8217;s results in the next (data parallel) step in the algorithm. MapReduce is an example of data parallel programming, but there are many other systems with the same structure. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For this to work, we need an efficient way to check whether all n threads have finished their work. This is called a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:synchronization barrier"}'>synchronization barrier</A></EM>. It has one operation, <TT>checkin</TT>. A thread calls <TT>checkin</TT>&nbsp;when it has completed its work; no thread may return from <TT>checkin</TT>&nbsp;until <EM>all</EM> n threads have checked in. Once all threads have checked in, it is safe to use the results of the previous step. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Note that a synchronization barrier is different from a memory barrier, defined earlier in the chapter. A synchronization barrier is called concurrently by many threads; the barrier prevents any thread from proceeding until all threads reach the barrier. A memory barrier is called by one thread, to prevent the thread from proceeding until all memory operations that occur before the barrier have completed and are visible to other threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An implementation of MapReduce using a synchronization barrier might look like the code in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6400111"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.11</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-6400111 name=x1-6400111></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;Create&nbsp;n&nbsp;threads.
&nbsp;Create&nbsp;barrier.
&nbsp;
&nbsp;Each&nbsp;thread&nbsp;executes&nbsp;map&nbsp;operation&nbsp;in&nbsp;parallel.
&nbsp;barrier.checkin();
&nbsp;
&nbsp;Each&nbsp;thread&nbsp;sends&nbsp;data&nbsp;in&nbsp;parallel&nbsp;to&nbsp;reducers.
&nbsp;barrier.checkin();
&nbsp;
&nbsp;Each&nbsp;thread&nbsp;executes&nbsp;reduce&nbsp;operation&nbsp;in&nbsp;parallel.
&nbsp;barrier.checkin();</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.11: </B>An implementation of MapReduce using synchronization barriers.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An alternative to using a synchronization barrier would be to create n threads at each step; the main thread could then call thread_join&nbsp;on each thread to ensure its completion. While this would be correct, it might be inefficient. Not only would n new threads need to be started at each step, the partitioning of work among threads would also need to be redone each time. Frequently, each thread in a data parallel computation can work on the same data repeatedly over many steps, maximizing the efficiency of the hardware processor cache. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can derive an implementation for a synchronization barrier in the same way as we described above for the readers/writers lock. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We create a Barrier class, with a lock to protect its internal state variables: how many have checked in so far (count), and how many we are expecting (numThreads). </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We <TT>acquire</TT>&nbsp;the lock at the beginning of <TT>checkin</TT>, and we <TT>release</TT>&nbsp;it at the end. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since threads may have to wait in <TT>checkin</TT>, we need a condition variable, allCheckedIn. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We put the <TT>wait</TT>&nbsp;in a while loop, checking if all n threads have checked in yet. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The last thread to <TT>checkin</TT>&nbsp;does a <TT>broadcast</TT>&nbsp;to wake up all of the waiters.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6400212"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> gives the full implementation. Note that we still use a while loop, even though the signal means that the thread can safely exit <TT>checkin</TT>. There is no harm in using a while statement, and it protects against the possibility of the runtime library issuing spurious wakeups. </FONT><A id=x1-6400212 name=x1-6400212></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;A&nbsp;single&nbsp;use&nbsp;synch&nbsp;barrier.
&nbsp;class&nbsp;Barrier{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Synchronization&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;allCheckedIn;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;State&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;numEntered;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;numThreads;
&nbsp;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Barrier(int&nbsp;n);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~Barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;checkin();
&nbsp;};
&nbsp;
&nbsp;Barrier::Barrier(int&nbsp;n)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numEntered&nbsp;=&nbsp;0;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numThreads&nbsp;=&nbsp;n;
&nbsp;}
&nbsp;
&nbsp;//&nbsp;No&nbsp;one&nbsp;returns&nbsp;until&nbsp;all&nbsp;threads
&nbsp;//&nbsp;have&nbsp;called&nbsp;checkin.
&nbsp;void
&nbsp;checkin()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numEntered++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(numEntered&nbsp;&lt;&nbsp;numThreads)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(numEntered&nbsp;&lt;&nbsp;numThreads)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allCheckedIn.wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;//&nbsp;last&nbsp;thread&nbsp;to&nbsp;checkin
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allCheckedIn.broadcast();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.12: </B>Candidate implementation of a synchronization barrier. With this implementation, each instance of a barrier can be safely used only one time.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The design is straightforward, but a problem is that the barrier can only be used once. One way to see this is that the state of the barrier does not revert to the same state it had when it was created. Implementing a reusable barrier is a bit more subtle. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The <EM>first</EM> thread to leave (the one that wakes up the other threads) cannot reset the state, because until the other threads have woken up, the state is needed so that they know to exit the while loop. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The <EM>last</EM> thread to leave the barrier cannot reset the state for the next iteration, because there is a possible race condition. Suppose a thread finishes <TT>checkin</TT>&nbsp;and calls <TT>checkin</TT>&nbsp;on the next barrier <EM>before</EM> the last thread wakes up and leaves the previous barrier. In that case, the thread would find that n threads have already checked in (because the state hasn&#8217;t been reset), and so it would think it is &#8220;ok to proceed!&#8221;</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A simple way to implement a re-usable barrier is to use two single-use barriers. The first barrier ensures that all threads are checked in, and the second ensures that all threads have woken up from allCheckedIn.wait. The nth thread to leave can safely reset numCheckedIn; the nth thread to call <TT>checkin</TT>&nbsp;can safely reset numLeaving. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6400313"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> gives the result. </FONT><A id=x1-6400313 name=x1-6400313></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;A&nbsp;re-usable&nbsp;synch&nbsp;barrier.
&nbsp;class&nbsp;Barrier{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Synchronization&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;allCheckedIn;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;allLeaving;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;State&nbsp;variables
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;numEntered;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;numLeaving;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;numThreads;
&nbsp;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Barrier(int&nbsp;n);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;~Barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;checkin();
&nbsp;};
&nbsp;
&nbsp;Barrier::Barrier(int&nbsp;n)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numEntered&nbsp;=&nbsp;0;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numLeaving&nbsp;=&nbsp;0;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numThreads&nbsp;=&nbsp;n;
&nbsp;}
&nbsp;
&nbsp;//&nbsp;No&nbsp;one&nbsp;returns&nbsp;until&nbsp;all&nbsp;threads
&nbsp;//&nbsp;have&nbsp;called&nbsp;checkin.
&nbsp;void
&nbsp;checkin()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numEntered++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(numEntered&nbsp;&lt;&nbsp;numThreads)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(numEntered&nbsp;&lt;&nbsp;numThreads)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allCheckedIn.wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;no&nbsp;threads&nbsp;in&nbsp;allLeaving.wait
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numLeaving&nbsp;=&nbsp;0;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allCheckedIn.broadcast();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numLeaving++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(numLeaving&nbsp;&lt;&nbsp;numThreads)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(numLeaving&nbsp;&lt;&nbsp;numThreads)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allLeaving.wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;no&nbsp;threads&nbsp;in&nbsp;allCheckedIn.wait
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;numEntered&nbsp;=&nbsp;0;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;allLeaving.broadcast();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.13: </B>Implementation of a re-usable synchronization barrier.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-64004r105 name=x1-64004r105></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.6.3 </FONT><A id=x1-650003 name=x1-650003></A><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO Blocking Bounded Queue</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming Mesa semantics for condition variables, our implementation of the thread-safe blocking bounded queue in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-560018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> does not guarantee freedom from starvation. For example, a thread may call remove and wait in the while loop because the queue is empty. Starvation would occur if every time another thread inserts an item into the queue, a <EM>different</EM> thread calls remove, acquires the lock, sees that the queue is full, and removes the item before the waiting thread resumes. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Often, starvation is not a concern. For example, if we have one thread putting items into the queue, and n equivalent worker threads removing items from the queue, it may not matter which of the worker threads goes first. Even if starvation is a concern, as long as calls to insert and remove are infrequent, or the buffer is rarely empty or full, every thread is highly likely to make progress. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose, however, we do need a thread-safe bounded buffer that does guarantee progress to all threads. We can more formally define the liveness constraint as: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Starvation-freedom.</B> If a thread waits in insert, then it is guaranteed to proceed after a bounded number of remove calls complete, and vice versa. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>First-in-first-out (FIFO).</B> A stronger constraint is that the queue is first-in-first-out, or FIFO. The nth thread to acquire the lock in remove retrieves the item inserted by the nth thread to acquire the lock in insert. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Under Hoare semantics, the implementation in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-560018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> is FIFO, and therefore also starvation-free, provided that <TT>signal</TT>&nbsp;wakes up the thread waiting the longest. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Here we consider a related question: can we implement a starvation-free or FIFO bounded buffer using Mesa semantics? We need to ensure that when one thread signals a waiter, the waiting thread (and not any other) removes the item. </FONT><A id=x1-6500114 name=x1-6500114></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;ConditionQueue&nbsp;insertQueue;
&nbsp;ConditionQueue&nbsp;removeQueue;
&nbsp;int&nbsp;numRemoveCalled&nbsp;=&nbsp;0;&nbsp;//&nbsp;#&nbsp;of&nbsp;times&nbsp;remove&nbsp;has&nbsp;been&nbsp;called
&nbsp;int&nbsp;numInsertCalled&nbsp;=&nbsp;0;&nbsp;//&nbsp;#&nbsp;of&nbsp;times&nbsp;insert&nbsp;has&nbsp;been&nbsp;called
&nbsp;
&nbsp;int
&nbsp;FIFOBBQ::remove()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;item;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;myPosition;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;*myCV,&nbsp;*nextWaiter;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myPosition&nbsp;=&nbsp;numRemoveCalled++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mycv&nbsp;=&nbsp;new&nbsp;CV;&nbsp;&nbsp;//&nbsp;Create&nbsp;a&nbsp;new&nbsp;condition&nbsp;variable&nbsp;to&nbsp;wait&nbsp;on.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;removeQueue.append(myCV);
&nbsp;
&nbsp;//&nbsp;Even&nbsp;if&nbsp;I&nbsp;am&nbsp;woken&nbsp;up,&nbsp;wait&nbsp;until&nbsp;it&nbsp;is&nbsp;my&nbsp;turn.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(front&nbsp;&lt;&nbsp;myPosition&nbsp;||&nbsp;front&nbsp;==&nbsp;nextEmpty)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mycv-&gt;Wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;delete&nbsp;self;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;The&nbsp;condition&nbsp;variable&nbsp;is&nbsp;no&nbsp;longer&nbsp;needed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item&nbsp;=&nbsp;items[front&nbsp;%&nbsp;size];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;front++;
&nbsp;
&nbsp;//&nbsp;Wake&nbsp;up&nbsp;the&nbsp;next&nbsp;thread&nbsp;waiting&nbsp;in&nbsp;insert,&nbsp;if&nbsp;any.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nextWaiter&nbsp;=&nbsp;insertQueue.removeFromFront();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(nextWaiter&nbsp;!=&nbsp;NULL)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nextWaiter-&gt;Signal(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;item;
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.14: </B>An implementation of FIFO Blocking Bounded Buffer using Mesa semantics. ConditionQueue is a linked list of condition variables.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The easiest way to do this is to create a condition variable for each separate waiting thread. Then, you can be precise as to which thread to wake up! Although you might be worried that this would be space inefficient, on modern computer systems a condition variable (or lock) takes up just a few words of DRAM; it is small compared to the rest of the storage needed per thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The outline of the solution is as follows: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Create a condition variable for every waiter. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Put condition variables on a queue in FIFO order. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Signal wakes up the thread at the front of the queue. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Be CAREFUL about spurious wakeups!</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We give an implementation of FIFOBBQ::remove in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6500114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">; insert is similar. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The implementation easily extends to the case where we want the queue to be last in first out (LIFO) rather than FIFO, or if want it to wake up threads in some priority order. With Hoare semantics, this is not as easy; we would need to have a different implementation of CV for each different queueing discipline, rather than leaving it to those few applications where the specific order matters. </FONT><A id=x1-65002r101 name=x1-65002r101></A></P><A id=x1-660007 name=x1-660007>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7 Implementing Synchronization Objects</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Now that we have described locks and condition variables and shown how to use them in shared objects, we turn to how to implement these important building blocks. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Recall from Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> that threads can be implemented in the kernel or at user level. We start by describing how to implement synchronization for kernel threads; at the end of this section we discuss the changes needed to support these abstractions for user-level threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Both locks and condition variables have state. For locks, this is the state of the lock (FREE&nbsp;or BUSY) and a queue of zero or more threads waiting for the lock to become FREE. For condition variables, the state is the queue of threads waiting to be signaled. Either way, the challenge is to atomically modify those data structures. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Too Much Milk discussion showed that it is both complex and costly to implement atomic actions with just memory reads and writes. Therefore, modern implementations use more powerful hardware primitives that let us atomically read, modify, and write pieces of state. We use two hardware primitives: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Disabling interrupts.</B> On a single processor, we can make a sequence of instructions atomic by disabling interrupts on that single processor. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Atomic read-modify-write instructions.</B> On a multiprocessor, disabling interrupts is insufficient to provide atomicity. Instead, architectures provide special instructions to atomically read and update a word of memory. These instructions are globally atomic with respect to the instructions on every processor.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Each of these primitives also serves as a memory barrier; they inform the compiler and hardware that all prior instructions must complete before the atomic instruction is executed. </FONT><A id=x1-66001r109 name=x1-66001r109></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7.1 </FONT><A id=x1-670001 name=x1-670001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Uniprocessor Locks by Disabling Interrupts</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">On a uniprocessor, any sequence of instructions by one thread appears atomic to other threads if no context switch occurs in the middle of the sequence. So, on a uniprocessor, a thread can make a sequence of actions atomic by disabling interrupts (and refraining from calling thread library functions that can trigger a context switch) during the sequence. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This observation suggests a trivial &#8212; but seriously limited &#8212; approach to implementing locks on a uniprocessor: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Lock::acquire()&nbsp;{&nbsp;disableInterrupts();&nbsp;}
   &nbsp;
   &nbsp;Lock::release()&nbsp;{&nbsp;enableInterrupts();&nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This implementation does provide the mutual exclusion property we need from locks. Some uniprocessor kernels use this simple approach, but it does not suffice as a general implementation for locks. If the code sequence the lock protects runs for a long time, interrupts will be disabled for that long. This will prevent other threads from running, and it will make the system unresponsive to handling user inputs or other real-time tasks. Furthermore, although this approach can work in the kernel where all code is (presumably) carefully crafted and trusted to release the lock quickly, we cannot let untrusted user-level code run with interrupts turned off since a malicious or buggy program could then monopolize the processor. </FONT><A id=x1-67001r112 name=x1-67001r112></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7.2 </FONT><A id=x1-680002 name=x1-680002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Uniprocessor Queueing Locks</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A more general solution is based on the observation that if the lock is BUSY, there is no point in running the acquiring thread until the lock is free. Instead, we should context switch to the next ready thread. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The implementation briefly disables interrupts to protect the lock&#8217;s data structures, but re-enables them once a thread has acquired the lock or determined that the lock is BUSY. The Lock implementation shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6800115"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.15</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this approach. If a lock is BUSY&nbsp;when a thread tries to acquire it, the thread moves its TCB onto the lock&#8217;s waiting list. The thread then suspends itself and switches to the next runnable thread. The call to suspend does not return until the thread is put back on the ready list, e.g., until some thread calls Lock::release. </FONT><A id=x1-6800115 name=x1-6800115></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;Lock&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;value&nbsp;=&nbsp;FREE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;waiting;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;release();
&nbsp;}
&nbsp;
&nbsp;Lock::acquire()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCB&nbsp;*chosenTCB;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disableInterrupts();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(value&nbsp;==&nbsp;BUSY)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waiting.add(runningThread);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runningThread-&gt;state&nbsp;=&nbsp;WAITING;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chosenTCB&nbsp;=&nbsp;readyList.remove();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_switch(runningThread,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chosenTCB);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runningThread-&gt;state&nbsp;=&nbsp;RUNNING;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;BUSY;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enableInterrupts();
&nbsp;}
&nbsp;
&nbsp;Lock::release()&nbsp;{
&nbsp;//&nbsp;next&nbsp;thread&nbsp;to&nbsp;hold&nbsp;lock
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCB&nbsp;*next;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disableInterrupts();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(waiting.notEmpty())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;move&nbsp;one&nbsp;TCB&nbsp;from&nbsp;waiting
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;to&nbsp;ready
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next&nbsp;=&nbsp;waiting.remove();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next-&gt;state&nbsp;=&nbsp;READY;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readyList.add(next);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;FREE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enableInterrupts();
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.15: </B>Pseudo-code for a uniprocessor queueing lock. Temporarily disabling interrupts provides atomic access to the data structures implementing the lock. suspend(oldTCB, newTCB) switches from the current thread to the next to be run. It returns only after some other thread calls release and moves it to the ready list.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In our implementation, if a thread is waiting for the lock, a call to release does not set value to FREE. Instead, it leaves value as BUSY. The woken thread is guaranteed to be the next that executes the critical section. This arrangement ensures freedom from starvation. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>WARNING</B>: This optimization is specific to this implementation. Users of locks should not make assumptions about the order in which waiting threads acquire a lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>In Lock::acquire, thread_switch is called with interrupts turned off. Who turns them back on? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>The next thread to run re-enables interrupts.</B> In particular, most implementations of thread systems enforce the invariant that a thread always disables interrupts before performing a context switch. As a result, interrupts are always disabled when the thread runs again after a context switch. Thus, whenever a thread returns from a context switch, it must re-enable interrupts. For example, the Lock::acquire code in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6800115"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.15</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> re-enables interrupts before returning; the yield implementation in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> disables interrupts before the context switch and then re-enables them afterwards. &#9633; </FONT><A id=x1-68002r113 name=x1-68002r113></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7.3 </FONT><A id=x1-690003 name=x1-690003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Multiprocessor Spinlocks</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">On a multiprocessor, however, disabling interrupts is insufficient. Even when interrupts are turned off on one processor, other threads are running concurrently. Operations by a thread on one processor are interleaved with operations by other threads on other processors. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since turning off interrupts is insufficient, most processor architectures provide <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:atomic read-modify-write instruction"}'>atomic read-modify-write instructions</A></EM> to support synchronization. These instructions can read a value from a memory location to a register, modify the value, and write the modified value to memory atomically with respect to all instructions on other processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Implementing read-modify-write instructions</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Students often ask at this point how the processor hardware implements atomic instructions such as test-and-set. If each processor has its own cache, what is to keep two processors from reading and updating the same location at the same time? Although a complete explanation is beyond the scope of this textbook, the hardware uses the same mechanism as it uses for cache coherence. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Every entry in a processor cache has a state, either <EM>exclusive</EM> or <EM>read-only</EM>. If any other processors have a cached copy of the data, it must be <EM>read-only</EM> everywhere. To modify a shared memory location, the processor must have an <EM>exclusive</EM> copy of the data; no other cache is allowed to have a copy. Otherwise, one processor could read an out-of-date value for some location that another processor has already updated. To read or write a location that is stored <EM>exclusive</EM> in some other cache, the processor needs to fetch the latest value from that cache. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Read-modify-write instructions piggyback on this mechanism. To execute one of these instructions, the hardware acquires an <EM>exclusive</EM> copy of the memory, removing copies from all other caches. Then the instruction executes on the local copy; after the instruction completes, other processors are allowed to read the result by fetching the latest value. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As an example, some architectures provide a <EM>test-and-set</EM> instruction, which atomically reads a value from memory to a register and writes the value 1 to that memory location. </FONT><A id=x1-6900116 name=x1-6900116></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;SpinLock&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;value&nbsp;=&nbsp;0;&nbsp;//&nbsp;0&nbsp;=&nbsp;FREE;&nbsp;1&nbsp;=&nbsp;BUSY
&nbsp;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;acquire()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(test_and_set(&amp;value))&nbsp;//&nbsp;while&nbsp;BUSY
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//&nbsp;spin
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;release()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;0;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.16: </B>A multiprocessor spinlock implementation using test-and-set.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-6900116"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> implements a lock using test_and_set. This lock is called a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:spinlock"}'>spinlock</A></EM> because a thread waiting for a BUSY&nbsp;lock &#8220;spins&#8221; (busy-waits) in a tight loop until some other lock releases the lock. This approach is inefficient if locks are held for long periods. However, for locks that are only held for short periods (i.e., less time than a context switch would take), spinlocks make sense. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Interrupt handlers and spinlocks</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Whenever an interrupt handler accesses shared data, that data must be protected by a spinlock instead of a queueing lock. As we explained in Chapter&nbsp;2 and Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, interrupt handlers are not threads: they must run to completion without blocking so that the hardware can deliver the next interrupt. With a queueing lock, the lock might be held when the interrupt handler starts, making it impossible for the interrupt handler to work correctly. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Whenever any thread acquires a spinlock used within an interrupt handler, the thread <EM>must</EM> disable interrupts first. Otherwise, deadlock can result if the interrupt arrives at an inopportune moment. The handler could spin forever waiting for a lock held by the thread it interrupted. Most likely, the system would need to be rebooted to clear the problem. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To avoid these types of errors, most operating systems keep interrupt handlers extremely simple. For example, many interrupt handlers simply wake up a thread to do the heavy lifting of managing the I/O device. Waking up a thread requires mutually exclusive access to the ready list, protected by a spinlock that is never used without first disabling interrupts. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-69002r115 name=x1-69002r115></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7.4 </FONT><A id=x1-700004 name=x1-700004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Multiprocessor Queueing Locks</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Often, we need to support critical sections of varying length. For example, we may want a general solution that does not make assumptions about the running time of methods that hold locks. </FONT><A id=x1-7000117 name=x1-7000117></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;Lock&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;value&nbsp;=&nbsp;FREE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SpinLock&nbsp;spinLock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;waiting;
&nbsp;&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;release();
&nbsp;}
&nbsp;
&nbsp;Lock::acquire()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spinLock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(value&nbsp;!=&nbsp;FREE)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waiting.add(runningThread);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduler.suspend(&amp;spinLock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;scheduler&nbsp;releases&nbsp;spinLock
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;BUSY;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spinLock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;
&nbsp;void&nbsp;Lock::release()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCB&nbsp;*next;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spinLock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(waiting.notEmpty())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;next&nbsp;=&nbsp;waiting.remove();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduler.makeReady(next);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;=&nbsp;FREE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spinLock.release();
&nbsp;}
</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;Scheduler&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;readyList;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SpinLock&nbsp;schedulerSpinLock;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;suspend(SpinLock&nbsp;*lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;makeReady(Thread&nbsp;*thread);
&nbsp;}
&nbsp;
&nbsp;void
&nbsp;Scheduler::suspend(SpinLock&nbsp;*lock)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCB&nbsp;*chosenTCB;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disableInterrupts();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;schedulerSpinLock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runningThread-&gt;state&nbsp;=&nbsp;WAITING;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chosenTCB&nbsp;=&nbsp;readyList.getNextThread();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread_switch(runningThread,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chosenTCB);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runningThread-&gt;state&nbsp;=&nbsp;RUNNING;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;schedulerSpinLock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enableInterrupts();
&nbsp;}
&nbsp;
&nbsp;void
&nbsp;Scheduler::makeReady(TCB&nbsp;*thread)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disableInterrupts();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;schedulerSpinLock.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readyList.add(thread);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread-&gt;state&nbsp;=&nbsp;READY;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;schedulerSpinLock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enableInterrupts();
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.17: </B>Pseudo-code for a multiprocessor queueing lock. Both the scheduler and the lock use spinlocks to protect their internal data structures. Any thread that tries to acquire the lock when it is BUSY&nbsp;is put on a queue for later wakeup. Care is needed to prevent the waiting thread from being put back on the ready list before it has completed the thread_switch.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We cannot completely eliminate busy-waiting on a multiprocessor, but we can minimize it. As we mentioned, the scheduler ready list needs a spinlock. The scheduler holds this spinlock for only a few instructions; further, if the ready list spinlock is BUSY, there is no point in trying to switch to a different thread, as that would require access to the ready list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To reduce contention on the ready list spinlock, we use a <EM>separate</EM> spinlock to guard access to each lock&#8217;s internal state. Once a thread holds the lock&#8217;s spinlock, the thread can inspect and update the lock&#8217;s state. If the lock is FREE, the thread sets the value and releases its spinlock. If the lock is BUSY, more work is needed: we need to put the current thread on the waiting list for the lock, suspend the current thread, and switch to a new thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Careful sequencing is needed, however, as shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-7000117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. To suspend a thread on a multiprocessor, we need to first disable interrupts to ensure the thread is not preempted while holding the ready list spinlock. We then acquire the ready list spinlock, and <EM>only then</EM> is it safe to release the lock&#8217;s spinlock and switch to a new thread. The ready list spinlock is released by the next thread to run. Otherwise, a different thread on another processor might put the waiting thread back on the ready list (and start it running) before the waiting thread has completed its context switch. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Later, when the lock is released, if any threads are waiting for the lock, one of them is moved off the lock&#8217;s waiting list to the scheduler&#8217;s ready list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What might happen if we released the Lock&#8217;s spinlock before the call to suspend? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The basic issue is that we want to make sure the acquiring thread finishes suspending itself before a thread releasing the lock tries to reschedule it. If we allowed makeReady to run before suspend, makeReady would mark the acquring thread READY, but suspend would then change the thread&#8217;s state to WAITING. The acquiring thread would then be stuck in the WAITING state forever. Since this sequence would happen very rarely, it would be extremely difficult to locate the problem. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>NOTE</B>: In the implementation in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-7000117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, the single scheduler spinlock can become a bottleneck as the number of processors increases. Instead, as we explain in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-780006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, most systems have one ready list per processor, each protected by a different spinlock. Different processors can then simultaneously add and remove threads to different lists. Typically, the WAITING&nbsp;thread is placed on the ready list of the same processor where it had previously been RUNNING; this improves cache performance as that processor&#8217;s cache may still contain code and data from the last time the thread ran. Putting the thread back on the same ready list also prevents the thread from being run by any other processor before the thread has completed its context switch. Once it is READY, any idle processor can run the thread by acquiring the spinlock of the ready list where it is enqueued, removing the thread, and releasing the spinlock. </FONT><A id=x1-70002r117 name=x1-70002r117></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7.5 </FONT><A id=x1-710005 name=x1-710005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Linux 2.6 Kernel Mutex Lock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">We illustrate how locks are implemented in practice by examining the Linux 2.6 kernel. The Linux code closely follows the approach we described above, except that it is <EM>optimized for the common case</EM>. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In Linux, most locks are FREE&nbsp;most of the time. Further, even if a lock is BUSY, it is likely that no other thread is waiting for it. The alternative, that locks are often BUSY, or have long queues of threads waiting for them, means that any thread that needs the lock will usually need to wait, slowing the system down. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Linux implementation of locks takes advantage of this by providing an extremely fast path for the case when the thread does not need to wait for the lock in acquire, and when there is no thread not need to wake up a thread in release. A slow path, similar to Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-7000117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, is used for all other cases. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Further, having a fast path for acquiring a FREE&nbsp;lock, and releasing a lock with no waiting thread, is also a concern for user-level thread libraries, discussed below. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To optimize the common case path, Linux takes advantage of hardware-specific features of the x86. The x86 supports a large number of different read-modify-write instructions, including atomic decrement (subtract one from the memory location, returning the previous value), atomic increment, atomic exchange (swap the value of the memory location with the value stored in a register), and atomic test-and-set. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The key idea is to design the lock data structures to allow the lock to be acquired and released on the fast path <EM>without</EM> first acquiring the spinlock or disabling interrupts. The slowpath does require acquiring the spinlock. Instead of being binary, the lock value is an integer count with three states: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;struct&nbsp;mutex&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/*&nbsp;1:&nbsp;unlocked,&nbsp;0:&nbsp;locked,&nbsp;negative:&nbsp;locked,&nbsp;possible&nbsp;waiters&nbsp;*/
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;atomic_t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spinlock_t&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wait_lock;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct&nbsp;list_head&nbsp;&nbsp;wait_list;
   &nbsp;};</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Linux lock acquire code is a macro (to avoid making a procedure call on the fast path) that translates to a short sequence of instructions. The x86 lock prefix before the decl instruction signifies to the processor that the instruction should be executed atomically. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock&nbsp;decl&nbsp;(%eax)&nbsp;&nbsp;//&nbsp;atomic&nbsp;decrement&nbsp;of&nbsp;a&nbsp;memory&nbsp;location
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;address&nbsp;in&nbsp;%eax&nbsp;is&nbsp;pointer&nbsp;to&nbsp;lock-&gt;count
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jns&nbsp;1f&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;jump&nbsp;if&nbsp;not&nbsp;signed&nbsp;(if&nbsp;value&nbsp;is&nbsp;now&nbsp;0)
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;call&nbsp;slowpath_acquire
   &nbsp;1:</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the lock was FREE, the lock is acquired with only two instructions; if the lock was BUSY, the code leaves count &lt; 0 and invokes a separate routine to handle the slow path. The slow path disables preemption, acquires the spinlock, puts the thread on the lock wait queue, and then re-checks whether the lock has been released in the meantime. For this, it uses the atomic exchange instruction: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;for&nbsp;(;;)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/*
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;Lets&nbsp;try&nbsp;to&nbsp;take&nbsp;the&nbsp;lock&nbsp;again&nbsp;-&nbsp;this&nbsp;is&nbsp;needed&nbsp;even&nbsp;if
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;we&nbsp;get&nbsp;here&nbsp;for&nbsp;the&nbsp;first&nbsp;time&nbsp;(shortly&nbsp;after&nbsp;failing&nbsp;to
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;acquire&nbsp;the&nbsp;lock),&nbsp;to&nbsp;make&nbsp;sure&nbsp;that&nbsp;we&nbsp;get&nbsp;a&nbsp;wakeup&nbsp;once
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;it&#8217;s&nbsp;unlocked.&nbsp;Later&nbsp;on,&nbsp;if&nbsp;we&nbsp;sleep,&nbsp;this&nbsp;is&nbsp;the
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;operation&nbsp;that&nbsp;gives&nbsp;us&nbsp;the&nbsp;lock.&nbsp;We&nbsp;xchg&nbsp;it&nbsp;to&nbsp;-1,&nbsp;so
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;that&nbsp;when&nbsp;we&nbsp;release&nbsp;the&nbsp;lock,&nbsp;we&nbsp;properly&nbsp;wake&nbsp;up&nbsp;the
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;other&nbsp;waiters:
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(atomic_xchg(&amp;lock-&gt;count,&nbsp;-1)&nbsp;==&nbsp;1)
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break;
   &nbsp;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/*&nbsp;didn&#8217;t&nbsp;get&nbsp;the&nbsp;lock,&nbsp;go&nbsp;to&nbsp;sleep:&nbsp;*/
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If successful, the lock is acquired. If unsuccessful, the thread releases the spinlock and switches to the next ready thread. When the thread returns from suspend, unlike in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-7000117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, the lock may not be FREE, and so the thread must try again. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Eventually, the thread breaks out of the loop, which means that it found a moment when the lock was FREE&nbsp;(lock-&gt;count = 1), and at that moment it set the lock to the &#8220;busy, possible waiters&#8221; state (by setting count = -1). The thread now has the lock, and it cleans up by resetting count = 0 if there are no other waiters. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;/*&nbsp;set&nbsp;it&nbsp;to&nbsp;0&nbsp;if&nbsp;there&nbsp;are&nbsp;no&nbsp;waiters&nbsp;left:&nbsp;*/
   &nbsp;if&nbsp;(list_empty(&amp;lock-&gt;wait_list))
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;atomic_set(&amp;lock-&gt;count,&nbsp;0);</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It then releases the spinlock and re-enables preemptions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On release, the fast path is two inlined instructions if the lock value was 0 (the lock has no waiters). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock&nbsp;incl&nbsp;(%eax)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;atomic&nbsp;increment
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jg&nbsp;1f&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;jump&nbsp;if&nbsp;new&nbsp;value&nbsp;is&nbsp;1
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;call&nbsp;slowpath_release
   &nbsp;1:</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On the slow path, count was negative. The increment instruction leaves the lock BUSY. Then, the thread acquires the spinlock, sets the count to be FREE, and wakes up one of the waiting threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;spin_lock_mutex(&amp;lock-&gt;wait_lock,&nbsp;flags);
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/*
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*&nbsp;Unlock&nbsp;lock&nbsp;here
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*/
   &nbsp;atomic_set(&amp;lock-&gt;count,&nbsp;1);
   &nbsp;if&nbsp;(!list_empty(&amp;lock-&gt;wait_list))&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct&nbsp;mutex_waiter&nbsp;*waiter&nbsp;=
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;list_entry(lock-&gt;wait_list.next,
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;struct&nbsp;mutex_waiter,&nbsp;list);
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wake_up_process(waiter-&gt;task);
   &nbsp;}
   &nbsp;spin_unlock_mutex(&amp;lock-&gt;wait_lock,&nbsp;flags);</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice that this function always sets count to 1, even if there are waiting threads. As a result, a new thread may swoop in and acquire the lock on its fast path, setting count = 0. In this case, the waiting thread is still woken up, and when it eventually runs, the main loop above will set count = -1. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This example demonstrates that acquiring and releasing a lock can be inexpensive. Programmers sometimes go to great lengths to avoid acquiring a lock in a particular situation. However, the reasoning in such cases can be subtle, and omitting needed locks is dangerous. In cases where there is little contention, avoiding locks is unlikely to significantly improve performance, so it is usually better just to keep things simple and rely on locks to ensure mutual exclusion when accessing shared state. </FONT><A id=x1-71001r119 name=x1-71001r119></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7.6 </FONT><A id=x1-720006 name=x1-720006></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Condition Variables</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">We can implement condition variables using a similar approach to the one used to implement locks, with one simplification: since the lock is held whenever the <TT>wait</TT>, <TT>signal</TT>, or <TT>broadcast</TT>&nbsp;is called, we already have mutually exclusive access to the condition wait queue. As with locks, care is needed to prevent a waiting thread from being put back on the ready list until it has completed its context switch; we can accomplish this by acquiring the scheduler spinlock <EM>before</EM> we release the monitor lock. Another thread may acquire the monitor lock and start to signal the waiting thread, but it will not be able to complete the signal until the scheduler lock is released immediately after the context switch. </FONT><A id=x1-7200118 name=x1-7200118></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;CV&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;waiting;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;wait(Lock&nbsp;*lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;signal();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;broadcast();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Monitor&nbsp;lock&nbsp;is&nbsp;held&nbsp;by&nbsp;current&nbsp;thread.
&nbsp;void&nbsp;CV::wait(Lock&nbsp;*lock)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert(lock.isHeld());
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waiting.add(myTCB);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Switch&nbsp;to&nbsp;new&nbsp;thread&nbsp;and&nbsp;release&nbsp;lock.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduler.suspend(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;acquire();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Monitor&nbsp;lock&nbsp;is&nbsp;held&nbsp;by&nbsp;current&nbsp;thread.
&nbsp;void&nbsp;CV::signal()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(waiting.notEmpty())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread&nbsp;=&nbsp;waiting.remove();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduler.makeReady(thread);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;
&nbsp;void&nbsp;CV::broadcast()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(waiting.notEmpty())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;thread&nbsp;=&nbsp;waiting.remove();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scheduler.makeReady(thread);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;5.18: </B>Pseudo-code for implementing a condition variable. suspend and makeReady are defined in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-7000117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-7200118"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows an implementation with Mesa semantics &#8212; when we signal a waiting thread, that thread becomes READY, but it may not run immediately and must still re-acquire the monitor lock. It is possible for another thread to acquire the monitor lock first and to change the state guarded by the lock before the waiting thread returns from CV::wait. </FONT><A id=x1-72002r120 name=x1-72002r120></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.7.7 </FONT><A id=x1-730007 name=x1-730007></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing Application-level Synchronization</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">The preceding discussion focused on implementing locks and condition variables for kernel threads. In that case, everything (code, shared state, lock data structures, thread control blocks, and the ready list) is in kernel memory, and all threads run in kernel mode. Fortunately, although some details change, the same basic approach works when we implement locks and condition variables for use by threads that run at user level. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Recall from Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> that there are two ways of supporting application-level concurrency: via system calls to access kernel thread operations or via a user-level thread scheduler. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Kernel-Managed Threads.</B> With kernel-managed threads, the kernel provides threads to a process and manages the thread ready list. The kernel scheduler needs to know when a thread is waiting for a lock or condition variable so that it can suspend the thread and switch to the next ready thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the simplest case, we can place the lock and condition variable data structures, including the waiting lists, in the kernel&#8217;s address space. Each method call on the synchronization object translates to a system call. Then, the implementations described above for kernel-level locks and condition variables can be used without significant change. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A more sophisticated approach splits the lock&#8217;s state and implementation into a fast path and slow path, similar to the Linux lock described above. For example, each lock has two data structures: (i) the process&#8217;s address space holds something similar to the count field and (ii) the kernel holds the spinlock and wait_list queue. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Then, acquiring a FREE&nbsp;lock or releasing a lock with no waiting threads takes a few instructions at user level, with no system call. The slow path still needs a system call (e.g., when a waiting thread needs to suspend execution). We leave the details of the implementation as an exercise for the reader. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>User-Managed Threads.</B> In a </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-300002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">thread library that operates completely at user level</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, the library creates multiple kernel threads to serve as virtual processors, and then multiplexes user-level threads over those virtual processors. This situation is similar to kernel threads, except operating inside the process&#8217;s address space rather than in the kernel&#8217;s address space. In particular, the code, shared state, lock and condition variable data structures, thread control blocks, and the ready list are in the process&#8217;s address space. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The only significant change has to do with disabling interrupts. Obviously, a user-level thread package cannot disable system-level interrupts; the kernel cannot allow an untrusted process to disable interrupts and potentially run forever. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, the thread library only needs to disable upcalls from the operating system; these are used to trigger thread preemption and other operations in the user-level scheduler, and they could cause inconsistency if they occur while the library is modifying scheduler data structures. Most modern operating systems have a way to temporarily disable upcalls, and then to deliver those upcalls once it is safe to do so. By ensuring the user-level scheduler and upcall handler cannot run at the same time, the fast path mutex implementation described above can be used here as well. </FONT><A id=x1-73001r111 name=x1-73001r111></A></P><A id=x1-740008 name=x1-740008>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.8 Semaphores Considered Harmful</FONT></H3></A>
<DIV class=makequote><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;During system conception it transpired that we used the semaphores in two completely different ways. The difference is so marked that, looking back, one wonders whether it was really fair to present the two ways as uses of the very same primitives. On the one hand, we have the semaphores used for mutual exclusion, on the other hand, the private semaphores.&#8221;<BR>(From Dijkstra &#8220;The structure of the &#8217;THE&#8217;-Multiprogramming System&#8221; <EM>Communications of the ACM</EM> v. 11 n. 5 May 1968.)</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This book focuses on constructing shared objects using locks and condition variables for synchronization. However, over the years, many different synchronization primitives have been proposed, including communicating sequential processes, event delivery, message passing, and so forth. It is important to realize that none of these are more powerful than using locks and condition variables; a program using any of these paradigms can be mapped to monitors using straightforward transformations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One type of synchronization, a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:semaphore"}'>semaphore</A></EM>, is worth discussing in detail since it is still widely used. Semaphores were introduced by Dijkstra to provide synchronization in the THE operating system, which (among other advances) explored structured ways of using concurrency in operating system design. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Semaphores are defined as follows: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A semaphore has a non-negative value. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When a semaphore is created, its value can be initialized to any non-negative integer. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Semaphore::P() waits until the value is positive. Then, it atomically decrements value by 1 and returns. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Semaphore::V() atomically increments the value by 1. If any threads are waiting in P, one is enabled, so that its call to P succeeds at decrementing the value and returns. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">No other operations are allowed on a semaphore; in particular, no thread can directly read the current value of the semaphore.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Note that Semaphore::P is an atomic operation: the read that observes the positive value is atomic with the update that decrements it. As a result, semaphores can never have a negative value, even when multiple threads call P concurrently. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Likewise, if V occurs when there is a waiting thread in P, then P&#8217;s increment and V&#8217;s decrement of value are atomic: no other thread can observe the incremented value, and the thread in P is guaranteed to decrement the value and return. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Given this definition, semaphores can be used for either mutual exclusion (like locks) or general waiting for another thread to do something (a bit like condition variables). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To use a semaphore as a mutual exclusion lock, initialize it to 1. Then, Semaphore::P is equivalent to Lock::acquire, and Semaphore::V is equivalent to Lock::release. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using a semaphore for more general waiting is trickier. A useful analogy for semaphores is thread_join. With thread_join, the precise order of events does not matter: if the forked thread finishes before the parent thread calls thread_join, then the call returns right away. On the other hand, if the parent calls thread_join&nbsp;first, then it waits until the thread finishes, and then returns. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Semaphore P and V can be set up to behave similarly. Typically (but not always), you initialize the semaphore to 0. Then, each call to Semaphore::P waits for the corresponding thread to call V. If the V is called first, then P returns immediately. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The difficulty comes when trying to coordinate shared state (needing mutual exclusion) with general waiting. From a distance, Semaphore::P is <EM>similar to</EM> CV::wait(&amp;lock) and Semaphore::V is <EM>similar to</EM> CV::signal. However, there are important differences. First, CV::wait(&amp;lock) atomically releases the monitor lock, so that you can safely check the shared object&#8217;s state and then atomically suspend execution. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By contrast, Semaphore::P does <EM>not</EM> release an associated mutual exclusion lock. Typically, the lock is released before the call to P; otherwise, no other thread can access the shared state until the thread resumes. The programmer must carefully construct the program to work properly in this case. Second, whereas a condition variable is stateless, a semaphore has a value. If no threads are waiting, a call to CV::signal has no effect, while a call to Semaphore::V increments the value. This causes the next call to Semaphore::P to proceed without blocking. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Semaphores considered harmful.</B> Our view is that programming with locks and condition variables is superior to programming with semaphores. We advise you to always write your code using those synchronization variables for two reasons. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">First, using separate lock and condition variable classes makes code more self-documenting and easier to read. As the quote from Dijkstra notes, two different abstractions are needed, and code is clearer when the role of each synchronization variable is made clear through explicit typing. For example, it is much easier to verify that every lock acquire is paired with a lock release, if they are not mixed with other calls to P and V for general waiting. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Second, a stateless condition variable bound to a lock is a better abstraction for generalized waiting than a semaphore. By binding a condition variable to a lock, we can conveniently wait on any arbitrary predicate on an object&#8217;s state. In contrast, semaphores rely on the programmer to carefully map the object&#8217;s state to the semaphore&#8217;s value so that a decision to wait or proceed in P can be made entirely based on the value, without holding a lock or examining the rest of the shared object&#8217;s state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although we do not recommend writing new code with semaphores, code based on semaphores is not uncommon, especially in operating systems. So, it is important to understand the semantics of semaphores and be able to read and understand semaphore-based code written by others. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>NOTE</B>: <B>Semaphores in interrupt handlers.</B> In one situation, semaphores are superior to condition variables and locks: synchronizing communication between an I/O device and threads waiting for I/O completion. Typically, the hardware communicates with the device driver via a shared in-memory data structure. This data structure is read and written concurrently by both hardware and the kernel, but the shared access cannot be coordinated with a software lock. Instead, the hardware and device drivers use carefully designed atomic memory operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If a hardware device needs attention, e.g., because a network packet has arrived that needs handling, or a disk request has completed, the hardware updates the shared data structure and starts an interrupt handler. The interrupt handler is often simple: it just wakes up a waiting thread and returns. For this, one might consider using a condition variable and calling <TT>signal</TT>&nbsp;without holding the lock (this is sometimes called a <EM>naked notify</EM>). Unfortunately, there is a corner case: suppose that the operating system thread first checks the data structure, sees that no work is currently needed, and is just about to call <TT>wait</TT>&nbsp;on the condition variable. At that moment, the hardware updates the data structure with the new work and triggers the interrupt handler to call <TT>signal</TT>. Because the thread has not called <TT>wait</TT>&nbsp;yet, the <TT>signal</TT>&nbsp;has no effect. Thus, when the thread calls <TT>wait</TT>, the signal has already occurred, and the thread waits &#8212; possibly for a long time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A common solution is for device interrupts to use semaphores instead. Because semaphores are stateful, it does not matter whether the thread calls P or the interrupt handler calls V first: the result is the same, the V cannot be lost. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To help illustrate the difference between semaphores and condition variables, we consider four candidate implementations of condition variables using semaphores. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose you are writing concurrent application software on an operating system that only provides semaphores. Does the following code correctly implement condition variables? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;CV::wait(Lock&nbsp;*lock)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;release();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.P();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;acquire();
   &nbsp;}
   &nbsp;
   &nbsp;void&nbsp;CV::signal()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.V();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>No. Condition variables are stateless, while semaphores have state.</B> We can illustrate this difference with a counterexample. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">What happens if a thread calls <TT>signal</TT>&nbsp;and no one is waiting? Nothing. What happens if another thread later calls wait? The thread waits. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By contrast, consider what happens with a semaphore. What happens if a thread calls V and no one is waiting? The value of the semaphore is incremented. What happens if a thread later calls P? The value of the semaphore is decremented, and the thread continues. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In other words, P and V are commutative. The result is the same no matter what order they occur. Condition variables are not commutative: <TT>wait</TT>&nbsp;does not return until the next <TT>signal</TT>. This is why condition variables must be accessed while holding a lock &#8212; code using a condition variable needs to access shared state variables to do its job. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">With condition variables, if a thread calls <TT>signal</TT>&nbsp;a thousand times, when no one is waiting, the next <TT>wait</TT>&nbsp;will still go to sleep. With the above code, the next thousand threads that <TT>wait</TT>&nbsp;will return immediately. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What about the following code? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;CV::wait(Lock&nbsp;*lock)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;release();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.P();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;acquire();
   &nbsp;}
   &nbsp;
   &nbsp;void&nbsp;CV::signal()&nbsp;{
   &nbsp;&nbsp;&nbsp;if&nbsp;(!semaphore.queueEmpty())
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.V();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>Closer, but still no.</B> For one, the definition of a semaphore does not allow users of the semaphore to look at the contents of the semaphore queue. But more importantly, there is a race condition. Once the lock is released, some other thread can slip in, <TT>acquire</TT>&nbsp;the lock and call <TT>signal</TT>&nbsp;before the waiting thread gets to call P. In that case, the queue is empty, so the waiter never exits the while loop. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Instead, the definition of CV::wait is that the lock is released and the thread goes to sleep atomically. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>What about the following code? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;CV::wait(Lock&nbsp;*lock)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitQueue.append(myTCB);
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;release();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.P();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock-&gt;acquire();
   &nbsp;}
   &nbsp;
   &nbsp;void&nbsp;CV::signal()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(!waitQueue.isEmpty())
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.V();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>Very close but still no.</B> There is still a race condition. Suppose a thread calls <TT>wait</TT>, and releases the lock. Then another thread acquires the lock and calls <TT>signal</TT>. With condition variables, the waiter should wake up, but with the implementation above, a third thread could swoop in, acquire the lock, call <TT>wait</TT>, and decrement the semaphore before the first waiter has a chance to run. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For some programs, this difference would not be noticeable, but for others, it could cause problems. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Is it possible to implement condition variables using semaphores? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>Yes, using the technique we outlined for implementing the FIFO bounded buffer: create a semaphore for each waiter and then wake up exactly the right waiter.</B> This solution was developed by Andrew Birrell in order to implement condition variables on top of Microsoft Windows before it supported them natively. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Put&nbsp;thread&nbsp;on&nbsp;queue&nbsp;of&nbsp;waiting
&nbsp;//&nbsp;threads.
&nbsp;void&nbsp;CV::wait(Lock&nbsp;*lock)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore&nbsp;=&nbsp;new&nbsp;Semaphore(0);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;waitQueue.Append(semaphore);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.P();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Wake&nbsp;up&nbsp;one&nbsp;waiter&nbsp;if&nbsp;any.
&nbsp;void&nbsp;CV::signal()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(!waitQueue.isEmpty())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore&nbsp;=&nbsp;queue.Remove();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semaphore.V();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
   </FONT></PRE><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT><A id=x1-74001r123 name=x1-74001r123></A><A id=x1-750009 name=x1-750009>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.9 Summary and Future Directions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">This chapter advocates using a systematic, structured approach to writing multi-threaded code that shares state across threads. The approach, shared objects with concurrent access managed with locks and condition variables, has stood the test of time. Using shared objects makes reasoning about multi-threaded programs vastly simpler than it would be if we tried to reason about the possible interleavings of individual loads and stores. Further, by following a systematic approach, we make it possible for others to read, understand, maintain, and change the multi-threaded code we write. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this chapter, we have discussed: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Race conditions.</B> The fundamental challenge to writing multi-threaded code that uses shared data is that the behavior of the program may depend on the precise ordering of operations executed by each thread. This non-deterministic behavior is difficult to reason about, reproduce, and debug. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Locks and condition variables.</B> Two useful sychronization abstractions are locks, providing mutual exclusion, and condition variables, for waiting for shared state to change. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>A methodology for writing shared objects.</B> Using locks and condition variables, we outlined a sequence of steps to writing correct synchronization code for coordinating access to shared objects. Following this methodology has proven enormously helpful for students in our classes by reducing the likelihood of design errors. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Implementations of synchronization.</B> Locks and condition variables can be efficiently implemented using hardware support for atomic read-modify-write instructions and, where necessary, the ability to temporarily defer hardware interrupts. In particular, we showed that the overhead of acquiring and releasing a non-contested lock can be as low as four instructions. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Semaphores.</B> Semaphores are a widely implemented alternative to locks and condition variables, with a constructive role in managing hardware I/O interrupts.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In short, this chapter defines a set of core skills that almost any programmer will use over and over again during the coming decade or longer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">That is not the whole story. As the next chapter will discuss, complex systems often include many shared objects and threads. This poses new challenges: synchronizing operations that span multiple shared objects, avoiding deadlocks in which a set of threads are all waiting for each other to do something, and maximizing performance when large numbers of threads are contending for a single object. </FONT><A id=x1-75001r122 name=x1-75001r122></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">5.9.1 </FONT><A id=x1-760001 name=x1-760001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Historical Notes</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Once researchers accepted the need to explicitly manage concurrency using threads, the challenge became how best to coordinate multi-threaded access to shared data. A large number of different abstractions were proposed, and it took some time to work out the different strengths and weaknesses of the various approaches. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Monitors &#8212; that is, managing shared data structures with locks and condition variables &#8212; were proposed in the early 1970&#8217;s in separate papers by Tony Hoare&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XHoare74monitors:an"}'><FONT style="BACKGROUND-COLOR: #7be1e1">83</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">] and Per Brinch Hansen&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XHansen:1970:NMS:362258.362278"}'><FONT style="BACKGROUND-COLOR: #7be1e1">75</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. One early advantage of monitors was the ability to formally prove properties about multi-threaded code; for example with Hoare-style semantics for condition variables, any statement which is true of the shared object immediately before a <TT>signal</TT>&nbsp;is also true of the object immediately after the return from <TT>wait</TT>. As we saw with the Too Much Milk example, without explicit synchronization, it can be quite difficult to reason about concurrent execution. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By the early 1980&#8217;s, Xerox PARC had built the first personal computer, the Alto, with all of its system software written using threads (called lightweight processes at the time) and monitors. The methodology we present in this chapter originated with that project&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XLampson:1980:EPM:358818.358824"}'><FONT style="BACKGROUND-COLOR: #7be1e1">98</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. It is hard to overstate how radical an approach this was; almost all widely used operating systems of the time, including UNIX, were built using semaphores. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An alternative line of work advocated completely prohibiting access by threads to shared data, as a way of eliminating race conditions. Instead of shared data, all data was private to a single thread; as a result, locks were never needed. An early example of this approach was Communicating Sequential Processes (CSP), also developed by Tony Hoare&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XHoare:1978:CSP:359576.359585"}'><FONT style="BACKGROUND-COLOR: #7be1e1">84</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. Google&#8217;s Go language for concurrent web programming is a modern language that supports both monitors and the CSP style of programming. With CSP and Go, a thread that needs to perform an operation on some other thread&#8217;s data sends it a message; the receiving thread can either reply with the result, or in data-flow style, forward the result onto some other thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While there was considerable and vigorous debate at the time as to whether message-passing or shared-memory were better models for programming concurrency, the debate was largely resolved by a simple observation made by Lauer and Needham&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "XLauer79onthe"}'><FONT style="BACKGROUND-COLOR: #7be1e1">101</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">]. Any program using monitors can be recast into CSP using a simple transformation, and vice versa. The execution of a procedure with a monitor lock is equivalent to processing a message in CSP; a monitor is, in effect, single-threaded while it is holding the lock. Thus, the choice of which style to use is largely a matter of taste and convention, and most programmers have chosen to use threads and monitors. </FONT><A id=Q1-1-126 name=Q1-1-126></A><A id=Q1-1-127 name=Q1-1-127></A></P><A id=x1-770001 name=x1-770001>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=problems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">True or False: If a multi-threaded program runs correctly in all cases on a single time-sliced processor, then it will run correctly if each thread is run on a separate processor of a shared-memory multiprocessor. Justify your answer. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Show that solution 3 to the Too Much Milk problem is safe &#8212; that it guarantees that at most one roommate buys milk. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Precisely describe the set of possible outputs that could occur when the program shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-530015"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> is run. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose that you mistakenly create an automatic (local) variable v in one thread t1 and pass a pointer to v to another thread t2. Is it possible that a write by t1 to some variable other than v will change the state of v as observed by t2? If so, explain how this can happen and give an example. If not, explain why not. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose that you mistakenly create an automatic (local) variable v in one thread t1 and pass a pointer to v to another thread t2. Is it possible that a write by t2 to v will cause t1 to execute the wrong code? If so, explain how. If not, explain why not. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming Mesa semantics for condition variables, our implementation of the blocking bounded queue in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-560018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> does not guarantee freedom from starvation: if a continuous stream of threads makes insert (or remove) calls, a waiting thread could wait forever. For example, a thread may call insert and wait in the while loop because the queue is full. Starvation would occur if every time another thread removes an item from the queue and signals the waiting thread, a <EM>different</EM> thread calls insert, sees that the queue is not full, and inserts an item before the waiting thread resumes. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Prove that under Hoare semantics and assuming that signal wakes the longest-waiting thread, our implementation of BBQ ensures freedom from starvation. More precisely, prove that if a thread waits in insert, then it is guaranteed to proceed after a bounded number of remove calls complete, and vice versa. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As noted in the previous problem, our implementation of the blocking bounded queue in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-560018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> does not guarantee freedom from starvation. Modify the code to ensure freedom from starvation so that if a thread waits in insert, it is guaranteed to proceed after a bounded number of remove() calls complete, and vice versa. <B>Note:</B> Your implementation must work under Mesa semantics for condition variables. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Wikipedia provides an implementation of Peterson&#8217;s algorithm to provide mutual exclusion using loads and stores at </FONT><A href="http://en.wikipedia.org/wiki/Peterson's_algorithm"><FONT style="BACKGROUND-COLOR: #7be1e1">http://en.wikipedia.org/wiki/Peterson&#8217;s_algorithm</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Unfortunately, this code is not guaranteed to work with modern compilers or hardware. Update the code to include memory barriers where necessary. (Of course, you could add a memory barrier before and after each instruction; your solution should instead add memory barriers only where necessary for correctness.) </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Linux provides a sys_futex() system call to assist in implementing hybrid user-level/kernel-level locks and condition variables. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A call to long sys_futex(void *addr1, FUTEX_WAIT, int val1, NULL, NULL, 0) checks to see if the memory at address addr1 has the same value as val1. If so, the calling thread is suspended. If not, the calling thread returns immediately with the error return value EWOULDBLOCK. In addition, the system call returns with the value EINTR if the thread receives a signal. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A call to long sys_futex(void *addr1, FUTEX_WAKE, 1, NULL, NULL, 0) causes one thread waiting on addr1 to return. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the following simple implementation of a hybrid user-level/kernel-level lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;class&nbsp;TooSimpleFutexLock&nbsp;{
   &nbsp;&nbsp;&nbsp;private:
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;val;
   &nbsp;
   &nbsp;&nbsp;&nbsp;public:
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TooSimpleMutex()&nbsp;:&nbsp;val&nbsp;(0)&nbsp;{&nbsp;}&nbsp;&nbsp;//&nbsp;Constructor
   &nbsp;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;acquire&nbsp;()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;c;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;atomic_inc&nbsp;returns&nbsp;*old*&nbsp;value
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;((c&nbsp;=&nbsp;atomic_inc&nbsp;(val))&nbsp;!=&nbsp;0)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;futex_wait&nbsp;(&amp;val,&nbsp;c&nbsp;+&nbsp;1);
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;release&nbsp;()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val&nbsp;=&nbsp;0;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;futex_wake&nbsp;(&amp;val,&nbsp;1);
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;};</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are three problems with this code. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance.</B> The goal of this code is to avoid making expensive system calls in the uncontested case of an <TT>acquire</TT>&nbsp;on a FREE&nbsp;lock or a <TT>release</TT>&nbsp;of a lock with no other waiting threads. This code fails to meet this goal. Why? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance.</B> A subtle corner case occurs when multiple threads try to acquire the lock at the same time. It can show up as occasional slowdowns and bursts of CPU usage. What is the problem? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Correctness</B>. A corner case can cause the mutual exclusion correctness condition to be violated, allowing two threads to both believe they hold the lock. What is the problem? </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">In the readers/writers lock example for the function RWLock::doneRead, why do we use writeGo.Signal rather than writeGo.Broadcast? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Show how to implement a semaphore by generalizing the multi-processor lock implementation shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-7000117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">In Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-430003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.1.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, we presented a solution to the Too Much Milk problem. To make the problem more interesting, we will also allow roommates to drink milk. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Implement in C++ or Java a Kitchen class with a drinkMilkAndBuyIfNeeded(). This method should randomly (with a 20% probability) change the value of milk from 1 to 0. Then, if the value just became 0, it should buy milk (incrementing milk back to 1). The method should return 1 if the roommate bought milk and 0 otherwise. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Your solution should use locks for synchronization and work for any number of roommates. Test your implementation by writing a program that repeatedly creates a Kitchen object and varying numbers of roommate threads; each roommate thread should call drinkMilkAndBuyIfNeeded() multiple times in a loop. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Hint:</B> You will probably write a main() thread that creates a Kitchen object, creates multiple roommate threads, and then waits for all of the roommates to finish their loops. If you are writing in C++ with the POSIX threads library, you can use pthread_join() to have one thread wait for another thread to finish. If you are writing in Java with the java.lang.Thread class, you can use the join() method. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For the solution to Too Much Milk suggested in the previous problem, each call to drinkMilkAndBuyIfNeeded() is atomic and holds the lock from the start to the end even if one roommate goes to the store. This solution is analogous to the roommate padlocking the kitchen while going to the store, which seems a bit unrealistic. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Implement a better solution to drinkMilkAndBuyIfNeeded() using both locks and condition variables. Since a roommate now needs to release the lock to the kitchen while going to the store, you will no longer acquire the lock at the start of this function and release it at the end. Instead, this function will call two helper-functions, each of which acquires/releases the lock. For example: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;int&nbsp;Kitchen::drinkMilkAndBuyIfNeeded()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;iShouldBuy&nbsp;=&nbsp;waitThenDrink();
   &nbsp;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(iShoudBuy)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buyMilk();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this function, waitThenDrink() should wait if there is no milk (using a condition variable) until there is milk, drink the milk, and if the milk is now gone, return a nonzero value to flag that the caller should buy milk. BuyMilk() should buy milk and then broadcast to let the waiting threads know that they can proceed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Again, test your code with varying numbers of threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Before entering a <EM>priority critical section</EM>, a thread calls PriorityLock::enter(priority). When the thread exits the critical section, it calls PriorityLock::exit(). If several threads are waiting to enter a priority critical section, the one with the numerically highest priority should be the next one allowed in. Implement PriorityLock using monitors (locks and condition variables) and following the programming standards defined in this chapter. </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Define the state and synchronization variables and describe the purpose of each. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Implement PriorityLock::enter(int priority). </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Implement PriorityLock::exit(). </FONT>
<P></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Implement a <EM>priority condition variable.</EM> A priority condition variable (PCV) has three public methods: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;PCV::wait(Lock&nbsp;*enclosingLock,&nbsp;int&nbsp;priority);
   &nbsp;
   &nbsp;void&nbsp;PCV::signal(Lock&nbsp;*enclosingLock);
   &nbsp;
   &nbsp;void&nbsp;PCV::broadcast(Lock&nbsp;*enclosingLock,&nbsp;int&nbsp;priority);</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These methods are similar to those of a standard condition variable. The one difference is that a PCV enforces both <EM>priority</EM> and <EM>ordering</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In particular, signal(Lock *lock) causes the currently waiting thread with the highest priority to return from wait; if multiple threads with the same priority are waiting, then the one that is waiting the longest should return before any that have been waiting a shorter amount of time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Similarly, broadcast(Lock *lock, int priority) causes all currently waiting threads whose priority equals or exceeds priority to return from wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For full credit, you must follow the <EM>thread coding standards</EM> described in this chapter. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">A synchronous buffer is one where the thread placing an item into the buffer waits until the thread retrieving the item has gotten it, and only then returns. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Implement a synchronous buffer using Mesa-style locks and condition variables, with the following routines: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Put&nbsp;item&nbsp;into&nbsp;the&nbsp;buffer&nbsp;and&nbsp;return&nbsp;only&nbsp;once&nbsp;the&nbsp;item
   &nbsp;//&nbsp;has&nbsp;been&nbsp;retrieved&nbsp;by&nbsp;some&nbsp;thread.
   &nbsp;SyncBuf::put(item);
   &nbsp;
   &nbsp;//&nbsp;Wait&nbsp;until&nbsp;there&nbsp;is&nbsp;an&nbsp;item&nbsp;in&nbsp;the&nbsp;buffer,&nbsp;and&nbsp;then&nbsp;return&nbsp;it.
   &nbsp;SyncBuf::get();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Any number of threads can concurrently call SyncBuf::get and SyncBuf::put; the module pairs off puts and gets. Each item should be returned exactly once, and there should be no unnecessary waiting. Once the item is retrieved, the thread that called put with the item should return. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">You have been hired by a company to do climate modelling of oceans. The inner loop of the program matches atoms of different types as they form molecules. In an excessive reliance on threads, each atom is represented by a thread. </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Your task is to write code to form water out of two hydrogen threads and one oxygen thread (H2O). You are to write the two procedures: HArrives() and OArrives(). A water molecule forms when two H threads are present and one O thread; otherwise, the atoms must wait. Once all three are present, one of the threads calls MakeWater(), and only then, all three depart. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">The company wants to extend its work to handle cloud modelling. Your task is to write code to form ozone out of three oxygen threads. Each of the threads calls OArrives(), and when three are present, one calls MakeOzone(), and only then, all three depart. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Extending the product line into beer production, your task is to write code to form alcohol (C2H6O) out of two carbon atoms, six hydrogens, and one oxygen. </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You must use locks and Mesa-style condition variables to implement your solutions, using the best practices as defined in this chapter. Obviously, an atom that arrives <EM>after</EM> the molecule is made must wait for a different group of atoms to be present. There should be no busy-waiting and you should correctly handle spurious wakeups. There must also be no useless waiting: atoms should not wait if there is a sufficient number of each type. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-780006 name=x1-780006>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">6. Multi-Object Synchronization</FONT></I></H2></A>
<DIV class=chapterQuote>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When two trains approach each other at a crossing, both shall come to a full stop and neither shall start up again until the other has gone. &#8212;<I>Kansas state law, early 1900s</I> </FONT></P>
<DL>
<DT><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DD><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DD></DL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
<BR></FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the previous chapter, we described a key building block for writing concurrent programs: how to design an object that can be shared between multiple threads. In this chapter, we need to go one step further: what happens as programs become more complex, with multiple shared objects and multiple locks? To answer this, we need to reason about the interactions between shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several considerations arise in this context: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiprocessor performance.</B> Modern computers have increasing numbers of processors because of the difficulty of improving single CPU performance. The design of shared objects can have a large impact on multiprocessor performance. For example, a lock protecting a frequently accessed shared object can become a bottleneck, since only one thread can hold the lock at a time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Correctness.</B> Performance considerations often cause designers to re-engineer their data structures for increased concurrency. Splitting a single shared object into a set of related objects each with their own lock can improve performance. However, it also raises issues of correctness. For programs with multiple shared objects, we face a problem similar to the one faced when reasoning about atomic loads and stores: even if each individual operation on a shared object is atomic, we must reason about interactions of sequences of operations across objects. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Deadlock.</B> One way to help reason about the behavior of operations across multiple objects is to hold multiple locks. This approach raises the possibility of deadlock, where threads are permanently stuck waiting for each other in a cycle. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">No cookbook recipe always works for addressing these challenges. In particular, current techniques have two basic limitations. First, they pose engineering trade-offs. Some solutions are general but complex or expensive; others are simple but slow; still others are simple and cheap but not general. Second, many solutions are inherently <EM>non-modular</EM>: they require reasoning about the global structure of the system and internal implementation details of modules to understand or restrict how different modules can interact. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Chapter roadmap:</B> </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiprocessor Lock Performance.</B> Can we predict when a lock will become a bottleneck on a multiprocessor? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-790001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Lock Design Patterns.</B> If a lock is a bottleneck, can we restructure the program to reduce the problem? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-800002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Lock Contention.</B> If a lock is still a bottleneck after re-structuring, what then? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-850003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multi-Object Atomicity.</B> How can we make a sequence of operations across multiple objects appear atomic to other threads? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-920004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Deadlock.</B> What causes deadlock in multi-threaded programs, and what solutions exist to prevent or break deadlocks? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-960005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Non-Blocking Synchronization.</B> Are there ways to eliminate locks in complex multi-object programs? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1040006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">)</FONT></P></LI></UL><A id=x1-78001r124 name=x1-78001r124></A><A id=x1-790001 name=x1-790001>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.1 Multiprocessor Lock Performance</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Client-server applications often have ample parallelism for modern multicore architectures with dozens of processors. Each separate client request can be handled by a different thread running on a different processor; this is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:request parallelism"}'>request parallelism</A></EM>. Likewise, server operating systems often have ample parallelism &#8211; applications with large numbers of threads can make a large number of concurrent system calls into the kernel. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even with ample request parallelism, however, performance can often be disappointing. Once locks and condition variables are added to a server application to allow it to process requests concurrently, throughput may be only slightly faster on a fifty-way multiprocessor than on a uniprocessor. Most often, this can be due to three causes: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-79002x1 name=x1-79002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Locking.</B> A lock implies mutual exclusion &#8212; only one thread at a time can hold the lock. As a result, access to a shared object can limit parallelism. </FONT></P>
<LI class=enumerate><A id=x1-79004x2 name=x1-79004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Communication of shared data.</B> The performance of a modern processor can vary by a factor of ten (or more) depending on whether the data needed by the processor is already in its cache or not. Modern processors are designed with large caches, so that almost all of the data needed by the processor will already be stored in the cache. On a uniprocessor, it is rare that the processor needs to wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, on a multiprocessor, the situation is different. Shared data protected by a lock will often need to be copied from one cache to another. Shared data is often in the cache of the processor that last held the lock, and it is needed in the cache of the processor that is next to acquire the lock. Moving data can slow critical section performance significantly compared to a uniprocessor. </FONT></P>
<LI class=enumerate><A id=x1-79006x3 name=x1-79006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>False sharing.</B> A further complication is that the hardware keeps track of shared data at a fixed granularity, often in units of a cache entry of 32 or 64 bytes. This reduces hardware management overhead, but it can cause performance problems if multiple data structures with different sharing behavior fit in the same cache entry. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:false sharing"}'>false sharing</A></EM>.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, these effects can be reduced through careful design of shared objects. We caution, however, that you should keep your shared object design simple until you have proven, through detailed measurement, that a more complex design is necessary to achieve your performance target. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>The evolution of Linux kernel locking</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The first versions of Linux ran only on uniprocessor machines. To allow Linux to run on multiprocessors, version 2.0 introduced the Big Kernel Lock (BKL) &#8212; a single lock that protected all of the kernel&#8217;s shared data structures. The BKL allowed the kernel to function on multiprocessor machines, but scalability and performance were limited. So, over time, different subsystems and different data structures got their own locks, allowing them to be accessed without holding the BKL. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By version 2.6, Linux has been highly optimized to run well on multiprocessor machines. It now has thousands of different locks, and researchers have demonstrated scalability for a range of benchmarks on a 48 processor machine. Still, the BKL remains in use in a few &#8212; mostly less performance-critical &#8212; parts of the Linux kernel, like the reboot system call, some older file systems, and some device drivers. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To illustrate these concepts, consider a web server with an in-memory cache of recently fetched pages. It is often faster to simply return a page from memory rather than regenerating it from scratch. For example, Google might receive a large number of searches for election results on election night, and there is little reason to do all of the work of a general search in that case. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To implement caching of web pages, the server might have a shared data structure, such as a hash table on the search terms, to point to the cached page if it exists. The hash table is shared among the threads handling client requests, and therefore needs a lock. The hash table is updated whenever a thread generates a new page that is not in the cache. The code might also mark pages that have been recently fetched, to keep them in memory in preference to other requests that do not occur as frequently. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An important question in this design is whether the single lock on the hash table will significantly limit server parallelism. How can we tell if the lock on a shared object is going to be a problem? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A convenient approach is to derive a bound on the performance of a parallel program by assuming that the rest of the program is perfectly parallelizable &#8212; in other words, that the only limiting factor is that only one thread at a time can hold the shared lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose that, on average, a web server spends 5% of each request accessing and manipulating its hash table of recently used web pages. If the hash table is protected by a single lock, what is the maximum possible throughput gain? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The time spent in the critical section is inherently sequential. If we assume all other parts of the server are perfectly parallelizable, then the maximum speedup is a <B>factor of 20</B> regardless of how many processors are used. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As we mentioned earlier, a further complication is that it can take much longer to fetch shared data on a multiprocessor because the data is unlikely to be in the processor cache. If the portion of the program holding the lock is slower on a multiprocessor than on a uniprocessor, the potential gain in throughput can be severely limited. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>In the example above, what is the maximum throughput improvement if the hash table code runs four times slower on a multiprocessor due to the need to move shared data between processor caches? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The potential throughput improvement would be small even if a large number of processors are used. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Throughput gain </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#8804; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1&#8725;4 &#215; 0.05 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>5</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can study the effect of cache behavior on multiprocessor performance with a simple experiment. The experiment is a intended only as an illustration; it is not meant a reflection of normal program behavior, but rather as a way of isolating the effect of hardware on the performance of code using shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we set up an array of a thousand shared objects, where each object is a simple integer counter protected by a spinlock. (We use a spinlock rather than a lock to avoid measuring context switch time.) The program iterates through the array. For each item, it acquires the lock, increments the counter, and releases the lock. We repeat the loop a thousand times to improve measurement precision. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the following scenarios: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>One thread, one array.</B> When one thread iterates through the array, incrementing each counter in turn, the test gives the time it takes to acquire and release an array of uncontended locks. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Two threads, two arrays.</B> When two threads iterate through disjoint arrays, this gives the slowdown when doing work in parallel. On most architectures, there is little to no slowdown to parallel execution. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Two threads, one array.</B> When two threads iterate through the <EM>same</EM> array, each lock is acquired by a thread running on one processor, and then, shortly afterwards, acquired by a different thread running on a different processor. Thus, the performance illustrates the added cost of moving the shared object data from one processor to another. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Two threads, alternate elements of one array.</B> To measure the impact of false sharing, one thread can iterate through the array acquiring the odd entries, and the other thread can iterate through the array acquiring the even entries. If there was no effect to false sharing, this would be identical to the two array case &#8212; the threads never use the same data.</FONT></P></LI></UL><A id=x1-790071 name=x1-790071></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">One thread, one array </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">51.2 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Two threads, two arrays </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">52.5 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Two threads, one array </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">197.4 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Two threads, alternating </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">127.3 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.1: </B>Number of CPU cycles to execute a simple critical section to increment a counter. Measurements taken on a 64-core AMD Opteron 6262, with threads assigned to processor cores that do not share a cache. The performance difference between these cases largely disappears when threads are assigned to cores that share an L2 cache.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Table&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-790071"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows example results for a single multiprocessor, a 64-core AMD Opteron; the performance on different machines will vary. The threads were assigned to cores that do not share a cache. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On this machine, there is very little slowdown in critical section performance when threads access disjoint locks. However, critical section execution time slows down by a factor of four when multiple processors access the same data. The slowdown is also significant when false sharing occurs. </FONT><A id=x1-79008r130 name=x1-79008r130></A></P><A id=x1-800002 name=x1-800002>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2 Lock Design Patterns</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">We next discuss a set of approaches that can reduce the impact of locking on multiprocessor performance. Often, the best practice is to start simple, with a single lock per shared object. If an object&#8217;s interface is well designed, then refactoring its implementation to increase concurrency and performance can be done once the system is built and performance measurements can identify any bottlenecks. An adage to follow is: &#8220;It is easier to go from a working system to a working, fast system than to go from a fast system to a fast, working system.&#8221; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We discuss four design patterns to increase concurrency when it is necessary: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Fine-Grained Locking.</B> Partition an object&#8217;s state into different subsets each protected by a different lock. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Per-Processor Data Structures.</B> Partition an object&#8217;s state so that all or most accesses are performed by threads running on the same processor. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Ownership Design Pattern.</B> Remove a shared object from a shared container so that only a single thread can read or modify the object. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Staged Architecture.</B> Divide system into multiple stages, so that only those threads within each stage can access that stage&#8217;s shared data.</FONT></P></LI></UL><A id=x1-80001r125 name=x1-80001r125></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.1 </FONT><A id=x1-810001 name=x1-810001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Fine-Grained Locking</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A simple and widely used approach to decrease contention for a shared lock is to partition the shared object&#8217;s state into different subsets, each protected by its own lock. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:fine-grained locking"}'>fine-grained locking</A></EM>. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The web server cache discussed above provides an example. The cache can use a shared hash table to store and locate recently used web pages; because the hash table is shared, it needs a lock to provide mutual exclusion. The lock is acquired and released at the start and end of each of the hash table methods: put(key, value), value = get(key), and value = remove(key). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the single lock limits performance, an alternative is to have one lock per hash bucket. The methods acquire the lock for bucket b before accessing any record that hashes to that bucket. Provided that the number of buckets is large enough, and no single bucket receives a large fraction of requests, then different threads can use and update the hash table in parallel. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, there is no free lunch. Dividing an object&#8217;s state into different pieces protected by different locks can significantly increase the object&#8217;s complexity. Suppose we want to implement a hash table whose number of hash buckets grows as the number of objects it stores increases. If we have a single lock, this is easy to do. But, what if we use fine-grained locking? Then, the design becomes more complex because we have some methods, like put and get, that operate on one bucket and other methods, like resize, that operate across multiple buckets. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several solutions are possible: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-81002x1 name=x1-81002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Introduce a readers/writers lock.</B> Suppose we have a readers/writers lock on the overall structure of the hash table (e.g., the number of buckets and the array of buckets) and a mutual exclusion lock on each bucket. Methods that work on a single bucket at a time, such as put and get, acquire the table&#8217;s readers/writers lock in read mode and also acquire the relevant bucket&#8217;s mutual exclusion lock. Methods that change the table&#8217;s structure, such as resize, must acquire the readers/writers lock in write mode; the readers/writers lock prevents any other threads from using the hash table while it is being resized. </FONT></P>
<LI class=enumerate><A id=x1-81004x2 name=x1-81004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Acquire every lock.</B> Methods that change the structure of the hash table, such as resize, must first iterate through every bucket, acquiring its lock, before proceeding. Once resize has a lock on every bucket, it is guaranteed that no other thread is concurrently accessing or modifying the hash table. </FONT></P>
<LI class=enumerate><A id=x1-81006x3 name=x1-81006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Divide the hash key space.</B> Another solution is to divide the hash key space into r regions, to have a mutual exclusion lock for each region, and to allow each region to be resized independently when it becomes heavily loaded. Then, get, put, and resizeRegion each acquire the relevant region&#8217;s mutual exclusion lock.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Which solution is best? It is not obvious. The first solution is simple and appears to allow high concurrency, but acquiring the readers/writers lock even in read mode may have high overhead. For example, we gave an implementation of a readers/writers lock in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> where acquiring a read-only lock involves acquiring a mutual exclusion lock on both entrance and exit. Access to the underlying mutual exclusion lock may become a bottleneck. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The second solution makes resize expensive, but if resize is a rare operation, that may be acceptable. The third solution balances concurrency for get/put against the cost of resize, but it is more complex and may require tuning the number of groups to get good performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Further, these trade-offs may change as the implementation becomes more complex. For example, to trigger resize at appropriate times, we probably need to maintain an additional nObjects count of the number of objects currently stored in the hash table, so whatever locking approach we use would need to be extended to cover this information. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>How might you use fine-grained locking to reduce contention for the lock protecting the shared memory heap in malloc/free or new/delete? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>One approach would be to partition the heap into separate memory regions, each with its own lock.</B> For example, a fast implementation of a heap on a uniprocessor uses n buckets, where the ith bucket contains blocks of size 2<SUP>i</SUP>, and serves requests of size 2<SUP>i-1</SUP> + 1 to 2<SUP>i</SUP>. If there are no free blocks in the ith bucket, an item from the next larger bucket i + 1 is split in two. Using fine-grained locking, each bucket can be given its own lock. &#9633; </FONT><A id=x1-81007r133 name=x1-81007r133></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.2 </FONT><A id=x1-820002 name=x1-820002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Per-Processor Data Structures</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A related technique to fine-grained locking is to partition the shared data structure based on the number of processors on the machine. For example, instead of one shared hash table of cached pages, an alternative design would have N separate hash tables, where N is the number of processors. Each thread uses the hash table based on the processor where it is currently running. Each hash table still needs its own lock in case a thread is context switched in the middle of an operation, but in the common case, only threads running on the same processor contend for the same lock. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Often, this is combined with a per-processor ready list, ensuring that each thread preferentially runs on the same processor each time it is context switched, further improving execution speed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An advantage of this approach is better hardware cache behavior; as we saw in the previous section, shared data that must be communicated between processors can slow down the execution of critical sections. Of course, the disadvantage is that the hash tables are now partitioned, so that a web page may be cached in one processor&#8217;s hash table, and needed in another. Whether this is a performance benefit depends on the relative impact of reducing communication of shared data versus the decreased effectiveness of the cache. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>How might you use per-processor data structures to reduce contention for the memory heap? Under what conditions would this work well? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>The heap can be partitioned into N separate memory regions, one for each processor.</B> Calls to malloc/new would use the local heap; free/delete would return the data to the heap where it was allocated. <B>This would perform well provided that (i) rebalancing the heaps was rare and (ii) most allocated data is freed by the thread that acquires it.</B> &#9633; </FONT><A id=x1-82001r134 name=x1-82001r134></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.3 </FONT><A id=x1-830003 name=x1-830003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Ownership Design Pattern</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A common synchronization technique in large, multi-threaded programs is an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:ownership design pattern"}'>ownership design pattern</A></EM>. In this pattern, a thread removes an object from a container and can then access the object without holding a lock: the program structure guarantees that at most one thread owns an object at a time. </FONT><A id=x1-830012 name=x1-830012></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00403.gif" data-calibre-src="OEBPS/Images/image00403.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.2: </B>A multi-stage server based on the ownership pattern. In the first stage, one thread exclusively owns each network connection. In later stages, one thread parses and renders a given object at a time.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As an example, a single web page can contain multiple objects, including HTML frames, style sheets, and images. Consider a multi-threaded web browser whose processing is divided into three stages: receiving an object via the network, parsing the object, and rendering the object (see Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-830012"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">). The first stage has one thread per network connection; the other stages have several worker threads, each of which processes one object at a time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The work queues between stages coordinate object ownership. Objects in the queues are not being accessed by any thread. When a worker thread in the <EM>parse</EM> stage removes an object from the stage&#8217;s work queue, it owns the object and has exclusive access to it. When the thread is done parsing the object, it puts it into the second queue and stops accessing it. A worker thread from the <EM>render</EM> stage then removes it from the second queue, gaining exclusive access to it to render it to the screen. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>How might you use the ownership design pattern to reduce contention for the memory heap? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Ownership can be seen as an extension of per-processor data structures; instead of one heap per processor, <B>we can have one heap per thread.</B> Provided that the same thread that allocates memory also frees it, the thread can safely use its own heap without a lock and only return to the global heap when the local heap is out of space. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Commutative interface design</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Class and interface design can often constrain implementations in ways that require locking. An example is the UNIX API. Like most operating systems, the UNIX open system call returns a file handle that is used for further operations on the file; the same system call is also used to initialize a network socket. The open call gives the operating system the ability to allocate internal data structures to track the current state of the file or network socket, and more broadly, which files and sockets are in use. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">UNIX also specifies that each successive call to open returns the next integer file handle; as we saw in Chapter&nbsp;3, the UNIX shell uses this feature when redirecting stdin and stdout to a file or pipe. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A consequence of the design of the UNIX API is that the implementation of open requires a lock. For early UNIX systems, this was not an issue, but modern multi-threaded web servers open extremely large numbers of network sockets and files. Because of the semantics of the API, the implementation of open cannot use fine-grained locking or a per-processor data structure. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A better choice, where possible, is to design the API to be <EM>commutative</EM>: the result of two calls is the same regardless of which call was made first. For example, if the implementation can return any unique integer as a file handle, rather than the next successive one, then the implementation could allocate out of a per-processor bucket of open file handles. The implementation would then need a lock only for the special case of allocating specific handles such as stdin and stdout. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-83002r135 name=x1-83002r135></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.4 </FONT><A id=x1-840004 name=x1-840004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Staged Architecture</FONT></H4><A id=x1-840013 name=x1-840013></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00404.gif" data-calibre-src="OEBPS/Images/image00404.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.3: </B>A staged architecture for a simple web server.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
The <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:staged architecture"}'>staged architecture</A></EM> pattern, illustrated in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-840013"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, divides a system into multiple subsystems, called stages. Each stage includes state private to the stage and a set of one or more worker threads that operate on that state. Different stages communicate by sending messages to each other via shared producer-consumer queues. Each worker thread repeatedly pulls the next message from a stage&#8217;s incoming queue and then processes it, possibly producing one or more messages for other stages&#8217; queues. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-840013"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows a staged architecture for a simple web server that has a first <EM>connect</EM> stage that uses one thread to set up network connections and that passes each connection to a second <EM>read and parse</EM> stage. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The <EM>read and parse</EM> stage has several threads, each of which repeatedly gets a connection from the incoming queue, reads a request from the connection, parses the request to determine what web page is being requested, and checks to see if the page is already cached. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming the page is not already cached, if the request is for a static web page (e.g., an HTML file), the <EM>read and parse</EM> stage passes the request and connection to the <EM>read static page</EM> stage, where one of the stage&#8217;s threads reads the specified page from disk. Otherwise, the <EM>read and parse</EM> stage passes the request and connection to the <EM>generate dynamic page</EM> stage, where one of the stage&#8217;s threads runs a program that dynamically generates a page in response to the request. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Once the page has been fetched or generated, the page and connection are passed to the <EM>send page</EM> stage, where one of the threads transmits the page over the connection. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The key property of a staged architecture is that the state of each stage is private to that stage. This improves modularity, making it easier to reason about each stage individually and about interactions across stages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As an example of the modularity benefits, consider a system where different stages are produced by different teams or even different companies. Each stage can be designed and tested almost independently, and the system is likely to work as expected when the stages are brought together. For example, it is common practice for a web site to use a web server from one company and a database from another company and for the two to communicate via messages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another benefit is improved cache locality. A thread operating on a subset of the system&#8217;s state may have better cache behavior than a thread that accesses state from all stages. On the other hand, for some workloads, passing a request from stage to stage could hurt cache behavior compared to doing all of the processing for a request on one processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Also note that for good performance, the processing in each stage must be large enough to amortize the cost of sending and receiving messages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The special case of exactly one thread per stage is <EM>event-driven programming</EM>, described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. With event-driven programming, there is no concurrency within a stage, so no locking is required. Each message is processed atomically with respect to that stage&#8217;s state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One challenge with staged architectures is dealing with overload. System throughput is limited by the throughput of the slowest stage. If the system is overloaded, the slowest stage will fall behind, and its work queue will grow. Depending on the system&#8217;s implementation, two bad things could happen. First, the queue could grow indefinitely, consuming more and more memory until the system memory heap is exhausted. Second, if the queue is limited to a finite size, once that size is reached, earlier stages must either discard work for the overloaded stage or block until the queue has room. Notice that if they block, then the backpressure will limit the throughput of earlier stages to that of the bottleneck stage, and their queues in turn may begin to grow. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One solution is to dynamically vary the number of threads per stage. If a stage&#8217;s incoming queue is growing, the program can shift processing resources to it by reducing the number of threads for a lightly-loaded stage in favor of more threads for the stage that is falling behind. </FONT><A id=x1-84002r132 name=x1-84002r132></A></P><A id=x1-850003 name=x1-850003>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.3 Lock Contention</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Sometimes, even after applying the techniques described in the previous section, locking may remain a bottleneck to good performance on a multiprocessor. For example, with fine-grained locking of a hash table, if a bucket contains a particularly popular item, say the cached page for Justin Bieber, then the lock on that bucket can be a source of contention. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this section, we discuss two alternate implementations of the lock abstraction that work better for locks that are bottlenecks: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MCS Locks.</B> MCS is an implementation of a spinlock optimized for the case when there are a significant number of waiting threads. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RCU Locks.</B> RCU is an implementation of a reader/writer lock, optimized for the case when there are many more readers than writers. RCU reduces the overhead for readers at a cost of increased overhead for writers. More importantly, RCU has somewhat different semantics than a normal reader/writer lock, placing a burden on the user of the lock to understand its dangers.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although both approaches are used in modern operating system kernels, we caution that neither is a panacea. They should only be used once profiling has shown that the lock is a source of contention and no other options are available. </FONT><A id=x1-85001r137 name=x1-85001r137></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.3.1 </FONT><A id=x1-860001 name=x1-860001></A><FONT style="BACKGROUND-COLOR: #7be1e1">MCS Locks</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Recall that the lock implementation described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> was tuned for the common case where the lock was usually FREE. Is there an efficient implementation of locks when the lock is usually BUSY? </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, the overhead of acquiring and releasing a lock can <EM>increase</EM> dramatically with the number of threads contending for the lock. For a contended lock, this can further increase the number of threads waiting for the lock. Consider again the example we used earlier, of a spinlock protecting a shared counter: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;Counter::Increment()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(test_and_set(&amp;lock))&nbsp;//&nbsp;while&nbsp;BUSY
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//&nbsp;spin
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value++;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock&nbsp;=&nbsp;FREE;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even if many threads try to increment the same counter, only one thread at a time can execute the critical section; the other threads must wait their turn. As we observed earlier, because the counter value must be communicated from one lock holder to the next, the critical section will take significantly longer on a multiprocessor than on a single processor. </FONT><A id=x1-860014 name=x1-860014></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00405.gif" data-calibre-src="OEBPS/Images/image00405.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.4: </B>The overhead of three alternative lock implementations as a function of the number of processors contending for the lock: (a) test-and-set, (b) test and test-and-set, and (c) MCS. Measurements taken on a 64-core AMD Opteron 6262. The non-smooth curves are typical of measurements of real systems.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, the situation with multiple waiting threads is even worse. The time to execute a critical section protected by a spinlock increases linearly with the number of spinning processors. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-860014"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this effect. The problem is that before a processor can execute an atomic read-modify-write instruction, the hardware must obtain exclusive access to that memory location. Any other read-modify-write instruction must occur either before or afterwards. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Thus, if a number of processors are executing a spin loop, they will all be trying to gain exclusive access to the memory location of the lock. The store instruction to clear the lock also needs exclusive access, and the hardware has no way to know that it should prioritize the lock release ahead of the competing requests to see if the lock is free. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might think that it would help to check that the lock is free before trying to acquire it with a test-and-set; this is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:test and test-and-set"}'>test and test-and-set</A></EM>: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;void&nbsp;Counter::Increment()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(lock&nbsp;==&nbsp;BUSY&nbsp;||&nbsp;test_and_set(&amp;lock))&nbsp;//&nbsp;while&nbsp;BUSY
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//&nbsp;spin
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock&nbsp;=&nbsp;FREE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, it turns out this does not help. When the lock is released, the new value of the lock, FREE, must be communicated to the other waiting processors. On modern systems, each processor separately fetches the data into its cache. Eventually one of them gets the new value and acquires the lock. If the critical section is not very long, the other processors will still be busy fetching the new value and trying to acquire the lock, preventing the lock release from completing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One approach is to adjust the frequency of polling to the length of time that the thread has been waiting. A more scalable solution is to assign each waiting thread a separate memory location where it can spin. To release a lock, the bit is set for <EM>one</EM> thread, telling it that it is the next to acquire the lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The most widely used implementation of this idea is known as the MCS lock, after the initials of its authors, Mellor-Crummey and Scott. The MCS lock takes advantage of an atomic read-modify-write instruction called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:compare-and-swap"}'>compare-and-swap</A></EM> that is supported on most modern multiprocessor architectures. Compare-and-swap tests the value of a memory location and swaps in a new value if the old value has not changed. </FONT><A id=x1-860025 name=x1-860025></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;MCSLock&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;*tail&nbsp;=&nbsp;NULL;
&nbsp;}
&nbsp;
&nbsp;MCSLock::release()&nbsp;{
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(compare_and_swap(&amp;tail,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB,&nbsp;NULL))&nbsp;{
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;If&nbsp;tail&nbsp;==&nbsp;myTCB,&nbsp;no&nbsp;one&nbsp;is
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;waiting.&nbsp;MCSLock&nbsp;is&nbsp;now&nbsp;free.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Someone&nbsp;is&nbsp;waiting.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(myTCB-&gt;next&nbsp;==&nbsp;NULL)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//&nbsp;spin&nbsp;until&nbsp;next&nbsp;is&nbsp;set
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Tell&nbsp;next&nbsp;thread&nbsp;to&nbsp;proceed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB-&gt;next-&gt;needToWait&nbsp;=&nbsp;FALSE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;
&nbsp;MCSLock::acquire()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;*oldTail&nbsp;=&nbsp;tail;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB-&gt;next&nbsp;=&nbsp;NULL;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(!compare_and_swap(&amp;tail,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oldTail,&nbsp;&amp;myTCB))&nbsp;{
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Try&nbsp;again&nbsp;if&nbsp;someone&nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;changed&nbsp;tail&nbsp;in&nbsp;the&nbsp;meantime.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oldTail&nbsp;=&nbsp;tail;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;If&nbsp;oldTail&nbsp;==&nbsp;NULL,&nbsp;lock&nbsp;acquired.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(oldTail&nbsp;!=&nbsp;NULL)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Need&nbsp;to&nbsp;wait.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB-&gt;needToWait&nbsp;=&nbsp;TRUE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oldTail-&gt;next&nbsp;=&nbsp;myTCB;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(myTCB-&gt;needToWait)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//spin
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.5: </B>Pseudo-code for an MCS queueing lock, where each waiting thread spins on a separate memory location in its thread control block (myTCB). The operation, compare-and-swap, atomically inserts the TCB at the tail of the queue.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-860036 name=x1-860036></A>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00406.gif" data-calibre-src="OEBPS/Images/image00406.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.6: </B>The behavior of the MCS queueing lock. Initially (a), tail is NULL indicating that the lock is FREE. To acquire the lock (b), thread A atomically sets tail to point to A&#8217;s TCB. Additional threads B and C queue by adding themselves (atomically) to the tail (c) and (d); they then spin on their respective TCB&#8217;s needToWait flag. Thread A hands the lock to B by clearing B&#8217;s needToWait flag (e); B hands the lock to C by clearing C&#8217;s needToWait fla (f). C releases the lock by setting tail back to NULL (a) iff no one else is waiting &#8212; that is, iff tail still points to C&#8217;s TCB.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Compare-and-swap can be used to build a queue of waiting threads, without a separate spinlock. A waiting thread atomically adds itself to the <EM>tail</EM> of the queue, and then spins on a flag in its queue entry. When a thread releases the lock, it sets the flag in the next queue entry, signaling to the thread that its turn is next. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-860025"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> provides an implementation, and Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-860036"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the algorithm in action. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because each thread in the queue spins on its own queue entry, the lock can be passed efficiently from one thread to another along the queue. Of course, the overhead of setting up the queue means that an MCS lock is less efficient than a normal spinlock unless there are a large number of waiting threads. </FONT><A id=x1-86004r140 name=x1-86004r140></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.3.2 </FONT><A id=x1-870002 name=x1-870002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Read-Copy-Update (RCU)</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:read-copy-update"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Read-copy-update</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> (RCU) provides high-performance synchronization for data structures that are frequently read and occasionally updated. In particular, RCU optimizes the read path to have extremely low synchronization costs even with a large number of concurrent readers. However, writes can be delayed for a long time &#8212; tens of milliseconds in some implementations. </FONT>
<H5 class=subsubsectionHead><A id=x1-880002 name=x1-880002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Why Not Use a Readers/Writers Lock?</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Standard readers/writers locks are a poor fit for certain types of read-dominated workloads. Recall that these locks allow an arbitrary number of concurrent active readers, but when there is an active writer, no other writer or reader can be active. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The problem occurs when there are many concurrent reads with short critical sections. Before reading, each reader must acquire a readers/writers lock in read mode and release it afterwards. On both entrance and exit, the reader must update some state in the readers/writers synchronization object. Even when there are only readers, the readers/writers synchronization object can become a bottleneck. This limits the rate at which readers can enter the critical section, because they can only acquire the lock one at a time. For critical sections of less than a few thousand cycles, and for programs with dozens of threads simultaneously reading a shared object, the standard readers/writers lock can limit performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While the readers/writers synchronization object could be implemented with an MCS lock and thereby reduce some of the effects of lock contention, it does not change the inherent serial access of the readers/writers control structure. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-890002 name=x1-890002></A><FONT style="BACKGROUND-COLOR: #7be1e1">The RCU Approach</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">How can concurrent reads access a data structure &#8212; one that can also be written &#8212; without having to update the state of a synchronization variable on each read? </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To meet this challenge, an RCU lock retains the basic structure of a reader/writers lock: readers (and writers) surround each critical section with calls to acquire and release the RCU lock in read-mode (or write-mode). An RCU lock makes three important changes to the standard interface: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-89002x1 name=x1-89002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Restricted update.</B> With RCU, the writer thread must <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:publish"}'>publish</A></EM> its changes to the shared data structure with a single, atomic memory write. Typically, this is done by updating a single pointer, as we illustrate below by using RCU to update a shared list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although restricted updates might seem to severely limit the types of data structure operations that are possible under RCU, this is not the case. A common pattern is for the writer thread to make a <EM>copy</EM> of a complex data structure (or a portion of it), update the copy, and then publish a pointer to the copy into a shared location where it can be accessed by new readers. </FONT></P>
<LI class=enumerate><A id=x1-89004x2 name=x1-89004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiple concurrent versions.</B> RCU allows any number of read-only critical sections to be in progress at the same time as the update. These read-only critical sections may see the old or new version of the data structure. </FONT></P>
<LI class=enumerate><A id=x1-89006x3 name=x1-89006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Integration with the thread scheduler.</B> Because there may be readers still in progress when an update is made, the shared object must maintain multiple versions of its state, to guarantee that an old version is not freed until all readers have finished accessing it. The time from when an update is published until the last reader is done with the previous version is called the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:grace period"}'>grace period</A></EM>. The RCU lock uses information provided by the thread scheduler to determine when a grace period ends. </FONT></P></LI></OL><A id=x1-890077 name=x1-890077></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00407.gif" data-calibre-src="OEBPS/Images/image00407.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.7: </B>Timeline for an update concurrent with several reads for a data structure accessed with read-copy-update (RCU) synchronization.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-890077"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the timeline for the critical sections of a writer and several reader threads under RCU. If a function that reads the data structure completes before a write is published, it sees the old version of the data structure; if a reader begins after a write is published, it sees the new version. But, if a reader begins <EM>before</EM> and ends <EM>after</EM> a write is published, it may see either the old version or the new one. If it reads the updated pointer more than once, it may see the old one and then the new one. Which version it sees depends on which version of the single, atomically-updated memory location it observes. However, the system guarantees that the old version is not deleted until the grace period expires. The deletion of the old version must be delayed until all reads that might observe the old version have completed. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-900002 name=x1-900002></A><FONT style="BACKGROUND-COLOR: #7be1e1">RCU API and Use</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">RCU is a synchronization abstraction that allows concurrent access to a data structure by multiple readers and a single writer at a time. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-900018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows a typical API. </FONT><A id=x1-900018 name=x1-900018></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td colSpan=2 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Reader API</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">readLock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Enter read-only critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">readUnlock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Exit read-only critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td colSpan=2 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Writer API</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">writeLock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Enter read-write critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">publish() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Atomically update shared data structure. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">writeUnlock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Exit read-write critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">synchronize() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Wait for all currently active readers to exit critical section, to allow for garbage collection of old versions of the object. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td colSpan=2 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduler API</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">quiescentState() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Of the read-only threads on this processor who were active during the most recent RCU::publish, all have exited the critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.8: </B>Sample programming interface for read-copy-update (RCU) synchronization. In Java&#8217;s implementation of RCU locks, synchronize and quiescentState are not needed because the language-level garbage collector automatically detects when old versions can no longer be accessed. In the implementation of RCU in the Linux kernel, synchronize is split into two calls: one to start the grace period, and one to wait until the grace period completes.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A reader calls RCU::readLock and RCU::readUnlock before and after accessing the shared data structure. A writer calls: RCU::writeLock to exclude other writers; RCU::publish to issue the write that atomically updates the data structure so that reads can see the updates; RCU::writeUnlock to let other writers proceed; and RCU::synchronize to wait for the grace period to expire so that the old version of the object can be freed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-900029"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates, writes are serialized &#8212; only one write can proceed at a time. However, a write can be concurrent with any number of reads. A write can also be concurrent with another write&#8217;s grace period: there may be any number of versions of the object until multiple overlapping grace periods expire. </FONT><A id=x1-900029 name=x1-900029></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00408.gif" data-calibre-src="OEBPS/Images/image00408.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.9: </B>RCU allows one write at a time, and it allows reads to overlap each other and writes. The initial version is v0, and overlapping writes update the version to v1, v2, and then v3.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>For each read in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-900029"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, which version(s) of the shared state can the read observe? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>If a read overlaps a publish, it can return the published value or the previous value: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Read</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Value Returned</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Reason</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>1</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v0 or v1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v1. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>2</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; After publish v2, before publish v3. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>3</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; After publish v3. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>4</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v0 or v1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v1. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>5</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v1 or v2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v2. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>6</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v0, v1, or v2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v1 and v2. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>7</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; After publish v2. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT><A id=x1-9000310 name=x1-9000310></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;typedef&nbsp;struct&nbsp;ElementS{
&nbsp;&nbsp;&nbsp;int&nbsp;key;
&nbsp;&nbsp;&nbsp;int&nbsp;value;
&nbsp;&nbsp;&nbsp;struct&nbsp;ElementS&nbsp;*next;
&nbsp;}&nbsp;Element;
&nbsp;
&nbsp;class&nbsp;RCUList&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RCULock&nbsp;rcuLock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*head;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;search(int&nbsp;key,&nbsp;int&nbsp;*value);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;insert(Element&nbsp;*item,&nbsp;value);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;remove(int&nbsp;key);
&nbsp;};</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;bool
&nbsp;RCUList::search(int&nbsp;key,&nbsp;int&nbsp;*valuep)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;result&nbsp;=&nbsp;FALSE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*current;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.readLock();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;=&nbsp;head;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(current&nbsp;=&nbsp;head;&nbsp;current&nbsp;!=&nbsp;NULL;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;=&nbsp;current-&gt;next)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(current-&gt;key&nbsp;==&nbsp;key)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*valuep&nbsp;=&nbsp;current-&gt;value;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;TRUE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.readUnlock();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;result;
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.10: </B>Declaration of data structures and API for a linked list that uses RCU for synchronization, and the implementation of a read-only method for searching the linked list using RCU.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: RCU linked list.</B> Figures&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9000310"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9000411"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.11</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> show how to use RCU locks to implement a linked list that can be accessed concurrently by many readers, while also being updated by one writer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The list data structure comprises an RCU lock and a pointer to the head of the list. Each entry in the list has two data fields &#8212; key and value &#8212; as well as a pointer to the next record on the list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The search method is read-only: after registering with readLock, it scans down the list until it finds an element with a matching key. If the element is found, the method uses the parameter to return the value field and then returns TRUE. Otherwise, the method returns FALSE&nbsp;to indicate that no matching record was found. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The methods to update the list are more subtle. Each of them is arranged so that a single pointer update is sufficient to publish the new version of the list to the readers. In particular, it is important that insert initialize the data structure <EM>before</EM> updating the head pointer to make the new element visible to readers. </FONT><A id=x1-9000411 name=x1-9000411></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;void
&nbsp;RCUList::insert(int&nbsp;key,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;value)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*item;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;One&nbsp;write&nbsp;at&nbsp;a&nbsp;time.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.writeLock();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Initialize&nbsp;item.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item&nbsp;=&nbsp;(Element*)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;malloc(sizeof(Element));
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item-&gt;key&nbsp;=&nbsp;key;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item-&gt;value&nbsp;=&nbsp;value;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item-&gt;next&nbsp;=&nbsp;head;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Atomically&nbsp;update&nbsp;list.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.publish(&amp;head,&nbsp;item);
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Allow&nbsp;other&nbsp;writes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;to&nbsp;proceed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.writeUnlock();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Wait&nbsp;until&nbsp;no&nbsp;reader
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;has&nbsp;old&nbsp;version.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.synchronize();
&nbsp;}</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;bool
&nbsp;RCUList::remove(int&nbsp;key)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;found&nbsp;=&nbsp;FALSE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*prev,&nbsp;*current;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;One&nbsp;write&nbsp;at&nbsp;a&nbsp;time.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.WriteLock();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(prev&nbsp;=&nbsp;NULL,&nbsp;current&nbsp;=&nbsp;head;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;!=&nbsp;NULL;&nbsp;prev&nbsp;=&nbsp;current,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;=&nbsp;current-&gt;next)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(current-&gt;key&nbsp;==&nbsp;key)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;found&nbsp;=&nbsp;TRUE;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Publish&nbsp;update&nbsp;to&nbsp;readers
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(prev&nbsp;==&nbsp;NULL)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.publish(&amp;head,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current-&gt;next);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.publish(&amp;(prev-&gt;next),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current-&gt;next);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Allow&nbsp;other&nbsp;writes&nbsp;to&nbsp;proceed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.writeUnlock();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Wait&nbsp;until&nbsp;no&nbsp;reader&nbsp;has&nbsp;old&nbsp;version.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(found)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.synchronize();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;free(current);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;found;
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.11: </B>Implementation of a linked list using RCU for synchronization.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<H5 class=subsubsectionHead><A id=x1-910002 name=x1-910002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing RCU</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">When implementing RCU, the central goal is to minimize the cost of read critical sections: the system must allow an arbitrary number of concurrent readers. Conversely, writes can have high <EM>latency</EM>. In particular, grace periods can be long, with tens of milliseconds from when an update is published until the system can guarantee that no readers are still using the old version. Even so, write <EM>overhead</EM> &#8212; the CPU time needed per write &#8212; should be modest. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A common technique for achieving these goals is to integrate the RCU implementation with that of the thread scheduler. This is in contrast with the readers/writers lock described in the previous chapter, which makes no assumptions about the thread scheduler, but which must track exactly how many readers are active at any given time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In particular, the implementation we present requires two things from the scheduler: (1) read-only critical sections complete without being interrupted and (2) whenever a thread on a processor is interrupted, the scheduler updates some per-processor RCU state. Then, once a write completes, RCULock::Synchronize simply waits for all processors to be interrupted at least once. At that point, the old version of the object is known to be <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:quiescent"}'>quiescent</A></EM> &#8212; no thread has access to the old version (other than the writer who changed it). </FONT><A id=x1-9100112 name=x1-9100112></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;RCULock{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;//&nbsp;Global&nbsp;state
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spinlock&nbsp;globalSpin;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;long&nbsp;globalCounter;
&nbsp;&nbsp;&nbsp;//&nbsp;One&nbsp;per&nbsp;processor
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DEFINE_PER_PROCESSOR(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;static&nbsp;long,&nbsp;quiescentCount);
&nbsp;
&nbsp;&nbsp;&nbsp;//&nbsp;Per-lock&nbsp;state
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spinlock&nbsp;writerSpin;
&nbsp;
&nbsp;&nbsp;&nbsp;//&nbsp;Public&nbsp;API&nbsp;omitted
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::ReadLock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disableInterrupts();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::ReadUnlock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enableInterrupts();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Called&nbsp;by&nbsp;scheduler
&nbsp;void&nbsp;RCULock::QuiescentState(){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PER_PROC_VAR(quiescentCount)&nbsp;=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globalCounter;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::writeLock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writerSpin.acquire();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::writeUnlock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writerSpin.release();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::publish&nbsp;(void&nbsp;**pp1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;*p2){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*pp1&nbsp;=&nbsp;p2;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;}
&nbsp;
&nbsp;void
&nbsp;RCULock::synchronize()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;p,&nbsp;c;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globalSpin.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c&nbsp;=&nbsp;++globalCounter;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globalSpin.release();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FOREACH_PROCESSOR(p)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while((PER_PROC_VAR(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;quiescentCount,&nbsp;p)&nbsp;-&nbsp;c)&nbsp;&lt;&nbsp;0)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;release&nbsp;CPU&nbsp;for&nbsp;10ms
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sleep(10);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.12: </B>A quiescence-based RCU implementation. The code assumes that spinlock acquire/release and interrupt enable/disable trigger a memory barrier. <EM>Credit:</EM> This pseudo-code is based on an implementation by Paul McKenney in &#8220;Is Parallel Programming Hard, And, If So, What Can be Done About It?&#8221;</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9100112"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows an implementation of RCU based on quiescent states. Notice first that readLock and readUnlock are inexpensive: they update no state and merely ensure that the read is not interrupted. RCU::writeLock and writeUnlock are also inexpensive. They acquire and release a spinlock to ensure that at most one write per RCULock can proceed at a time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">RCU::publish is also simple. It executes a memory barrier so that all modifications to the shared object are completed before the pointer is updated. It then updates the pointer, and then executes another memory barrier so that other processors observe the update. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">RCU::synchronize and quiescentState work together to ensure that when synchronize returns, all threads are guaranteed to be done with the old version of the object. RCU::synchronize increments a global counter and then waits until all processors&#8217; match the new value of that counter. RCU::quiescentState is called by the scheduler whenever that processor is interrupted. It updates that processor&#8217;s quiescentCount to match the current globalCounter. Thus, once quiescentCount is at least as large as c, on every processor, synchronize knows that no remaining readers can observe the old version. </FONT><A id=x1-91002r139 name=x1-91002r139></A></P><A id=x1-920004 name=x1-920004>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4 Multi-Object Atomicity</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Once a program has multiple shared objects, it becomes both necessary and challenging to reason about interactions across objects. For example, consider a system storing a bank&#8217;s accounts. A reasonable design choice might be for each customer&#8217;s account to be a shared object with a lock (either a </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-490003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">mutual exclusion lock</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> or a </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-630001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">readers/writers lock</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, as described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">). Consider, however, transferring $100 from account A to account B, as follows: </FONT>
<P><BR></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;A-&gt;subtract(100);
   &nbsp;B-&gt;add(100);</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although each individual action is atomic, the sequence of actions is not. As a result, there may be a time where, say, A tells B that the money has been sent, but the money is not yet in B&#8217;s account. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Similarly, consider a bank manager who wants to answer a question: &#8220;How much money does the bank have?&#8221; If the manager&#8217;s program simply reads from each account, the calculation may exclude or double-count money &#8220;in flight&#8221; between accounts, such as in the transfer from A to B. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These examples illustrate a general problem that arises whenever a program contains multiple shared objects. Even if the object guarantees that each method operates atomically, <EM>sequences</EM> of operations by different threads can be interleaved. The same issues of managing multiple locks also apply to fine-grained locking within an object. </FONT><A id=x1-92001r144 name=x1-92001r144></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4.1 </FONT><A id=x1-930001 name=x1-930001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Careful Class Design</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Sometimes it is possible to address this issue through careful class and interface design. This includes the design of individual objects (e.g., specifying clean interfaces that expose the right abstractions). It also includes the architecture of how those objects interact (e.g., structuring a system architecture in well-defined layers). </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, you would face the same issues if you tried to solve Too Much Milk problem with a Note object that has two methods, readNote and writeNote, and a Fridge object with two methods, checkForMilk and addMilk. Atomicity of these individual operations is not sufficient to provide the desired behavior without considerable programming effort. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On the other hand, if we refactor the objects so that we have: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Fridge::checkforMilkAndSetNoteIfNeeded();
   &nbsp;Fridge::addMilk();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Then, the problem becomes straightforward. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This advice may seem obvious: of course, you should strive for elegant designs for both single- and multi-threaded code. Nonetheless, we emphasize that the choices you make for your interfaces, abstractions, and software architecture can dramatically affect the complexity or feasibility of your designs. </FONT><A id=x1-93001r156 name=x1-93001r156></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4.2 </FONT><A id=x1-940002 name=x1-940002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Acquire-All/Release-All</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Better interface design has limits, however. Sometimes, multiple locks are needed for program structure or for greater concurrency. Is there a general technique to perform a set of operations that require multiple locks, so that the group of operations appears atomic? For clarity, we will refer to a group of operations as a <EM>request</EM>. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One approach, called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:acquire-all/release-all"}'>acquire-all/release-all</A></EM> is to acquire <EM>every</EM> lock that may be needed at any point while processing the entire set of operations in the request. Then, once the thread has all of the locks it might need, the thread can execute the request, and finally, release the locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Consider a hash table with one lock per hash bucket. To move an item from one bucket to another, the hash table supports a changeKey(item, k1, k2) operation. With acquire-all/release-all, this function could be implemented to first acquire both the locks for k1 and k2, then remove the item under k1 and insert it under k2, and finally release both locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Acquire-all/release-all allows significant concurrency. When individual requests touch non-overlapping subsets of state protected by different locks, they can proceed in parallel. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A key property of this approach is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:serializability"}'>serializability</A></EM> across requests: the result of any program execution is equivalent to an execution in which requests are processed one at a time in some sequential order. Serializability allows one to reason about multi-step tasks <EM>as if </EM>each task executed alone. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9400113"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates, requests that access non-overlapping data can proceed in parallel. The result is the same as if the system first executed one request and then the other (or equivalently, the reverse). On the other hand, if two requests touch the same data, then the fact that all locks are acquired at the beginning and released at the end implies that one request is completed before the other one begins. </FONT><A id=x1-9400113 name=x1-9400113></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00409.gif" data-calibre-src="OEBPS/Images/image00409.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.13: </B>Locking multiple objects using an acquire-all/release-all pattern results in a serializable execution that is equivalent to an execution where requests are executed sequentially in some order.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One challenge to using this approach is knowing exactly what locks will be needed by a request before beginning to process it. A potential solution is to conservatively acquire more locks than needed (e.g., acquire any locks that <EM>may</EM> be needed by a particular request), but this may be difficult to determine. Without first executing the request, how can we know which locks will be needed? </FONT><A id=x1-94002r157 name=x1-94002r157></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4.3 </FONT><A id=x1-950003 name=x1-950003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Two-Phase Locking</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:two-phase locking"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Two phase locking</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> refines the acquire-all/release-all pattern to address this concern. Instead of acquiring all locks before processing the request, locks can be acquired as needed for each operation. However, locks are not <EM>released</EM> until all locks needed by the request have been acquired. Most implementations simply release all locks at the end of the request. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Two-phase locking avoids needing to know what locks to grab <EM>a priori</EM>. Therefore, programs can avoid acquiring locks they do not need, and they may not need to hold locks as long. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>The changeKey(item, k1, k2) function for a hash table with per-bucket locks could be implemented to acquire k1&#8217;s lock, remove the item using key k1, acquire k2&#8217;s lock, insert the item using key k2, and release both locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Like acquire-all/release-all, two-phase locking is serializable. If two requests have non-overlapping data, they are commutative and therefore serializable. Otherwise, there is some overlapping data between the two requests, protected by one or more locks. Provided a request completes, it must have acquired all of those locks, and made its changes to the overlapping data, before releasing any of them. Thus, any overlapping request must have read or modified the data in the overlap either entirely before or after the other request. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unlike acquire-all/release-all, however, two-phase locking can in some cases lead to deadlock, the topic of the next section. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose one thread starts executing changeKey(item, k1, k2) and another thread simultaneously tries to move a different item in the other direction from k2 to k1. If the first thread acquires k1&#8217;s lock and the second thread acquires k2&#8217;s lock, neither will be able to make progress. </FONT><A id=x1-95001r155 name=x1-95001r155></A></P><A id=x1-960005 name=x1-960005>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5 Deadlock</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">A challenge to constructing complex multi-threaded programs is the possibility of deadlock. A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:deadlock"}'>deadlock</A></EM> is a cycle of waiting among a set of threads, where each thread waits for some other thread in the cycle to take some action. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock can occur in many different situations, but one of the simplest is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:mutually recursive locking"}'>mutually recursive locking</A></EM>, shown in the code fragment below: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;A
   &nbsp;
   &nbsp;lock1.acquire();
   &nbsp;lock2.acquire();
   &nbsp;lock2.release();
   &nbsp;lock1.release();</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;B
   &nbsp;
   &nbsp;lock2.acquire();
   &nbsp;lock1.acquire();
   &nbsp;lock1.release();
   &nbsp;lock2.release();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose two shared objects with mutual exclusion locks can call into each other while holding their locks. Deadlock can occur when one thread holds the lock on the first object, and another thread holds the lock on the second object. If the first thread calls into the second object while still holding onto its lock, it will need to wait for the second object&#8217;s lock. If the other thread does the same thing in reverse, neither will be able to make progress. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can also get into deadlock with two locks and a condition variable, shown below: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;A
   &nbsp;
   &nbsp;lock1.acquire();
   &nbsp;...
   &nbsp;lock2.acquire();
   &nbsp;while&nbsp;(need&nbsp;to&nbsp;wait)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv.wait(&amp;lock2);
   &nbsp;}
   &nbsp;...
   &nbsp;lock2.release();
   &nbsp;...
   &nbsp;lock1.release();</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;B
   &nbsp;
   &nbsp;lock1.acquire();
   &nbsp;...
   &nbsp;lock2.acquire();
   &nbsp;...
   &nbsp;cv.signal();
   &nbsp;lock2.release();
   &nbsp;...
   &nbsp;lock1.release();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:nested waiting"}'>nested waiting</A></EM>, one shared object calls into another shared object while holding the first object&#8217;s lock, and then waits on a condition variable. CV::wait releases the lock of the second object, but not the first. Deadlock results if the thread that can signal the condition variable needs the first lock to make progress. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The problem of deadlock is much broader than just locks and condition variables. Deadlock can occur anytime a thread waits for an event that cannot happen because of a cycle of waiting for a resource held by the first thread. As in the examples above, resources can be locks, but they can also be any other scarce quantity: memory, processing time, disk blocks, or space in a buffer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we have two bounded buffers, where one thread puts a request into one buffer, and gets a response out of the other. Deadlock can result if another thread does the reverse. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;A
   &nbsp;
   &nbsp;buffer1.put();
   &nbsp;buffer1.put();
   &nbsp;...
   &nbsp;buffer2.get();
   &nbsp;buffer2.get();</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;B
   &nbsp;
   &nbsp;buffer2.put();
   &nbsp;buffer2.put();
   &nbsp;...
   &nbsp;buffer1.get();
   &nbsp;buffer1.get();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the buffers are almost full, both threads will need to wait for there to be room, and so neither will be able to reach the point where they can pull data out of the other buffer to allow the other thread to make progress. </FONT><A id=x1-9600114 name=x1-9600114></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00410.gif" data-calibre-src="OEBPS/Images/image00410.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.14: </B>An example of deadlock where three tractor-trailer trucks enter an intersection without first checking whether they can clear the intersection.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlocks also occur in real life. We encourage you to develop your intuition about deadlocks by considering why deadlocks occur and how we might prevent them. For example, if we lived in a world without stop signs, we might see the deadlock in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9600114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> more often. </FONT><A id=x1-9600215 name=x1-9600215></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00411.gif" data-calibre-src="OEBPS/Images/image00411.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.15: </B>In this example of the dining philosophers problem, there are 5 philosophers, 5 plates, and 5 chopsticks.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The scarce resource leading to deadlock can even be a chopstick. The Dining Philosophers problem is a classic illustration of both the challenges and solutions to deadlock; an example is shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9600215"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.15</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. There is a round table with n plates alternating with n chopsticks around the circle. A philosopher sitting at a plate requires two chopsticks to eat. Suppose that each philosopher proceeds by picking up the chopstick on the left, picking up the chopstick on the right, eating, and then putting down both chopsticks. If every philosopher follows this approach, there can be a deadlock: each philosopher takes the chopstick on the left but can be stuck waiting for the philosopher on the right to release the chopstick. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Note that mutually recursive locking is equivalent to Dining Philosophers with n = 2. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The rest of this section addresses the following questions: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Deadlock vs. Starvation.</B> How does deadlock relate to the concepts of liveness and starvation? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Necessary Conditions for Deadlock.</B> What conditions are required for deadlock to be possible? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Preventing Deadlock.</B> What techniques can be used to prevent deadlock? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The Banker&#8217;s Algorithm for Avoiding Deadlock.</B> The Banker&#8217;s Algorithm is a general-purpose mechanism for preventing deadlock by exploiting knowledge of what resources may be needed in the future. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Detecting and Recovering From Deadlock.</B> In some systems, deadlock is not prevented but repaired when it occurs. How can we detect deadlock and then recover?</FONT></P></LI></UL><A id=x1-96003r159 name=x1-96003r159></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.1 </FONT><A id=x1-970001 name=x1-970001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock vs. Starvation</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock and starvation are both liveness concerns. In <EM>starvation</EM>, a thread fails to make progress for an indefinite period of time. Deadlock is a form of starvation but with the stronger condition: a group of threads forms a cycle where none of the threads make progress because each thread is waiting for some other thread in the cycle to take action. Thus, deadlock implies starvation (literally, for the dining philosophers), but starvation does not imply deadlock. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, recall the readers/writers example discussed in Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-630001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.6.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. A writer only waits if a reader or writer is active. In the writers-preferred solution we gave, waiting readers can starve if new writers arrive sufficiently frequently; likewise, waiting writers can starve if there is an active reader, and new readers arrive and become active before the last one completes. Note that such starvation would not be deadlock because there is no cycle. The waiting readers are waiting on the active writers to finish, and the waiting writers are waiting on the active readers to finish, but no active thread is waiting on a waiting reader or writer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Just because a system can suffer deadlock or starvation does not mean that it always will. A system is <EM>subject to starvation</EM> if a thread could starve in some circumstances. A system is <EM>subject to deadlock</EM> if a group of threads could deadlock in some circumstances. Here, the circumstances that affect whether deadlock or starvation occurs could include a broad range of factors, such as: the choices made by the scheduler, the number of threads running, the workload or sequence of requests processed by the system, which threads win races to acquire locks, and which threads are enabled in what order when signals or broadcasts occur. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A system that is subject to starvation or deadlock may be live in many or most runs and starve or deadlock only for particular workloads or &#8220;unlucky&#8221; interleavings. For example, in mutually recursive locking, the deadlock only occurs if both threads obtain the outer locks at about the same time. For the Dining Philosophers problem, philosophers may succeed in eating for a long time before hitting the unlucky sequence of events that causes them to deadlock. Similarly, in the readers/writers example, the writers-preferred solution will allow some reads to complete as long as the rate of writes stays below some threshold. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since testing may not discover deadlock problems, it is important to construct systems that are deadlock-free by design. </FONT><A id=x1-97001r163 name=x1-97001r163></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.2 </FONT><A id=x1-980002 name=x1-980002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Necessary Conditions for Deadlock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">There are four necessary conditions for deadlock to occur. Knowing these conditions is useful for designing solutions: if you can prevent any one of these conditions, then you can eliminate the possibility of deadlock. </FONT>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-98002x1 name=x1-98002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bounded resources.</B> There are a finite number of threads that can simultaneously use a resource. </FONT></P>
<LI class=enumerate><A id=x1-98004x2 name=x1-98004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>No preemption.</B> Once a thread acquires a resource, its ownership cannot be revoked until the thread acts to release it. </FONT></P>
<LI class=enumerate><A id=x1-98006x3 name=x1-98006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Wait while holding.</B> A thread holds one resource while waiting for another. This condition is sometimes called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multiple independent requests"}'>multiple independent requests</A></EM> because it occurs when a thread first acquires one resource and then tries to acquire another. </FONT></P>
<LI class=enumerate><A id=x1-98008x4 name=x1-98008x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Circular waiting.</B> There is a set of waiting threads such that each thread is waiting for a resource held by another. </FONT></P></LI></OL><A id=x1-9800916 name=x1-9800916></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00412.gif" data-calibre-src="OEBPS/Images/image00412.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.16: </B>Graph representation of the state of a deadlocked Dining Philosophers system. Circles represent threads, boxes represent resources, an arrow from a box/resource to a circle/thread represents an <EM>owned by</EM> relationship, and an arrow from a circle/thread to a box/resource represents a <EM>waiting for</EM> relationship.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Show that the Dining Philosophers meet all four conditions for deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>To see that all four conditions are met, observe that </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-98011x1 name=x1-98011x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bounded resources.</B> Each chopstick can be held by a single philosopher at a time. </FONT></P>
<LI class=enumerate><A id=x1-98013x2 name=x1-98013x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>No preemption.</B> Once a philosopher picks up a chopstick, she does not release it until she is done eating, even if that means no one will ever eat. </FONT></P>
<LI class=enumerate><A id=x1-98015x3 name=x1-98015x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Wait while holding.</B> When a philosopher needs to wait for a chopstick, she continues to hold onto any chopsticks she has already picked up. </FONT></P>
<LI class=enumerate><A id=x1-98017x4 name=x1-98017x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Circular waiting.</B> Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9800916"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> maps the state of a deadlocked Dining Philosophers implementation to an abstract graph that shows which resources are <EM>owned by</EM> which threads and which threads <EM>wait for</EM> which resources. In this type of graph, if there is one instance of each type of resource (e.g., a particular chopstick), then a cycle implies deadlock assuming the system does not allow preemption.</FONT></P></LI></OL><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The four conditions are necessary <EM>but not sufficient</EM> for deadlock. When there are multiple instances of a type of resource, there can be a cycle of waiting without deadlock because a thread not in the cycle may return resources that enable a waiting thread to proceed. </FONT><A id=x1-9801817 name=x1-9801817></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00413.gif" data-calibre-src="OEBPS/Images/image00413.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.17: </B>Graph representation of the state of a Dining Philosophers system that includes a cycle among waiting threads and resources but that is not deadlocked. Circles represent threads, boxes represent resources, dots within a box represent multiple instances of a resource, an arrow from a dot/resource instance to a circle/thread represents an <EM>owned by</EM> relationship and an arrow from a circle/thread to a box/resource represents a <EM>waiting for</EM> relationship.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we have 5 philosophers at a table with 5 chopsticks, but the chopsticks are placed in a tray at the center of the table when not in use. We could be in the state illustrated in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9801817"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, where philosopher 1 has two chopsticks, philosophers 2, 3, and 4 each have one chopstick and are waiting for another chopstick, while philosopher 5 has no chopsticks. In this state, we have bounded resources (five chopsticks), no preemption (we cannot forcibly remove a chopstick from a hungry philosopher&#8217;s hand), wait while holding (philosophers 2, 3 and 4 are holding a chopstick while waiting for another), and circular waiting (each of philosophers 2, 3, and 4 are waiting for a resource held by another of them). However, we do not have deadlock. Eventually, philosopher 1 will release its two chopsticks, which may, for example, allow philosophers 2 and 3 to eat and release their chopsticks. In turn, this would allow philosophers 4 and 5 to eat. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although the system shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9801817"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> is not currently deadlocked, it is still <EM>subject to deadlock</EM>. For example, if philosopher 1 returns two chopsticks, philosopher 5 picks up one, and philosopher 1 picks up the other, then the system would deadlock. </FONT><A id=x1-98019r164 name=x1-98019r164></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.3 </FONT><A id=x1-990003 name=x1-990003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Preventing Deadlock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Preventing deadlock can be challenging. For example, consider a system with three resources &#8212; A, B, and C &#8212; and two threads that access them. Thread 1 acquires A then C then B, and thread 2 acquires B then C then A. The following sequence can lead to deadlock: </FONT>
<P><BR></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread 1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread 2</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Acquire A </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Acquire B </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Acquire C </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Wait for C </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Wait for B </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How could we avoid this deadlock? The deadlock&#8217;s circular waiting occurs when we reach step 5, but our fate was sealed much earlier. In particular, once we complete step 2 and thread 2 acquires B, deadlock is inevitable. To prevent the deadlock, we have to realize at step 2 that it will occur at step 5. Once step 1 completes and thread 1 acquires A, we cannot let thread 2 complete step 2 and acquire B or deadlock will follow. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This example illustrates that for an arbitrary program, preventing deadlock can take one of three approaches: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-99002x1 name=x1-99002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Exploit or limit the behavior of the program.</B> Often, we can change the behavior of a program to prevent one of the four necessary conditions for deadlock, and thereby eliminate the possibility of deadlock. In the above example, we can eliminate deadlock by changing the program to never wait for B while holding C. </FONT></P>
<LI class=enumerate><A id=x1-99004x2 name=x1-99004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Predict the future.</B> If we can know what threads may or will do, then we can avoid deadlock by having threads wait (e.g., thread 2 can wait at step 2 above) <EM>before</EM> they would head into a possible deadlock. </FONT></P>
<LI class=enumerate><A id=x1-99006x3 name=x1-99006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Detect and recover.</B> Another alternative is to allow threads to recover or &#8220;undo&#8221; actions that take a system into a deadlock; in the above example, when thread 2 finds itself in deadlock, it can recover by reverting to an earlier state.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We discuss these three options in this and the following two sub-sections. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-980002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> listed four necessary conditions for deadlock. These conditions are useful because they suggest approaches for preventing deadlock: if a system is structured to prevent at least one of the conditions, then the system cannot deadlock. Considering these conditions in the context of a given system often points to a viable deadlock prevention strategy. Below, we discuss some commonly used approaches. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bounded resources: Provide sufficient resources.</B> One way to ensure deadlock freedom is to arrange for sufficient resources to satisfy all threads&#8217; demands. A simple example would be to add a single chopstick to the middle of the table in Dining Philosophers; that is enough to eliminate the possibility of deadlock. As another example, thread implementations often reserve space in the TCB for the thread to be inserted into a waiting list or the ready list. While it would be theoretically possible to dynamically allocate space for the list entry only when it is needed, that could open up the chance that the system would run out of memory at exactly the wrong time, leading to deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>No preemption: Preempt resources.</B> Another technique is to allow the runtime system to forcibly reclaim resources held by a thread. For example, an operating system can preempt a page of memory from a running process by copying it to disk in order to prevent applications from deadlocking as they acquire memory pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Wait while holding: Release lock when calling out of module.</B> For nested modules, each of which has its own lock, waiting on a condition variable in an inner module can lead to a nested waiting deadlock. One solution is to restructure a module&#8217;s code so that no locks are held when calling other modules. For example, we can change the code on the left to the code on the right, provided that the program does not depend on the three steps occurring atomically: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Module::foo()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doSomeStuff();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherModule-&gt;bar();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doOtherStuff();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
   &nbsp;}
   &nbsp;
   &nbsp;Module::doSomeStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;x&nbsp;+&nbsp;1;
   &nbsp;}
   &nbsp;
   &nbsp;Module::doOtherStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;y&nbsp;-&nbsp;2;
   &nbsp;}</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Module::foo()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doSomeStuff();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherModule-&gt;bar();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doOtherStuff();
   &nbsp;}
   &nbsp;
   &nbsp;Module::doSomeStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;x&nbsp;+&nbsp;1;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
   &nbsp;}
   &nbsp;
   &nbsp;Module::doOtherStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;y&nbsp;-&nbsp;2;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Deadlock and kernel paging</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Early operating systems were often run on machines with very limited amounts of main memory. In response, going back at least as far as Multics, portions of the kernel (both code and data) could be swapped to disk in order to save space. Then, when the code and data was needed, they could be brought into main memory, swapping with some other portion of the kernel that was not currently in use. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A challenge to making this work was deadlock. The code to swap in or out portions of the kernel needed to be kept in memory, along with any code or data it might touch along any possible execution path. Without very strict module layering, it would be easy to miss a dependency that would, in rare cases, trigger a latent deadlock. Often, the only possible repair would be to reboot. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because of the inherent complexity of this approach, most modern operating systems keep all kernel code and almost all data structures memory resident; the one exception is that some kernels still swap the page tables for application virtual memory, a topic we will discuss in Chapter&nbsp;9. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In theory, one could eliminate the risk of deadlocks due to nested monitors by always releasing locks when calling code outside of a module. In practice, doing so is likely to be cumbersome, not only from the extra code needed to acquire and release locks, but also because of the extra thought needed to transform a single atomic method that holds a lock across a series of actions to a sequence of atomic methods that each acquire and release the lock. As a result, programmers often take the decidedly non-modular and admittedly unsatisfying approach of considering whether the outside module being called is likely to wait on something that depends on enclosing monitor lock. If such waiting is unlikely, the call can made with the enclosing lock held. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Circular waiting: Lock ordering.</B> An approach used in many systems is to identify an ordering among locks and only acquire locks in that order. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, C printf acquires a lock to ensure printed messages appear atomic rather than mixed up with those of other threads. Because waiting for that lock does not lead to circular waiting, printf can be safely called while holding most kernel locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For a hash table with per-bucket locks and an operation changeKeys(item, k1, k2) to move an item from one bucket to another, we can avoid deadlock by always acquiring the lock for the lower-numbered bucket before the one for the higher-numbered bucket. This prevents circular waiting since a thread only waits for threads holding higher-numbered locks. Those threads can be waiting as well, but only for threads with even higher-numbered locks, and so forth. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Likewise, we can eliminate deadlock among the dining philosophers if &#8212; instead of always picking up the chopstick on the left and then the one on the right &#8212; the philosophers number the chopsticks from 1 to n and always pick up the lower-numbered chopstick before the higher-numbered one. </FONT><A id=x1-99007r167 name=x1-99007r167></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.4 </FONT><A id=x1-1000004 name=x1-1000004></A><FONT style="BACKGROUND-COLOR: #7be1e1">The Banker&#8217;s Algorithm for Avoiding Deadlock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A general technique to eliminate wait-while-holding is to wait until all needed resources are available and then to acquire them atomically at the beginning of an operation, rather than incrementally as the operation proceeds. We saw this earlier with acquire-all/release-all; it cannot deadlock as long as the implementation acquires all of the locks atomically rather than one at a time. As another example, a dining philosopher might wait until the two neighboring chopsticks are available and then simultaneously pick them both up. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, a thread may not know exactly which resources it will need to complete its work, but it can still acquire all resources that it <EM>might</EM> need. Consider an operating system for mobile phones where memory is constrained and cannot be preempted by copying it to disk. Rather than having applications request additional memory as needed, we might instead have each application state its maximum memory needs and allocate that much memory when it starts. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Disadvantages of this approach include: the effect on program modularity, the challenge of having applications accurately estimate their worst-case needs, and the cost of allocating significantly more resources than may be necessary in the common case. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Dijkstra developed the Banker&#8217;s Algorithm as a way to improve on the performance of acquire-all. Although few systems use it in its full generality, we include the discussion because simplified versions of the algorithm are common. The Banker&#8217;s Algorithm also sheds light on the distinction between <EM>safe</EM> and <EM>unsafe</EM> states and how the occurrence of deadlocks often depends on a system&#8217;s workload and sequence of operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the Banker&#8217;s Algorithm, a thread states its maximum resource requirements when it begins a task, but it then acquires and releases those resources incrementally as the task runs. The runtime system delays granting some requests to ensure that the system never deadlocks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The insight behind the algorithm is that a system that may deadlock will not necessarily do so: for some interleavings of requests it will deadlock, but for others it will not. By delaying when some resource requests are processed, a system can avoid interleavings that could lead to deadlock. </FONT><A id=x1-10000118 name=x1-10000118></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00414.gif" data-calibre-src="OEBPS/Images/image00414.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.18: </B>A process can be in a <EM>safe</EM>, <EM>unsafe</EM>, or <EM>deadlocked</EM> state. The dashed line illustrates a sequence of states visited by a thread &#8212; some are safe, some are unsafe, and the final state is a deadlock.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A deadlock-prone system can be in one of three states: a <EM>safe state</EM>, an <EM>unsafe state</EM>, and a <EM>deadlocked state</EM> (see Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000118"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.) </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:safe state"}'>safe state</A></EM>, for any possible sequence of resource requests, there is at least one <EM>safe sequence</EM> of processing the requests that eventually succeeds in granting all pending and future requests. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:unsafe state"}'>unsafe state</A></EM>, there is at least one sequence of future resource requests that leads to deadlock no matter what processing order is tried. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:deadlocked state"}'>deadlocked state</A></EM>, the system has at least one deadlock.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A system in a safe state controls its own destiny: for any workload, it can avoid deadlock by delaying the processing of some requests. In particular, the Banker&#8217;s Algorithm delays any request that takes it from a safe to an unsafe state. Once the system enters an unsafe state, it may not be able to avoid deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice that an unsafe state does not always lead to deadlock. A system in an unsafe state may remain that way or return to a safe state, depending on the specific interleaving of resource requests and completions. However, as long as the system remains in an unsafe state, a bad workload or unlucky scheduling of requests can force it to deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Banker&#8217;s Algorithm keeps a system in a safe state. The algorithm is based on a loose analogy with a small-town banker who has a maximum amount, total, that can be loaned at one time and a set of businesses that each have a credit line, max[i], for business i. A business borrows and pays back amounts of money as various projects start and end, so that business i always has an outstanding loan amount between 0 and max[i]. If all of a business&#8217;s requests within the credit line are granted, the business eventually reaches a state where all current projects are finished, and the loan balance returns to zero. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A conservative banker might issue credit lines only until the sum is at most the total funds that the banker has available. This approach is analogous to <EM>acquire-all</EM> or <EM>provide sufficient resources</EM>. It guarantees that the system remains in a safe state. All businesses with credit lines eventually complete their projects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, a more aggressive banker can issue more credit as long as the bank can cover its commitment to each business &#8212; i.e., to provide a loan of max[i] if business i requests it. The algorithm assumes the bank is permitted to <EM>delay</EM> requests to increase a loan amount. For example, the bank might lose the paperwork for a few hours, days, or weeks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By delaying loan requests, the bank remains in a safe state &#8212; a state for which there exists at least one series of loan fulfillments by which every business i can eventually receive its maximal loan max[i], complete its projects, and pay back all of its loan. The bank can then use that repaid money to grant pending loans to other businesses. </FONT><A id=x1-10000219 name=x1-10000219></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;ResourceMgr{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;cv;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;r;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Number&nbsp;of&nbsp;resources
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;t;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Number&nbsp;of&nbsp;threads
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;avail[];&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;avail[i]:&nbsp;instances&nbsp;of&nbsp;resource&nbsp;i&nbsp;available
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;max[][];&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;max[i][j]:&nbsp;max&nbsp;of&nbsp;resource&nbsp;i&nbsp;needed&nbsp;by&nbsp;thread&nbsp;j
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;alloc[][];&nbsp;&nbsp;//&nbsp;alloc[i][j]:&nbsp;current&nbsp;allocation&nbsp;of&nbsp;resource&nbsp;i&nbsp;to&nbsp;thread&nbsp;j
&nbsp;&nbsp;&nbsp;...
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.19: </B>State maintained by the Banker Algorithm&#8217;s resource manager. Resource manager code is in Figures&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000320"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.20</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-10000320 name=x1-10000320></A><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Invariant:&nbsp;the&nbsp;system&nbsp;is&nbsp;in&nbsp;a&nbsp;safe&nbsp;state.
&nbsp;ResourceMgr::Request(int&nbsp;resourceID,&nbsp;int&nbsp;threadID)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.Acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert(isSafe());
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(!wouldBeSafe(resourceID,&nbsp;threadID))&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv.Wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alloc[resourceID][threadID]++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avail[resourceID]--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert(isSafe());
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.Release();
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.20: </B>High-level pseudo-code for the Banker&#8217;s Algorithm. The state maintained by the algorithm is defined in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000219"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.19</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. The methods isSafe and wouldBeSafe are defined in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-10000421 name=x1-10000421></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;A&nbsp;state&nbsp;is&nbsp;safe&nbsp;iff&nbsp;there&nbsp;exists&nbsp;a&nbsp;safe&nbsp;sequence&nbsp;of&nbsp;grants&nbsp;that&nbsp;are&nbsp;sufficient
&nbsp;//&nbsp;to&nbsp;allow&nbsp;all&nbsp;threads&nbsp;to&nbsp;eventually&nbsp;receive&nbsp;their&nbsp;maximum&nbsp;resource&nbsp;needs.
&nbsp;
&nbsp;bool
&nbsp;ResourceMgr::isSafe()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;j;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;toBeAvail[]&nbsp;=&nbsp;copy&nbsp;avail[];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;need[][]&nbsp;=&nbsp;max[][]&nbsp;-&nbsp;alloc[][];&nbsp;&nbsp;//&nbsp;need[i][j]&nbsp;is&nbsp;initialized&nbsp;to
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;max[i][j]&nbsp;-&nbsp;alloc[i][j]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;finish[]&nbsp;=&nbsp;[false,&nbsp;false,&nbsp;false,&nbsp;...];&nbsp;//&nbsp;finish[j]&nbsp;is&nbsp;true
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;thread&nbsp;j&nbsp;is&nbsp;guaranteed&nbsp;to&nbsp;finish
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;j&nbsp;=&nbsp;any&nbsp;threadID&nbsp;such&nbsp;that:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(finish[j]&nbsp;==&nbsp;false)&nbsp;&amp;&amp;&nbsp;forall&nbsp;i:&nbsp;need[i][j]&nbsp;&lt;=&nbsp;toBeAvail[i];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(no&nbsp;such&nbsp;j&nbsp;exists)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(forall&nbsp;j:&nbsp;finish[j]&nbsp;==&nbsp;true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;false;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;//&nbsp;Thread&nbsp;j&nbsp;will&nbsp;eventually&nbsp;finish&nbsp;and&nbsp;return&nbsp;its
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;current&nbsp;allocation&nbsp;to&nbsp;the&nbsp;pool.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;finish[j]&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forall&nbsp;i:&nbsp;&nbsp;toBeAvail[i]&nbsp;=&nbsp;toBeAvail[i]&nbsp;+&nbsp;alloc[i][j];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Hypothetically&nbsp;grant&nbsp;request&nbsp;and&nbsp;see&nbsp;if&nbsp;resulting&nbsp;state&nbsp;is&nbsp;safe.
&nbsp;
&nbsp;bool
&nbsp;ResourceMgr::wouldBeSafe(int&nbsp;resourceID,&nbsp;int&nbsp;threadID)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;result&nbsp;=&nbsp;false;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avail[resourceID]--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alloc[resourceID][threadID]++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(isSafe())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avail[resourceID]++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alloc[resourceID][threadID]--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;result;
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.21: </B>Pseudo-code for the Banker&#8217;s Algorithm test whether the next state would be safe to enter. If not, the system delays until it would be safe.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000320"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.20</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows pseudo-code for a version of the Banker&#8217;s Algorithm that manages a set of r resources for a set of t threads. To simplify the discussion, threads request each unit of resource separately, but the algorithm can be extended to allow multiple resources to be requested at the same time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The high-level idea is simple: when a request arrives, wait to grant the request until it is safe to do so. As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000219"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.19</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows, we can realize this high-level approach by tracking: (i) the current allocation of each resource to each thread, (ii) the maximum allocation possible for each thread, and (iii) the current set of available, unallocated resources. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows how to test whether a state is safe. Recall that a state is safe if some sequence of thread executions allows each thread to obtain its maximum resource need, finish its work, and release its resources. We first see if the currently free resources suffice to allow any thread to finish. If so, then the resources held by that thread will eventually be released back to the system. Next, we see if the currently free resources plus any resources held by the thread identified in the first step suffice to allow any other thread to finish; if so, the second thread&#8217;s resources will also eventually be released back to the system. We continue this process until we have identified all threads guaranteed to finish, provided we serve requests in a particular order. If that set includes all of the threads, the state is safe. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: Page allocation with the Banker&#8217;s Algorithm.</B> Suppose we have a system with 8 pages of memory and three processes: A, B, and C, which need 4, 5, and 5 pages to complete, respectively. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If they take turns requesting one page each, and the system grants requests in order, the system deadlocks, reaching a state where each process is stuck until some other process releases memory: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Process</B> </FONT></P></TD>
<TD class=td colSpan=6 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Allocation</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">A </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">B </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">C </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Total </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 6 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On the other hand, if the system follows the Banker&#8217;s Algorithm, then it can delay some processes and guarantee that all processes eventually complete: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Process</B> </FONT></P></TD>
<TD class=td colSpan=6 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Allocation</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">A </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>4</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>0</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">B </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>4</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>5</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>0</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">C </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>4</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>5</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>0</B></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Total </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 6 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 6 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By delaying B and C in the ninth through twelfth steps, A can complete and release its resources. Then, by delaying C in the fifteenth and sixteenth steps, B can complete and release its resources. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Banker&#8217;s Algorithm is noticeably more involved than other approaches we discuss. Although it is rarely used in its entirety, understanding the distinction between <EM>safe</EM>, <EM>unsafe</EM>, and <EM>deadlocked</EM> states and how deadlock events depend on request ordering are key to preventing deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Additionally, understanding the Banker&#8217;s Algorithm can help to design simple solutions for specific problems. For example, if we apply the Banker&#8217;s Algorithm to the Dining Philosopher&#8217;s problem, then it is safe for a philosopher to pick up a chopstick provided that afterwards (a) some philosopher will have two chopsticks or (b) a chopstick will remain on the table. In case (a), eventually that philosopher will finish eating and the other philosophers will be able to proceed. In case (b), the philosopher can pick up the chopstick because deadlock can still be avoided in the future. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Use the Banker&#8217;s Algorithm to devise a rule for when it is safe for a thread to acquire a pair of locks, A and B, with mutually recursive locking. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Suppose a thread needs to acquire locks A and B, in that order, while another thread needs to acquire lock B first, then A. <B>A thread is always allowed to acquire its second lock. It may acquire its first lock provided the other thread does not already hold its first lock.</B> &#9633; </FONT><A id=x1-100005r168 name=x1-100005r168></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.5 </FONT><A id=x1-1010005 name=x1-1010005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Detecting and Recovering From Deadlocks</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Rather than preventing deadlocks, some systems allow deadlocks to occur and recover from them when they arise. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Why allow deadlocks to occur at all? Sometimes, it is difficult or expensive to enforce sufficient structure on the system&#8217;s data and workloads to guarantee that deadlock will never occur. If deadlocks are rare, why pay the overhead in the common case to prevent them? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For this approach to work, we need: (i) a way to recover from deadlock when it occurs, ideally with minimal harm to the goals of the user, and (ii) a way to detect deadlock so that we know when to invoke the recovery mechanism. We discuss recovery first because it provides context for understanding the tradeoffs in implementing detection. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1020005 name=x1-1020005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Recovering From Deadlocks</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Recovering from a deadlock once it has occurred is challenging. A deadlock implies that some threads hold resources while waiting for others, and that progress is impossible. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because the resources are by definition not revocable, forcibly taking resources away from some or all of the deadlocked threads is not an ideal solution. As a simple example, if a process is part of a deadlock, some operating systems give the user the option to kill the process and release the process&#8217;s resources. Although this sounds drastic, if a deadlocked process cannot make any progress, killing it does not make the user much worse off. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, under the lock-based shared object programming abstractions we have discussed, killing all of the threads in a given process can be dangerous. If a deadlocked thread holds a lock on a shared kernel object, killing the thread and marking the lock as free could leave the kernel object in an inconsistent state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Instead, we need some systematic way to recover when some required resource is unavailable. Two widely used approaches have been developed to deal with this issue: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Proceed without the resource.</B> Web services are often designed to be resilient to resource unavailability. A rule of thumb for the web is that a significant fraction of a web site&#8217;s customers will give up and go elsewhere if the site&#8217;s latency becomes too long, for whatever reason. Whether the problem is a hardware failure, software failure, or deadlock, does not really matter. The web site needs to be designed to quickly respond back to the user, regardless of the type of problem. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Amazon&#8217;s web site is a good example of this design paradigm. It is designed as an interlocking set of modules, where any individual module can be offline because of a failure. Thus, all other parts of the web site must be designed to be able to cope when some needed resource is unavailable. For example, under normal operation, Amazon&#8217;s software will check the inventory to ensure that an item is in stock before completing an order. However, if a deadlock or failure causes the inventory server to delay responding beyond some threshold, the front-end web server will give up, complete the order, and then queue a background check to make sure the item was in fact in the inventory. If the item was in fact not available (e.g., because some other customer purchased it in the meantime), an apology is sent to the customer. As long as that does not happen often, it can be better than making the customer wait, especially in the case of deadlock, where the wait could be indefinite. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because deadlocks are rare and hard to test for, this requires coding discipline to handle error conditions systematically throughout the program. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Optimistic concurrency control</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Transactions can also be used to avoid deadlocks. Optimistic concurrency control lets transactions execute in parallel without locking any data, but it only lets a transaction commit if none of the objects accessed by the transaction have been modified since the transaction began. Otherwise, the transaction must abort and retry. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To implement transactions with optimistic concurrency control, Each transaction keeps track of which versions of which objects it reads and updates. All updates are applied to a local copy. Then, before a transaction commits, the system verifies that no object the transaction accessed has been modified in the meantime; if there is a conflict, the transaction must abort. Of course, committing a transaction may invalidate other transactions that are in progress (ones that use data modified by this transaction). Those conflicts will be detected when the later transactions try to commit. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Optimistic concurrency control works well when different transactions most commonly use different subsets of data. In these cases, the approach not only eliminates deadlock, but it also maximizes concurrency since threads do not wait for locks. On the other hand, many conflicting, concurrent transactions increase overhead by repeatedly rolling back and re-executing transactions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Transactions: rollback and retry.</B> A more general technique is used by <EM>transactions</EM>; transactions provide a safe mechanism for revoking resources assigned to a thread. We discuss transactions in detail in Chapter&nbsp;14; they are widely used in both databases and file systems. For deadlock recovery, transactions provide two important services: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-102002x1 name=x1-102002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread rollback.</B> Transactions ensure that revoking locks from a thread does not leave the system&#8217;s objects in an inconsistent state. Instead, we rollback, or undo, the deadlocked thread&#8217;s actions to a clean state. To fix the deadlock, we can choose one or more victim threads, stop them, undo their actions, and let other threads proceed. </FONT></P>
<LI class=enumerate><A id=x1-102004x2 name=x1-102004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread restarting.</B> Once the deadlock is broken and other threads have completed some or all of their work, the victim thread is restarted. When these threads complete, the system behaves as if the victim threads never caused a deadlock but, instead, just had their executions delayed.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A transaction defines a safe point for rollback and restart. Each transaction has a beginTransaction and endTransaction statement; rollback undoes all changes back to beginTransaction. After a rollback, the thread can be safely restarted at the beginTransaction. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A key feature of transactions is that no other thread is allowed to see the results of a transaction until the transaction completes. That way, if the changes a transaction makes need to be rolled back due to a deadlock, only that one thread is affected. This can be accomplished with two-phase locking, provided locks are not released until after the transaction is complete. If the transaction is successful, it <EM>commits</EM>, the transaction&#8217;s locks are released, and the transaction&#8217;s changes to shared state become visible to other threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If, however, a transaction fails to reach its endTransaction statement (e.g., because of a deadlock or because some other exception occurred), the transaction <EM>aborts</EM>. The system can reset all of the state modified by the transaction to what it was when the transaction began. One way to support this is to maintain a copy of the initial values of all state modified by each transaction; this copy can be discarded when the transaction commits. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If a transactional system becomes deadlocked, the system can abort one or more of the deadlocked transactions. Aborting these transactions rolls back the system&#8217;s state to what it would have been if these transactions had never started and releases the aborted transactions&#8217; locks and other resources. If aborting the chosen transactions releases sufficient resources, the deadlock is broken, and the remaining transactions can proceed. If not, the system can abort additional transactions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A related question that arises in transactional systems is <EM>which</EM> thread to abort and which threads to allow to proceed. An important consideration is liveness. Progress can be ensured, and starvation avoided, by prioritizing the oldest transactions. Then, when the system needs to abort some transaction, it can abort the <EM>youngest</EM>. This ensures that <EM>some</EM> transaction, e.g., the oldest, will eventually complete. The aborted transaction eventually becomes the oldest, and so it also will complete. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An example of this approach is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:wound wait"}'>wound wait</A></EM>. With wound wait, a younger transaction may wait for a resource held by an older transaction. Eventually, the older transaction will complete and release the resource, so deadlock cannot result. However, if an older transaction needs to wait on a resource held by a younger transaction, the resource is preempted and the younger transaction is aborted and restarted. </FONT></P></LI></UL>
<H5 class=subsubsectionHead><A id=x1-1030005 name=x1-1030005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Detecting Deadlock</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Once we have a general way to recover from a deadlock, we need a way to tell if a deadlock has occurred, so we know when to trigger the recovery. An important consideration is that the detection mechanism can be conservative: it can trigger the repair if we <EM>might</EM> be in a deadlock state. This approach risks a false positive where a non-deadlocked thread is incorrectly classified as deadlocked. Depending on the overhead of the repair operation, it can sometimes be more efficient to use a simpler mechanism for detection even if that leads to the occasional false positive. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, a program can choose to wait only briefly (or not to wait at all) before declaring that recovery is needed. We saw an example earlier with how Amazon&#8217;s web site is designed. As another example, in old-style, circuit-switched telephone networks, a call reserved a circuit at a series of switches along its path. If the connection setup failed to find a free circuit at any hop, rather than wait for a circuit at the next hop to become free, it cancelled the connection attempt and gave the user an error message, &#8220;All circuits are busy. Please try again later." </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A modern analogue is the Internet. When a router is overloaded and runs out of packet buffers, it simply drops incoming packets. An alternative would be for each router to wait to send a packet until it knows the next router has room &#8212; an approach that could lead to deadlock. Precisely identifying whether deadlock has occurred would incur more overhead than simply dropping and resending some packets. </FONT><A id=x1-10300122 name=x1-10300122></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00415.gif" data-calibre-src="OEBPS/Images/image00415.gif"> </FONT></P>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.22: </B>Example graphs used for deadlock detection. Left: single instance of each resource. Right: multiple instances of one resource. Threads and resources are nodes; directed edges represent the <EM>owned by</EM> and <EM>waiting for</EM> relationships among them. </FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are various ways to identify deadlocks more precisely. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If there are several resources and only one thread can hold each resource at a time (e.g., one printer, one keyboard, and one audio speaker or several mutual exclusion locks), then we can detect a deadlock by analyzing a simple graph. In the graph, shown on the left in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10300122"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.22</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, each thread and each resource is represented by a node. There is a directed edge (i) from a resource to a thread if the resource is <EM>owned by</EM> the thread and (ii) from a thread to a resource if the thread is <EM>waiting for</EM> the resource. There is a deadlock if and only if there is a cycle in such a graph. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If there are multiple instances of some resources, then we represent a resource with k interchangeable instances (e.g., k equivalent printers) as a node with k connection points. This is illustrated by the right graph in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10300122"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.22</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Now, a cycle is a necessary but not sufficient condition for deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another solution, described by Coffman, Elphick, and Shoshani in 1971 is a variation of Dijkstra&#8217;s Banker&#8217;s Algorithm. In this algorithm, we assume we no longer know max[][], so we cannot assess whether the current state is safe or whether some future sequence of requests can force deadlock. However, we can look at the current set of resources, granted requests, and pending requests and ask whether it is possible for the current set of requests to eventually be satisfied assuming no more requests come and all threads eventually complete. If so, there is no deadlock (although we may be in an unsafe state); otherwise, there is a deadlock. </FONT><A id=x1-10300223 name=x1-10300223></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;A&nbsp;state&nbsp;is&nbsp;safe&nbsp;iff&nbsp;there&nbsp;exists&nbsp;a&nbsp;safe&nbsp;sequence&nbsp;of&nbsp;grants&nbsp;that&nbsp;would&nbsp;allow
&nbsp;//&nbsp;all&nbsp;threads&nbsp;to&nbsp;eventually&nbsp;receive&nbsp;their&nbsp;maximum&nbsp;resource&nbsp;needs.
&nbsp;//
&nbsp;//&nbsp;avail[]&nbsp;holds&nbsp;free&nbsp;resource&nbsp;count
&nbsp;//&nbsp;alloc[][]&nbsp;holds&nbsp;current&nbsp;allocation
&nbsp;//&nbsp;request[][]&nbsp;holds&nbsp;currently-blocked&nbsp;requests
&nbsp;bool
&nbsp;ResourceMgr::isDeadlocked()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;j;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;toBeAvail[]&nbsp;=&nbsp;copy&nbsp;avail[];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;finish[]&nbsp;=&nbsp;[false,&nbsp;false,&nbsp;false,&nbsp;...];&nbsp;//&nbsp;finish[j]&nbsp;is&nbsp;true&nbsp;if&nbsp;thread
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;j&nbsp;is&nbsp;guaranteed&nbsp;to&nbsp;finish
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while(true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;j&nbsp;=&nbsp;any&nbsp;threadID&nbsp;such&nbsp;that&nbsp;(finish[j]&nbsp;==&nbsp;false)&nbsp;&amp;&amp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(forall&nbsp;i:&nbsp;request[i][j]&nbsp;&lt;=&nbsp;toBeAvail[i]);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(no&nbsp;such&nbsp;j&nbsp;exists)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(forall&nbsp;j:&nbsp;finish[j]&nbsp;==&nbsp;true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;false;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Thread&nbsp;j&nbsp;*may*&nbsp;eventually&nbsp;finish&nbsp;and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;return&nbsp;its&nbsp;current&nbsp;allocation&nbsp;to&nbsp;the&nbsp;pool.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;finish[j]&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forall&nbsp;i:&nbsp;toBeAvail[i]&nbsp;=&nbsp;toBeAvail[i]&nbsp;+&nbsp;alloc[i][j];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.23: </B>Coffman et al.&#8217;s test for deadlock. This algorithm is similar to the isSafe() test of the Banker&#8217;s Algorithm shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10300223"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.23</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the pseudo-code for the isDeadlocked method, a variation of the isSafe method shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> for the Banker&#8217;s Algorithm. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might hope that we could avoid deadlock by asking, &#8220;Will satisfying the current request put us in a deadlocked state?&#8221; and then blocking any request that does. The Coffman et al. algorithm highlights that deadlock is determined not just by what requests are granted but also by what requests are waiting. The request that triggers deadlock (&#8220;circular wait&#8221;) will be a request that waits, not one that is granted. </FONT><A id=x1-103003r160 name=x1-103003r160></A></P><A id=x1-1040006 name=x1-1040006>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.6 Non-Blocking Synchronization</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> described a core abstraction for synchronization &#8212; shared objects, with one lock per object. This abstraction works well for building multi-threaded programs the vast majority of the time. As concurrent programs become more complicated, however, issues of lock contention, the semantics of operations that span multiple objects, and deadlock can arise. Worse, the solutions to these issues often require us to compromise modularity; for example, whether a particular program can deadlock requires understanding in detail how the implementations of various shared objects interact. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some researchers have posed a radical question: would it be better to write complex concurrent programs without locks? By eliminating locking, we would remove lock contention and deadlock as design considerations, fostering a more modular program structure. However, these techniques can be <EM>much</EM> more complex to use. To date, concurrent implementations without locks have only been used for a few carefully designed runtime library modules written by expert programmers. We sketch the ideas because there is a chance that they will become more important as the number of processors per computer continues to increase. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Today, the cases where these approaches are warranted are rare. These advanced techniques should only be considered by experienced programmers who have mastered the basic lock-based approaches. Many of you will probably never need to use these techniques. If you are tempted to do so, take extra care. Measure the performance of your system to ensure that these techniques yield significant gains, and seek out extra peer review from trusted colleagues to help ensure that the code works as intended. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Programmers often assume that acquiring a lock is an expensive operation, and therefore try to reduce locking throughout their programs. The most likely result from premature optimization is a program that is buggy, hard to maintain, no faster than a clean implementation, and, ironically, harder to tune than a cleanly architected program. On most platforms, acquiring or releasing a lock is a highly tuned primitive &#8212; acquiring an uncontended lock is often nearly free. If there is contention, you probably needed the lock! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-850003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, we saw an example of synchronization without locks. RCU lets reads proceed without acquiring a lock or updating shared synchronization state, but it still requires updates to acquire locks. If the thread that holds the lock is interrupted, has a bug that causes it to stop making progress, or becomes deadlocked, other threads can be delayed for a long &#8212; perhaps unlimited &#8212; period of time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is possible to build data structures that are completely <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:non-blocking data structure"}'>non-blocking</A></EM> for both read and write operations. A non-blocking method is one where one thread is never required to wait for another thread to complete its operation. Acquiring a lock is a blocking operation: if the thread holding the lock stops, is delayed, or deadlocks, all other threads must wait for it to finish the critical section. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">More formally, a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:wait-free data structures"}'>wait-free data structure</A></EM> is one that guarantees progress for every thread: every method finishes in a finite number of steps, regardless of the state of other threads executing in the data structure or their rate of execution. A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:lock-free data structures"}'>lock-free data structure</A></EM> is one that guarantees progress for some thread: some method will finish in a finite number of steps. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A common building block for wait-free and lock-free data structures is the atomic compare-and-swap instruction available on most modern processors. We saw a taste of this in the implementation of the MCS lock in Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-850003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. There, we used compare-and-swap to atomically append to a linked list of waiting threads <EM>without first acquiring a lock</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Wait-free and lock-free data structures apply this idea more generally to completely eliminate the use of locks. For example, a lock-free hash table could be built as an array of pointers to each bucket: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Lookup.</B> A lookup de-references the pointer and checks the bucket. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Update.</B> To update a bucket, the thread allocates a new copy of the bucket, and then uses compare-and-swap to atomically replace the pointer if and only if it has not been changed in the meantime. If two threads simultaneously attempt to update the bucket (for example, to add a new entry), one succeeds and the other must retry.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The logic can be much more complex for more intricate data structures, and as a result, designing efficient wait-free and lock-free data structures remains the domain of experts. Nonetheless, non-blocking algorithms exist for a wide range of data structures, including FIFO queues, double-ended queues, LIFO stacks, sets, and balanced trees. Several of these can be found in the Java Virtual Machine runtime library. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In addition, considerable effort has also gone into studying ways to automate the construction of wait-free and lock-free data structures. For example, transactions with optimistic concurrency control provide a very flexible approach to implementing lock-free applications. Recall that optimistic concurrency control lets transactions proceed without locking the data they access. Transactions abort if, at commit-time, any of their accessed data has changed in the meantime. Most modern databases use a form of optimistic concurrency control to provide atomic and fault-tolerant updates of on-disk data structures. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Is optimistic concurrency control lock-free, wait-free, or both? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>To see that <B>optimistic concurrency control is lock-free,</B> consider two conflicting transactions executing at the same time. The first one to commit succeeds, and the second must abort and retry. <B>An implementation is wait-free if it uses wound wait or some other mechanism to bound the number of retries for a transaction to successfully commit.</B> &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Extending this idea, <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:software transactional memory (STM)"}'>software transactional memory (STM)</A></EM> is a promising approach to support general-purpose transactions for in-memory data structures. Unfortunately, the cost of an STM transaction is often significantly higher than that of a traditional critical section; this is because of the need to maintain the state required to check dependencies and the state required either to update the object if there is no conflict or to roll back its state if a conflict is detected. It is an open question whether the overhead of STM can be reduced to where it can be used more widely. In situations where STM can be used, it provides a way to compose different modules without having to lock contention or deadlock concerns. </FONT><A id=x1-104001r178 name=x1-104001r178></A></P><A id=x1-1050007 name=x1-1050007>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.7 Summary and Future Directions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Advanced synchronization techniques should be approached with caution. Your first goal should be to construct a program that works, even if doing so means putting &#8220;one big lock&#8221; around everything in a data structure or even in an entire program. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Resist the temptation to do anything more complicated unless you <B>know</B> that doing so is necessary. How do you know? Do not guess. Measure your system&#8217;s performance. Measuring the &#8220;before&#8221; and &#8220;after&#8221; performance of a program and its subsystems not only helps you make good decisions about the program on which you are working, but it also helps you develop good intuition for the programs you write in the future. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Spend time early in the design process developing a clean structure for your program. Given that issues with multi-object synchronization often blur module boundaries, it is vital to have an overall structure that lets you reason about how the different pieces of your program will interact. Strive for a strict layering or hierarchy of modules. It is easier to make such programs deadlock-free, and it is easier to test them as well. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although performance is important, it is usually easier to start with a clean, simple, and correct design, measure it to identify its bottlenecks, and then optimize the bottlenecks than to start with a complex design and try to tune its performance, let alone fix its bugs. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this chapter, we have presented a set of conceptual tools and techniques for managing complex, multi-object concurrent programs. We have addressed: estimating the impact of locks on multiprocessor performance, design patterns to reduce contention for locks, implementation techniques such as MCS and RCU for high-contention locks, strategies for achieving atomicity across multiple operations on the same object or across objects, and algorithms for deadlock prevention and recovery. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Yet, writing concurrent programs remains frustratingly complex. We believe that an important area for future work will be to develop better tools for managing and reducing that complexity. The last decade has seen the development of a new generation of tools for helping programmers improve software reliability, by automatically identifying test coverage, memory leaks, reuse of de-allocated data, buffer overflows, and bad pointer arithmetic. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Extending this approach to concurrent programs is a grand challenge. A promising avenue is to use automated tools for detecting memory races; a well-written program should have no reads or writes to shared memory without holding the lock that protects that data structure. Once a program has been shown to be without races, model checking can be used to systematically test that shared objects work for all possible thread interleavings. </FONT><A id=Q1-1-180 name=Q1-1-180></A><A id=Q1-1-181 name=Q1-1-181></A></P><A id=x1-1060007 name=x1-1060007>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=problems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9400113"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the parallel execution of some requests and an equivalent sequential execution &#8212; request 1 then request 2 then request 3. Two other sequential executions are also equivalent to the parallel execution shown in the figure. What are these other equivalent sequential executions? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Generalize the rules for two-phase locking to include both mutual exclusion locks and readers/writers locks. What can be done in the expanding phase? What can be done in the contracting phase? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the variation of the Dining Philosophers problem shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9801817"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, where all unused chopsticks are placed in the center of the table and any philosopher can eat with any two chopsticks. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One way to prevent deadlock in this system is to </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-990003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">provide sufficient resources</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. For a system with n philosophers, what is the minimum number of chopsticks that ensures deadlock freedom? Why? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">If the queues between stages are finite, is it possible for a staged architecture to deadlock even if each individual stage is internally deadlock free? If so, give an example. If not, prove it. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you build a system using a staged architecture with some fixed number of threads operating in each stage. Assuming each stage is individually deadlock free, describe two ways to guarantee that your system as a whole cannot deadlock. Each way should eliminate a different one of the 4 necessary conditions for deadlock. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a system with four mutual exclusion locks (A, B, C, and D) and a readers/writers lock (E). Suppose the programmer follows these rules: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Processing for each request is divided into two parts. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">During the first part, no lock may be released, and, if E is held in writing mode, it cannot be downgraded to reading mode. Furthermore, lock A may not be acquired if any of locks B, C, D, or E are held in any mode. Lock B may not be acquired if any of locks C, D, or E are held in any mode. Lock C may not be acquired if any of locks D or E are held in any mode. Lock D may not be acquired if lock E is held in any mode. Lock E may always be acquired in read mode or write mode, and it can be upgraded from read to write mode but not downgraded from write to read mode. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">During the second part, any lock may be released, and lock E may be downgraded from write mode to read mode; releases and downgrades can happen in any order; by the end of part 2, all locks must be released; and no locks may be acquired or upgraded. </FONT>
<P></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Do these rules ensure serializability? Do they ensure freedom from deadlock? Why? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">In RCUList::remove, a possible strategy to increase concurrency would be to hold a read lock while searching for the target item, and to grab the write lock once it is found. Specifically: (i) replace the writeLock and writeUnlock calls with readLock and readUnlock calls, and (ii) insert new writeLock and writeUnlock calls at the beginning and end of the code that is executed when the if conditional test succeeds. Will this work? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Implement a highly concurrent, multi-threaded file buffer cache. A buffer cache stores recently used disk blocks in memory for improved latency and throughput. Disk blocks have unique numbers and are fixed size. The cache provides two routines: </FONT>
<P><BR></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;blockread(char&nbsp;*x,&nbsp;int&nbsp;blocknum);
   &nbsp;
   &nbsp;void&nbsp;blockwrite(char&nbsp;*x,&nbsp;int&nbsp;blocknum);</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These routines read/write complete, block-aligned, fixed-size blocks. blockread reads a block of data into x; blockwrite (eventually) writes the data in x to disk. On a read, if the requested data is in the cache, the buffer will return it. Otherwise, the buffer must fetch the data from disk, making room in the cache by evicting a block as necessary. If the evicted block is modified, the cache must first write the modified data back to disk. On a write, if the block is not already in the buffer, it must make room for the new block. Modified data is stored in the cache and written back later to disk when the block is evicted. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multiple threads can call blockread and blockwrite concurrently, and to the maximum degree possible, those operations should be allowed to complete in parallel. You should assume the disk driver has been implemented; it provides the same interface as the file buffer cache: diskblockread and diskblockwrite. The disk driver routines are synchronous (the calling thread blocks until the disk operation completes) and re-entrant (while one thread is blocked, other threads can call into the driver to queue requests). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we have a version of the Dining Philosopher&#8217;s problem where the chopsticks are placed in the middle of the table, each Philosopher needs three chopsticks before she will start to eat, and every Philosopher will return all of their chopsticks to the shared pool when done eating. (For example, the Philosopher needs two chopsticks to eat with and one to point at the white board.) </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Using the Banker&#8217;s Algorithm, devise a rule for when is it safe for a Philosopher to pick up a chopstick. Explain why. </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Now suppose each Philosopher needs k chopsticks, for k &gt; 3. Generalize the rule you developed above to work for any k. </FONT></LI></OL></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-1070007 name=x1-1070007>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">7. Scheduling</FONT></I></H2></A>
<DIV class=chapterQuote>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Time is money &#8212;<I>Ben Franklin</I> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The best performance improvement is the transition from the non-working state to the working state. That&#8217;s infinite speedup. &#8212;<I>John Ousterhout</I> </FONT></P>
<DL>
<DT><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DD><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DD></DL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
<BR></FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When there are multiple things to do, how do you choose which one to do first? In the last few chapters, we have described how to create threads, switch between them, and synchronize their access to shared data. At any point in time, some threads are running on the system&#8217;s processor. Others are waiting their turn for a processor. Still other threads are blocked waiting for I/O to complete, a condition variable to be signaled, or for a lock to be released. When there are more runnable threads than processors, the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:processor scheduling policy"}'>processor scheduling policy</A></EM> determines which threads to run first. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You might think the answer to this question is easy: just do the work in the order in which it arrives. After all, that seems to be the only fair thing to do. Because it is obviously fair, almost all government services work this way. When you go to your local Department of Motor Vehicles (DMV) to get a driver&#8217;s license, you take a number and wait your turn. Although fair, the DMV often feels slow. There&#8217;s a reason why: as we&#8217;ll see later in this chapter, doing things in order of arrival is sometimes the worst thing you can do in terms of improving user-perceived response time. Advertising that your operating system uses the same scheduling algorithm as the DMV is probably not going to increase your sales! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You might think that the answer to this question is unimportant. With the million-fold improvement in processor performance over the past thirty years, it might seem that we are a million times less likely to have anything waiting for its turn on a processor. We disagree! Server operating systems in particular are often overloaded. Parallel applications can create more work than processors, and if care is not taken in the design of the scheduling policy, performance can badly degrade. There are subtle relationships between scheduling policy and energy management on battery-powered devices such as smartphones and laptops. Further, scheduling issues apply to any scarce resource, whether the source of contention is the processor, memory, disk, or network. We will revisit the issues covered in this chapter throughout the rest of the book. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling policy is not a panacea. Without enough capacity, performance may be poor regardless of which thread we run first. In this chapter, we will also discuss how to predict overload conditions and how to adapt to them. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, you probably have quite a bit of intuition as to impact of different scheduling policies and capacity on issues like response time, fairness, and throughput. Anyone who waits in line probably wonders how we could get the line to go faster. That&#8217;s true whether we&#8217;re waiting in line at the supermarket, a bank, the DMV, or at a popular restaurant. Remarkably, in each of these settings, there is a different approach to how they deal with waiting. We will try to answer why. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There is no one right answer; rather, any scheduling policy poses a complex set of tradeoffs between various desirable properties. The goal of this chapter is not to enumerate all of the interesting possibilities, explore the full design space, or even to identify specific useful policies. Instead, we describe some of the trade-offs and try to illustrate how a designer can approach the problem of selecting a scheduling policy. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider what happens if you are running the web site for a company trying to become the next Facebook. Based on history, you&#8217;ll be able to guess how much server capacity you need to be able to keep up with demand and still have reasonable response time. What happens if your site appears on Slashdot, and suddenly you have twice as many users as you had an hour ago? If you are not careful, everyone will think your site is terribly slow, and permanently go elsewhere. Google, Amazon, and Yahoo have each estimated that they lose approximately 5-10% of their customers if their response time increases by as little as 100 milliseconds. If faced with overload: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Would quickly implementing a different scheduling policy help, or hurt? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How much worse will your performance be if the number of users doubles again? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Should you turn away some users so that others will get acceptable performance? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Does it matter which users you turn away? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If you run out to the local electronics store and buy a server, how much better will performance get? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Do the answers change if you are under a denial-of-service attack by a competitor?</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this chapter, we will try to give you the conceptual and analytic tools to help you answer these questions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Performance terminology</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In Chapter 1 we defined some performance-related terms we will use throughout this chapter and the rest of the book; we summarize those terms here. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Task.</B> A user request. A task is also often called a <EM>job</EM>. A task can be any size, from simply redrawing the screen to show the movement of the mouse cursor to computing the shape of a newly discovered protein. When discussing scheduling, we use the term task, rather than thread or process, because a single thread or process may be responsible for multiple user requests or tasks. For example, in a word processor, each character typed is an individual user request to add that character to the file and display the result on the screen. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Response time (or delay).</B> The user-perceived time to do some task. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Predictability.</B> Low variance in response times for repeated requests. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Throughput.</B> The rate at which tasks are completed. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Scheduling overhead.</B> The time to switch from one task to another. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Fairness.</B> Equality in the number and timeliness of resources given to each task. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Starvation.</B> The lack of progress for one task, due to resources given to a higher priority task.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Chapter roadmap:</B> </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Uniprocessor Scheduling.</B> How do uniprocessor scheduling policies affect fairness, response time, and throughput? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1080001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiprocessor Scheduling.</B> How do scheduling policies change when we have multiple processor cores per computer? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1150002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Energy-Aware Scheduling.</B> Many new computer systems can save energy by turning off portions of the computer, slowing the execution speed. How do we make this tradeoff while minimizing the impact on user perceived response time? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1210003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Real-Time Scheduling.</B> More generally, how do we make sure tasks finish in time? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1220004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Queueing Theory.</B> In a server environment, how are response time and throughput affected by the rate at which requests arrive for processing and by the scheduling policy? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1230005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Overload Management.</B> How do we keep response time reasonable when a system becomes overloaded? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1330006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Case Study: Servers in a Data Center.</B> How do we combine these technologies to manage servers a data center? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1340007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P></LI></UL><A id=x1-107001r179 name=x1-107001r179></A><A id=x1-1080001 name=x1-1080001>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1 Uniprocessor Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">We start by considering one processor, generalizing to multiprocessor scheduling policies in the next section. We begin with three simple policies &#8212; first-in-first-out, shortest-job-first, and round robin &#8212; as a way of illustrating scheduling concepts. Each approach has its own the strengths and weaknesses, and most resource allocation systems (whether for processors, memory, network or disk) combine aspects of all three. At the end of the discussion, we will show how the different approaches can be synthesized into a more practical and complete processor scheduler. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Before proceeding, we need to define a few terms. A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:workload"}'>workload</A></EM> is a set of tasks for some system to perform, along with when each task arrives and how long each task takes to complete. In other words, the workload defines the input to a scheduling algorithm. Given a workload, a processor scheduler decides when each task is to be assigned the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We are interested in scheduling algorithms that work well across a wide variety of environments, because workloads will vary quite a bit from system to system and user to user. Some tasks are <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:compute-bound task"}'>compute-bound</A></EM> and only use the processor. Others, such as a compiler or a web browser, mix I/O and computation. Still others, such as a BitTorrent download, are <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:I/O-bound task"}'>I/O-bound</A></EM>, spending most of their time waiting for I/O and only brief periods computing. In the discussion, we start with very simple compute-bound workloads and then generalize to include mixtures of different types of tasks as we proceed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some of the policies we outline are the best possible policy on a particular metric and workload, and some are the worst possible policy. When discussing optimality and pessimality, we are only comparing to policies that are <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:work-conserving scheduling policy"}'>work-conserving</A></EM>. A scheduler is work-conserving if it never leaves the processor idle if there is work to do. Obviously, a trivially poor policy has the processor sit idle for long periods when there are tasks in the ready list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our discussion also assumes the scheduler has the ability to <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:preemption"}'>preempt</A></EM> the processor and give it to some other task. Preemption can happen either because of a timer interrupt, or because some task arrives on the ready list with a higher priority than the current task, at least according to some scheduling policy. We explained how to switch the processor between tasks in Chapter&nbsp;2 and Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. While much of the discussion is also relevant to non-preemptive schedulers, there are few such systems left, so we leave that issue aside for simplicity. </FONT><A id=x1-108001r173 name=x1-108001r173></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.1 </FONT><A id=x1-1090001 name=x1-1090001></A><FONT style="BACKGROUND-COLOR: #7be1e1">First-In-First-Out (FIFO)</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Perhaps the simplest scheduling algorithm possible is first-in-first-out (FIFO): do each task in the order in which it arrives. (FIFO is sometimes also called first-come-first-served, or FCFS.) When we start working on a task, we keep running it until it finishes. FIFO minimizes overhead, switching between tasks only when each one completes. Because it minimizes overhead, if we have a fixed number of tasks, and those tasks only need the processor, FIFO will have the best throughput: it will complete the most tasks the most quickly. And as we mentioned, FIFO appears to be the definition of fairness &#8212; every task patiently waits its turn. </FONT><A id=x1-1090011 name=x1-1090011></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00416.gif" data-calibre-src="OEBPS/Images/image00416.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.1: </B>Completion times with FIFO (top) and SJF (bottom) scheduling when several short tasks (2-5) arrive immediately after a long task (1).</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, FIFO has a weakness. If a task with very little work to do happens to land in line behind a task that takes a very long time, then the system will seem very inefficient. Figure &nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1090011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates a particularly bad workload for FIFO; it also shows SJF, which we will discuss in a bit. If the first task in the queue takes one second, and the next four arrive an instant later, but each only needs a millisecond of the processor, then they will all need to wait until the first one finishes. The average response time will be over a second, but the optimal average response time is much less than that. In fact, if we ignore switching overhead, there are some workloads where FIFO is literally the worst possible policy for average response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>FIFO and memcached</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although you may think that FIFO is too simple to be useful, there are some important cases where it is exactly the right choice for the workload. One such example is memcached. Many web services, such as Facebook, store their user data in a database. The database provides flexible and consistent lookups, such as, which friends need to be notified of a particular update to a user&#8217;s Facebook wall. In order to improve performance, Facebook and other systems put a cache called memcached in front of the database, so that if a user posts two items to her Facebook wall, the system only needs to lookup the friend list once. The system first checks whether the information is cached, and if so uses that copy. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because almost all requests are for small amounts of data, memcached replies to requests in FIFO order. This minimizes overhead, as there is no need to time slice between requests. For this workload where tasks are roughly equal in size, FIFO is simple, minimizes average response time, and even maximizes throughput. Win-win! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-109002r185 name=x1-109002r185></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.2 </FONT><A id=x1-1100002 name=x1-1100002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Shortest Job First (SJF)</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">If FIFO can be a poor choice for average response time, is there an optimal policy for minimizing average response time? The answer is yes: schedule the shortest job first (SJF). </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we could know how much time each task needed at the processor. (In general, we will not know, so this is not meant as a practical policy! Rather, we use it as a thought experiment; later on, we will see how to approximate SJF in practice.) If we always schedule the task that has the least remaining work to do, that will minimize average response time. (For this reason, some call SJF shortest-remaining-time-first or SRTF.) </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To see that SJF is optimal, consider a hypothetical alternative policy that is not SJF, but that we think might be optimal. Because the alternative is not SJF, at some point it will choose to run a task that is longer than something else in the queue. If we now switch the order of tasks, keeping everything the same, but doing the shorter task first, we will reduce the average response time. Thus, any alternative to SJF cannot be optimal. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1090011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates SJF on the same example we used for FIFO. If a long task is the first to arrive, it will be scheduled (if we are work-conserving). When a short task arrives a bit later, the scheduler will preempt the current task, and start the shorter one. The remaining short tasks will be processed in order of arrival, followed by finishing the long task. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">What counts as &#8220;shortest&#8221; is the remaining time left on the task, not its original length. If we are one nanosecond away from finishing an hour-long task, we will minimize average response time by staying with that task, rather than preempting it for a minute long task that just arrived on the ready list. Of course, if they both arrive at about the same time, doing the minute long task first will dramatically improve average response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Starvation and sample bias</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Systems that might suffer from starvation require extra care when being measured. Suppose you want to compare FIFO and SJF experimentally. You set up two computers, one running each scheduler, and send them the same sequence of tasks. After some period, you stop and report the average response time of completed tasks. If some tasks starve, however, the set of completed tasks will be different for the two policies. We will have excluded the longest tasks from the results for SJF, skewing the average response time even further. Put another way, if you want to manipulate statistics to &#8220;prove&#8221; a point, this is a good trick to use! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How might you redesign the experiment to provide a valid comparison between FIFO and SJF? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Does SJF have any other downsides (other than being impossible to implement because it requires knowledge of the future)? It turns out that SJF is pessimal for variance in response time. By doing the shortest tasks as quickly as possible, SJF necessarily does longer tasks as slowly as possible (among policies that are work-conserving). In other words, there is a fundamental tradeoff between reducing average response time and reducing the variance in average response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Worse, SJF can suffer from starvation and frequent context switches. If enough short tasks arrive, long tasks may never complete. Whenever a new task on the ready list is shorter than the remaining time left on the currently scheduled task, the scheduler will switch to the new task. If this keeps happening indefinitely, a long task may never finish. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose a supermarket manager reads a portion of this textbook and decides to implement shortest job first to reduce average waiting times. The manager tells herself: who cares about variance! A benefit is that there would no longer be any need for express lanes &#8212; if someone has only a few items, she can be immediately whisked to the front of the line, interrupting the parent shopping for eighteen kids. Of course, the wait times of the customers with full baskets skyrockets; if the supermarket is open twenty-four hours a day, customers with the largest purchases might have to wait until 3am to finally get through the line. This would probably lead their best customers to go to the supermarket down the street, not exactly what the manager had in mind! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Customers could also try to game the system: if you have a lot of items to purchase, simply go through the line with one item at a time &#8212; you will always be whisked to the front, at least until everyone else figures out the same dodge. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Shortest Job First and bandwidth-constrained web service</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although SJF may seem completely impractical, there are circumstances where it is exactly the right policy. One example is in a web server for static content. Many small-scale web servers are limited by their bandwidth to the Internet, because it is often more expensive to pay for more capacity. Web pages at most sites vary in size, with most pages being relatively short, while some pages are quite large. The average response time for accessing web pages is dominated by the more frequent requests to short pages, while the bandwidth costs are dominated by the less frequent requests to large pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This combination is almost ideal for using SJF for managing the allocation of network bandwidth by the server. With static pages, it is possible to predict from the name of the page how much bandwidth each request will consume. By transferring short pages first, the web server can ensure that its average response time is very low. Even if most requests are to small pages, the aggregate bandwidth for small pages is low, so requests to large pages are not significantly slowed down. The only difficulty comes when the web server is overloaded, because then the large page requests can be starved. As we will see later, overload situations need their own set of solutions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-110001r187 name=x1-110001r187></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.3 </FONT><A id=x1-1110003 name=x1-1110003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Round Robin</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A policy that addresses starvation is to schedule tasks in a round robin fashion. With Round Robin, tasks take turns running on the processor for a limited period of time. The scheduler assigns the processor to the first task in the ready list, setting a timer interrupt for some delay, called the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:time quantum"}'>time quantum</A></EM>. At the end of the quantum, if the task has not completed, the task is preempted and the processor is given to the next task in the ready list. The preempted task is put back on the ready list where it can wait its next turn. With Round Robin, there is no possibility that a task will starve &#8212; it will eventually reach the front of the queue and get its time quantum. </FONT><A id=x1-1110012 name=x1-1110012></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00417.gif" data-calibre-src="OEBPS/Images/image00417.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.2: </B>Completion times with Round Robin scheduling when short tasks arrive just after a long task, with a time quantum of 1 ms (top) and 100 ms (bottom).</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, we need to pick the time quantum carefully. One consideration is overhead: if we have too short a time quantum, the processor will spend all of its time switching and getting very little useful work done. If we pick too long a time quantum, tasks will have to wait a long time until they get a turn. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1110012"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the behavior of Round Robin, on the same workload as in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1090011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, for two different values for the time quantum. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A good analogy for Round Robin is a particularly hyperkinetic student, studying for multiple finals simultaneously. You won&#8217;t get much done if you read a paragraph from one textbook, then switch to reading a paragraph from the next textbook, and then switch to yet a third textbook. However, if you never switch, you may never get around to studying for some of your courses. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>What is the overhead of a Round Robin time slice?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might think that the cost of switching tasks after a time slice is modest: the cost of interrupting the processor, saving its registers, dispatching the timer interrupt handler, and restoring the registers of the new task. On a modern processor, all these steps can be completed in a few tens of microseconds. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, we must also include the impact of time slices on the efficiency of the processor cache. Each newly scheduled task will need to fetch its data from memory into cache, evicting some of the data that had been stored by the previous task. Exactly how long this takes will depend on the memory hierarchy, the reference pattern of the new task, and whether any of its state is still in the cache from its previous time slice. Modern processors often have multiple levels of cache to improve performance. Reloading just the first level on-chip cache from scratch can take several milliseconds; reloading the second and third level caches takes even longer. Thus, it is typical for operating systems to set their time slice interval to be somewhere between 10 and 100 milliseconds, depending on the goals of the system: better responsiveness or reduced overhead. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One way of viewing Round Robin is as a compromise between FIFO and SJF. At one extreme, if the time quantum is infinite (or at least, longer than the longest task), Round Robin behaves exactly the same as FIFO. Each task runs to completion and then yields the processor to the next in line. At the other extreme, suppose it was possible to switch between tasks with zero overhead, so we could choose a time quantum of a single instruction. With fine-grained time slicing, tasks would finish in the order of length, as with SJF, but slower: a task A will complete within a factor of n of when it would have under SJF, where n is the maximum number of other runnable tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Simultaneous multi-threading</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although zero overhead switching may seem far-fetched, most modern processors do a form of it called <EM>simultaneous multi-threading (SMT)</EM> or <EM>hyperthreading</EM>. With SMT, each processor simulates two (or more) virtual processors, alternating between them on a cycle-by-cycle basis. Since most threads need to wait for memory from time to time, another thread can use the processor during those gaps, or vice versa. In normal operation, neither thread is significantly slowed when running on an SMT. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You can test whether your computer implements SMT by testing how fast the processor operates when it has one or more tasks, each running a tight loop of arithmetic operations. (Note that on a multicore system, you will need to create enough tasks to fill up each of the cores, or physical processors, before the system will begin to use SMT.) With one task per physical processor, each task will run at the maximum rate of the processor. With a two-way SMT and two tasks per processor, each task will run at somewhat less than the maximum rate, but each task will run at approximately the same uniform speed. As you increase the number of tasks beyond the SMT level, however, the operating system will begin to use coarse-grained time slicing, so tasks will progress in spurts &#8212; alternating time on and off the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-1110023 name=x1-1110023></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00418.gif" data-calibre-src="OEBPS/Images/image00418.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.3: </B>Completion times with Round Robin (top) versus FIFO and SJF (bottom) when scheduling equal length tasks.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, Round Robin has some weaknesses. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1110023"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates what happens for FIFO, SJF, and Round Robin when several tasks start at roughly same time and are of the same length. Round Robin will rotate through the tasks, doing a bit of each, finishing them all at roughly the same time. This is nearly the worst possible scheduling policy for this workload! FIFO does much better, picking a task and sticking with it until it finishes. Not only does FIFO reduce average response time for this workload relative to Round Robin, no task is worse off under FIFO &#8212; every task finishes at least as early as it would have under Round Robin. Time slicing added overhead without any benefit. Finally, consider what SJF does on this workload. SJF schedules tasks in exactly the same order as FIFO. The first task that arrives will be assigned the processor, and as soon as it executes a single instruction, it will have less time remaining than all of the other tasks, and so it will run to completion. Since we know SJF is optimal for average response time, this means that both FIFO and Round Robin are optimal for some workloads and pessimal for others, just different ones in each case. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Round Robin and streaming video</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Round Robin is sometimes the best policy even when all tasks are roughly the same size. An example is managing the server bandwidth for streaming video. When streaming, response time is much less of a concern than achieving a predictable, stable rate of progress. For this, Round Robin is nearly ideal: all streams progress at the same rate. As long as Round Robin serves the data as fast or faster than the viewer consumes the video stream, the time to completely download the stream is unimportant. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Depending on the time quantum, Round Robin can also be quite poor when running a mixture of I/O-bound and compute-bound tasks. I/O-bound tasks often need very short periods on the processor in order to compute the next I/O operation to issue. Any delay to be scheduled onto the processor can lead to system-wide slowdowns. For example, in a text editor, it often takes only a few milliseconds to echo a keystroke to the screen, a delay much faster than human perception. However, if we are sharing the processor between a text editor and several other tasks using Round Robin, the editor must wait several time quanta to be scheduled for each keystroke &#8212; with a 100 ms time quantum, this can become annoyingly apparent to the user. </FONT><A id=x1-1110034 name=x1-1110034></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00419.gif" data-calibre-src="OEBPS/Images/image00419.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.4: </B>Scheduling behavior with Round Robin when running a mixture of I/O-bound and compute-bound tasks. The I/O-bound task yields the processor when it does I/O. Even though the I/O completes quickly, the I/O-bound task must wait to be reassigned the processor until the compute-bound tasks both complete their time quanta.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1110034"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates similar behavior with a disk-bound task. Suppose we have a task that computes for 1 ms and then uses the disk for 10 ms, in a loop. Running alone, the task can keep the disk almost completely busy. Suppose we also have two compute bound tasks; again, running by themselves, they can keep the processor busy. What happens when we run the disk-bound and compute-bound tasks at the same time? With Round Robin and a time quantum of 100 ms, the disk-bound task slows down by nearly a factor of twenty &#8212; each time it needs the processor, it must wait nearly 200 ms for its turn. SJF on this workload would perform well &#8212; prioritizing short tasks at the processor keeps the disk-bound task busy, while modestly slowing down the compute-bound tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If you have ever tried to surf the web while doing a large BitTorrent download over a slow link, you can see that network operations visibly slow during the download. This is even though your browser may need to transfer only a very small amount of data to provide good responsiveness. The reason is quite similar. Browser packets get their turn, but only after being queued behind a much larger number of packets for the bulk download. Prioritizing the browser&#8217;s packets would have only a minimal impact on the download speed and a large impact on the perceived responsiveness of the system. </FONT><A id=x1-111004r188 name=x1-111004r188></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.4 </FONT><A id=x1-1120004 name=x1-1120004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Max-Min Fairness</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">In many settings, a fair allocation of resources is as important to the design of a scheduler as responsiveness and low overhead. On a multi-user machine or on a server, we do not want to allow a single user to be able to monopolize the resources of the machine, degrading service for other users. While it might seem that fairness has little value in single-user machines, individual applications are often written by different companies, each with an interest in making their application performance look good even if that comes at a cost of degrading responsiveness for other applications. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another complication arises with whether we should allocate resources fairly among users, applications, processes, or threads. Some applications may run inside a single process, while others may create many processes, and each process may involve multiple threads. Round robin among threads can lead to starvation if applications with only a single thread are competing with applications with hundreds of threads. We can be concerned with fair allocation at any of these levels of granularity: threads within a process, processes for a particular user, users sharing a physical machine. For example, we could be concerned with making sure that every thread within a process makes progress. For simplicity, however, our discussion will assume we are interested in providing fairness among processes &#8212; the same principles apply if the unit receiving resources is the user, application, or thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fairness is easy if all processes are compute-bound: Round Robin will give each process an equal portion of the processor. In practice, however, different processes consume resources at different rates. An I/O-bound process may need only a small portion of the processor, while a compute-bound process is willing to consume all available processor time. What is a fair allocation when there is a diversity of needs? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One possible answer is to say that whatever Round Robin does is fair &#8212; after all, each process gets an equal chance at the processor. As we saw above, however, Round Robin can result in I/O-bound processes running at a much slower rate than they would if they had the processor to themselves, while compute-bound processes are barely affected at all. That hardly seems fair! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While there are many possible definitions of fairness, a particularly useful one is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:max-min fairness"}'>max-min fairness</A></EM>. Max-min fairness iteratively maximizes the minimum allocation given to a particular process (user, application or thread) until all resources are assigned. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If all processes are compute-bound, the behavior of max-min is simple: we maximize the minimum by giving each process exactly the same share of the processor &#8212; that is, by using Round Robin. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The behavior of max-min fairness is more interesting if some processes cannot use their entire share, for example, because they are short-running or I/O-bound. If so, we give those processes their entire request and redistribute the unused portion to the remaining processes. Some of the processes receiving the extra portion may not be able to use their entire revised share, and so we must iterate, redistributing any unused portion. When no remaining requests can be fully satisfied, we divide the remainder equally among all remaining processes. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the example in the previous section. The disk-bound process needed only 10% of the processor to keep busy, but Round Robin only gave it 0.5% of the processor, while each of the two compute-bound processes received nearly 50%. Max-min fairness would assign 10% of the processor to the I/O-bound process, and it would split the remainder equally between the two compute-bound processes, with 45% each. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A hypothetical but completely impractical implementation of max-min would be to give the processor at each instant to whichever process has received the least portion of the processor. In the example above, the disk-bound task would always be scheduled instantly, preempting the compute-bound processes. However, we have already seen why this would not work well. With two equally long tasks, as soon as we execute one instruction in one task, it would have received more resources than the other one, so to preserve &#8220;fairness&#8221; we would need to instantly switch to the next task. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can approximate a max-min fair allocation by relaxing this constraint &#8212; to allow a process to get ahead of its fair allocation by one time quantum. Every time the scheduler needs to make a choice, it chooses the task for the process with the least accumulated time on the processor. If a new process arrives on the queue with much less accumulated time, such as the disk-bound task, it will preempt the process, but otherwise the current process will complete its quantum. Tasks may get up to one time quantum more than their fair share, but over the long term the allocation will even out. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The algorithm we just described was originally defined for network, and not processor, scheduling. If we share a link between a browser request and a long download, we will get reasonable responsiveness for the browser if we have approximately fair allocation &#8212; the browser needs few network packets, and so under max-min its packets will always be scheduled ahead of the packets from the download. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even this approximation, though, can be computationally expensive, since it requires tasks to be maintained on a priority queue. For some server environments, there can be tens or even hundreds of thousands of scheduling decisions to be made every second. To reduce the computational overhead of the scheduler, most commercial operating systems use a somewhat different algorithm, to the same goal, which we describe next. </FONT><A id=x1-112001r192 name=x1-112001r192></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.5 </FONT><A id=x1-1130005 name=x1-1130005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Multi-Level Feedback</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Most commercial operating systems, including Windows, MacOS, and Linux, use a scheduling algorithm called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multi-level feedback queue"}'>multi-level feedback queue (MFQ)</A></EM>. MFQ is designed to achieve several simultaneous goals: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Responsiveness.</B> Run short tasks quickly, as in SJF. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Low Overhead.</B> Minimize the number of preemptions, as in FIFO, and minimize the time spent making scheduling decisions. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Starvation-Freedom.</B> All tasks should make progress, as in Round Robin. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Background Tasks.</B> Defer system maintenance tasks, such as disk defragmentation, so they do not interfere with user work. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Fairness.</B> Assign (non-background) processes approximately their max-min fair share of the processor.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As with any real system that must balance several conflicting goals, MFQ does not perfectly achieve any of these goals. Rather, it is intended to be a reasonable compromise in most real-world cases. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">MFQ is an extension of Round Robin. Instead of only a single queue, MFQ has multiple Round Robin queues, each with a different priority level and time quantum. Tasks at a higher priority level preempt lower priority tasks, while tasks at the same level are scheduled in Round Robin fashion. Further, higher priority levels have <EM>shorter</EM> time quanta than lower levels. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Tasks are moved between priority levels to favor short tasks over long ones. A new task enters at the top priority level. Every time the task uses up its time quantum, it drops a level; every time the task yields the processor because it is waiting on I/O, it stays at the same level (or is bumped up a level); and if the task completes it leaves the system. </FONT><A id=x1-1130015 name=x1-1130015></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00420.gif" data-calibre-src="OEBPS/Images/image00420.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.5: </B>Multi-level Feedback Queue when running a mixture of I/O-bound and compute-bound tasks. New tasks enter at high priority with a short quantum; tasks that use their quantum are reduced in priority.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1130015"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the operation of an MFQ with four levels. A new compute-bound task will start as high priority, but it will quickly exhaust its time quantum and fall to the next lower priority, and then the next. Thus, an I/O-bound task needing only a modest amount of computing will almost always be scheduled quickly, keeping the disk busy. Compute-bound tasks run with a long time quantum to minimize switching overhead while still sharing the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">So far, the algorithm we have described does not achieve starvation freedom or max-min fairness. If there are too many I/O-bound tasks, the compute-bound tasks may receive no time on the processor. To combat this, the MFQ scheduler monitors every process to ensure it is receiving its fair share of the resources. At each level, Linux actually maintains two queues &#8212; tasks whose processes have already reached their fair share are only scheduled if all other processes at that level have also received their fair share. Periodically, any process receiving less than its fair share will have its tasks increased in priority; equally, tasks that receive more than their fair share can be reduced in priority. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Adjusting priority also addresses strategic behavior. From a purely selfish point of view, a task can attempt to keep its priority high by doing a short I/O request immediately before its time quantum expires. Eventually the system will detect this and reduce its priority to its fair-share level. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our previously hapless supermarket manager reads a bit farther into the textbook and realizes that supermarket express lanes are a form of multi-level queue. By limiting express lanes to customers with a few items, the manager can ensure short tasks complete quickly, reducing average response time. The manager can also monitor wait times, adding extra lanes to ensure that everyone is served reasonably quickly. </FONT><A id=x1-113002r193 name=x1-113002r193></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.6 </FONT><A id=x1-1140006 name=x1-1140006></A><FONT style="BACKGROUND-COLOR: #7be1e1">Summary</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">We summarize the lessons from this section: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO is simple and minimizes overhead. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are variable in size, then FIFO can have very poor average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are equal in size, FIFO is optimal in terms of average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Considering only the processor, SJF is optimal in terms of average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">SJF is pessimal in terms of variance in response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are variable in size, Round Robin approximates SJF. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are equal in size, Round Robin will have very poor average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Tasks that intermix processor and I/O benefit from SJF and can do poorly under Round Robin. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Max-min fairness can improve response time for I/O-bound tasks. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Round Robin and Max-min fairness both avoid starvation. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By manipulating the assignment of tasks to priority queues, an MFQ scheduler can achieve a balance between responsiveness, low overhead, and fairness.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the rest of this chapter, we extend these ideas to multiprocessors, energy-constrained environments, real-time settings, and overloaded conditions. </FONT><A id=x1-114001r184 name=x1-114001r184></A></P><A id=x1-1150002 name=x1-1150002>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.2 Multiprocessor Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Today, most general-purpose computers are multiprocessors. Physical constraints in circuit design make it easier to add computational power by adding processors, or cores, onto a single chip, rather than making individual processors faster. Many high-end desktops and servers have multiple processing chips, each with multiple cores, and each core with hyperthreading. Even smartphones have 2-4 processors. This trend is likely to accelerate, with systems of the future having dozens or perhaps hundreds of processors per computer. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This poses two questions for operating system scheduling: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How do we make effective use of multiple cores for running sequential tasks? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How do we adapt scheduling algorithms for parallel applications?</FONT></P></LI></UL><A id=x1-115001r195 name=x1-115001r195></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.2.1 </FONT><A id=x1-1160001 name=x1-1160001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Sequential Applications on Multiprocessors</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a server handling a very large number of web requests. A common software architecture for servers is to allocate a separate thread for each user connection. Each thread consults a shared data structure to see which portions of the requested data are cached, and fetches any missing elements from disk. The thread then spools the result out across the network. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How should the operating system schedule these server threads? Each thread is I/O-bound, repeatedly reading or writing data to disk and the network, and therefore makes many small trips through the processor. Some requests may require more computation; to keep average response time low, we will want to favor short tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A simple approach would be to use a centralized multi-level feedback queue, with a lock to ensure only one processor at a time is reading or modifying the data structure. Each idle processor takes the next task off the MFQ and runs it. As the disk or network finishes requests, threads waiting on I/O are put back on the MFQ and executed by the network processor that becomes idle. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are several potential performance problems with this approach: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Contention for the MFQ lock.</B> Depending on how much computation each thread does before blocking on I/O, the centralized lock may become a bottleneck, particularly as the number of processors increases. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Cache Coherence Overhead.</B> Although only a modest number of instructions are needed for each visit to the MFQ, each processor will need to fetch the current state of the MFQ from the cache of the previous processor to hold the lock. On a single processor, the scheduling data structure is likely to be already loaded into the cache. On a multiprocessor, the data structure will be accessed and modified by different processors in turn, so the most recent version of the data is likely to be cached only by the processor that made the most recent update. Fetching data from a remote cache can take two to three orders of magnitude longer than accessing locally cached data. Since the cache miss delay occurs while holding the MFQ lock, the MFQ lock is held for longer periods and so can become even more of a bottleneck. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Limited Cache Reuse.</B> If threads run on the first available processor, they are likely to be assigned to a different processor each time they are scheduled. This means that any data needed by the thread is unlikely to be cached on that processor. Of course, some of the thread&#8217;s data will have been displaced from the cache during the time it was blocked, but on-chip caches are so large today that much of the thread&#8217;s data will remain cached. Worse, the most recent version of the thread&#8217;s data is likely to be in a remote cache, requiring even more of a slowdown as the remote data is fetched into the local cache.</FONT></P></LI></UL><A id=x1-1160016 name=x1-1160016></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00421.gif" data-calibre-src="OEBPS/Images/image00421.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.6: </B>Per-processor scheduling data structures. Each processor has its own (multi-level) queue of ready threads.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For these reasons, commercial operating systems such as Linux use a <EM>per-processor</EM> data structure: a separate copy of the multi-level feedback queue for each processor. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1160016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this approach. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Each processor uses <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:affinity scheduling"}'>affinity scheduling</A></EM>: once a thread is scheduled on a processor, it is returned to the same processor when it is re-scheduled, maximizing cache reuse. Each processor looks at its own copy of the queue for new work to do; this can mean that some processors can idle while others have work waiting to be done. Rebalancing occurs only if the queue lengths are persistent enough to compensate for the time to reload the cache for the migrated threads. Because rebalancing is possible, the per-processor data structures must still be protected by locks, but in the common case the next processor to use the data will be the last one to have written it, minimizing cache coherence overhead and lock contention. </FONT><A id=x1-116002r197 name=x1-116002r197></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.2.2 </FONT><A id=x1-1170002 name=x1-1170002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Parallel Applications</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A different set of challenges occurs when scheduling parallel applications onto a multiprocessor. There is often a natural decomposition of a parallel application onto a set of processors. For example, an image processing application may divide the image up into equal size chunks, assigning one to each processor. While the application could divide the image into many more chunks than processors, this comes at a cost in efficiency: less cache reuse and more communication to coordinate work at the boundary between each chunk. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If there are multiple applications running at the same time, the application may receive fewer or more processors than it expected or started with. New applications can start up, acquiring processing resources. Other applications may complete, releasing resources. Even without multiple applications, the operating system itself will have system tasks to run from time to time, disrupting the mapping of parallel work onto a fixed number of processors. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1180002 name=x1-1180002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Oblivious Scheduling</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">One might imagine that the scheduling algorithms we have already discussed can take care of these cases. Each thread is time sliced onto the available processors; if two or more applications create more threads in aggregate than processors, multi-level feedback will ensure that each thread makes progress and receives a fair share of the processor. This is often called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:oblivious scheduling"}'>oblivious scheduling</A></EM>, as the operating system scheduler operates without knowledge of the intent of the parallel application &#8212; each thread is scheduled as a completely independent entity. Figure &nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1180017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates oblivious scheduling. </FONT><A id=x1-1180017 name=x1-1180017></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00422.gif" data-calibre-src="OEBPS/Images/image00422.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.7: </B>With oblivious scheduling, threads are time sliced by the multiprocessor operating system, with no attempt to ensure threads from the same process run at the same time.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, several problems can occur with oblivious scheduling on multiprocessors: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bulk synchronous delay.</B> A common design pattern in parallel programs is to split work into roughly equal sized chunks; once all the chunks finish, the processors synchronize at a barrier before communicating their results to the next stage of the computation. This <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:bulk synchronous"}'>bulk synchronous</A></EM> parallelism is easy to manage &#8212; each processor works independently, sharing its results only with the next stage in the computation. Google MapReduce is a widely used bulk synchronous application. </FONT><A id=x1-1180028 name=x1-1180028></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00423.gif" data-calibre-src="OEBPS/Images/image00423.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.8: </B>Bulk synchronous design pattern for a parallel program; each processor computes on local data and waits for every other processor to complete before proceeding to the next step. Preempting one processor can stall all processors until the preempted process is resumed.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1180028"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the problem with bulk synchronous computation under oblivious scheduling. At each step, the computation is limited by the slowest processor to complete that step. If a processor is preempted, its work will be delayed, stalling the remaining processors until the last one is scheduled. Even if one of the waiting processors picks up the preempted task, a single preemption can delay the entire computation by a factor of two, and possibly even more with cache effects. Since the application does not know that a processor was preempted, it cannot adapt its decomposition for the available number of processors, so each step is similarly delayed until the processor is returned. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Producer-consumer delay.</B> Some parallel applications use a producer-consumer design pattern, where the results of one thread are fed to the next thread, and the output of that thread is fed onward, as in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1180039"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Preempting a thread in the middle of a producer-consumer chain can stall all of the processors in the chain. </FONT><A id=x1-1180039 name=x1-1180039></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00424.gif" data-calibre-src="OEBPS/Images/image00424.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.9: </B>Producer-consumer design pattern for a parallel program. Preempting one stage can stall the remainder.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-11800410 name=x1-11800410></A>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00425.gif" data-calibre-src="OEBPS/Images/image00425.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.10: </B>Critical path of a parallel program; delays on the critical path increase execution time.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Critical path delay.</B> More generally, parallel programs have a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:critical path"}'>critical path</A></EM> &#8212; the minimum sequence of steps for the application to compute its result. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11800410"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the critical path for a fork-join parallel program. Work off the critical path can occur in parallel, but its precise scheduling is less important. Preempting a thread on the critical path, however, will slow down the end result. Although the application programmer may know which parts of the computation are on the critical path, with oblivious scheduling, the operating system will not; it will be equally likely to preempt a thread on the critical path as off. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Preemption of lock holder.</B> Many parallel programs use locks and condition variables for synchronizing their parallel execution. Often, to reduce the cost of acquiring locks, parallel programs will use a &#8220;spin-then-wait&#8221; strategy &#8212; if a lock is busy, the waiting thread spin-waits briefly for it to be released, and if the lock is still busy, it blocks and looks for other work to do. This can reduce overhead in the common case that the lock is held for only short periods of time. With oblivious scheduling, however, the lock holder can be preempted &#8212; other tasks will spin-then-wait until the lock holder is re-scheduled, increasing overhead. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>I/O.</B> Many parallel applications do I/O, and this can cause problems if the operating system scheduler is oblivious to the application decomposition into parallel work. If a read or write request blocks in the kernel, the thread blocks as well. To reuse the processor while the thread is waiting, the application program must have created more threads than processors, so that the scheduler can have an extra one to run in place of the blocked thread. However, if the thread does not block (e.g., on a file read when the file is cached in memory), that means that the scheduler has more threads than processors, and so needs to do time slicing to multiplex threads onto processors &#8212; causing all of the problems we have listed above. </FONT></P></LI></UL>
<H5 class=subsubsectionHead><A id=x1-1190002 name=x1-1190002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Gang Scheduling</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">One possible approach to some of these issues is to schedule all of the tasks of a program together. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:gang scheduling"}'>gang scheduling</A></EM>. The application picks some decomposition of work into some number of threads, and those threads run either together or not at all. If the operating system needs to schedule a different application, if there are insufficient idle resources, it preempts all of the processors of an application to make room. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900111"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.11</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates an example of gang scheduling. </FONT><A id=x1-11900111 name=x1-11900111></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00426.gif" data-calibre-src="OEBPS/Images/image00426.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.11: </B>With gang scheduling, threads from the same process are scheduled at exactly the same time, and they are time sliced together to provide a chance for other processes to run.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because of the value of gang scheduling, commercial operating systems, such as Linux, Windows, and MacOS, have mechanisms for dedicating a set of processors to a single application. This is often appropriate on a server dedicated to a single primary use, such as a database needing precise control over thread assignment. The application can <EM>pin</EM> each thread to a specific processor and (with the appropriate permissions) mark it to run with high priority. The system reserves a small subset of the processors to run other applications, multiplexed in the normal way but without interfering with the primary application. </FONT><A id=x1-11900212 name=x1-11900212></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00427.gif" data-calibre-src="OEBPS/Images/image00427.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.12: </B>Performance as a function of the number of processors, for some typical parallel applications. Some applications scale linearly with the number of processors; others achieve diminishing returns.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For multiplexing multiple parallel applications, however, gang scheduling can be inefficient. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900212"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates why. It shows the performance of three example parallel programs as a function of the number of processors assigned to the application. While some applications have perfect speedup and can make efficient use of many processors, other applications reach a point of diminishing returns, and still others have a maximum parallelism. For example, if adding processors does not decrease the time spent on the program&#8217;s critical path, there is no benefit to adding those resources. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An implication of Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900212"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> is that it is usually more efficient to run two parallel programs each with half the number of processors, than to time slice the two programs, each gang scheduled onto all of the processors. Allocating different processors to different tasks is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:space sharing"}'>space sharing</A></EM>, to differentiate it from time sharing, or time slicing &#8212; allocating a single processor among multiple tasks by alternating in time when each is scheduled onto the processor. Space sharing on a multiprocessor is also more efficient in that it minimizes processor context switches: as long as the operating system has not changed the allocation, the processors do not even need to be time sliced. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900313"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates an example of space sharing. </FONT><A id=x1-11900313 name=x1-11900313></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00428.gif" data-calibre-src="OEBPS/Images/image00428.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.13: </B>With space sharing, each process is assigned a subset of the processors.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Space sharing is straightforward if all tasks start and stop at the same time; in that case, we can just allocate evenly. However, the number of available processors is often a dynamic property in a multiprogrammed setting, because tasks start and stop at irregular intervals. How does the application know how many processors to use if the number changes over time? </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1200002 name=x1-1200002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduler Activations</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">A solution, recently added to Windows, is to make the assignment and re-assignment of processors to applications visible to applications. Applications are given an execution context, or <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:scheduler activations"}'>scheduler activation</A></EM>, on each processor assigned to the application; the application is informed explicitly, via an upcall, whenever a processor is added to its allocation or taken away. Blocking on an I/O request also causes an upcall to allow the application to repurpose the processor while the thread is waiting for I/O. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As we noted in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, user-level thread management is possible with scheduler activations. The operating system kernel assigns processors to applications, either evenly or according to some priority weighting. Each application then schedules its user-level threads onto the processors assigned to it, changing its allocation as the number of processors varies due to external events such as other processes starting or stopping. If no other application is running, an application can use all of the processors of the machine; with more contention, the application must remap its work onto a smaller number of processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduler activations defines a <EM>mechanism</EM> for informing an application of its processor allocation, but it leaves open the question of the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multiprocessor scheduling policy"}'>multiprocessor scheduling policy</A></EM>: how many processors should we assign each process? This is an open research question. As we explained in our discussion of uniprocessor scheduling policies, there is a fundamental tradeoff between policies (such as Shortest Job First) that improve average response time and those (such as max-min fairness) that attempt to achieve fair allocation of resources among different applications. In the multiprocessor setting, average response time may be improved by giving extra resources to parallel interactive tasks provided this did not cause long-running compute intensive parallel tasks to starve for resources. </FONT><A id=x1-120001r196 name=x1-120001r196></A></P><A id=x1-1210003 name=x1-1210003>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.3 Energy-Aware Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Another important consideration for processor scheduling is its impact on battery life and energy use. Laptops and smartphones compete on the basis of battery life, and even for servers, energy usage is a large fraction of the overall system cost. Choices that the operating system makes can have a large effect on these issues. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might think that processor scheduling has little role to play with respect to system energy usage. After all, each application has a certain amount of computing that needs to be done, computing that requires energy whether we are running on a direct power line or off of a battery. Of course, the operating system should delay background or system maintenance tasks (such as software upgrades) for when the system is connected to power, but this is likely to be a relatively minor effect on the overall power budget. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In part because of the importance of battery life to computer users, modern architectures have developed a number of ways of trading reduced computation speed for lower energy use. In other words, the mental model of each computation taking a fixed amount of energy is no longer accurate. There is quite a bit of flux in the types of hardware support available on different systems, and systems five years from now are likely to make very different tradeoffs than those in place today. Thus, our goal in this section is not to provide a set of widely used algorithms for managing power, but rather to outline the design issues energy management poses for the operating system. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several power optimizations are possible, provided hardware support: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Processor design.</B> There can be several orders of magnitude difference between one processor design and another with respect to power consumption. Often, making a processor faster requires extra circuitry, such as out of order execution, that itself consumes power; low power processors are slower and simpler. Likewise, processors designed for lower clock speeds can tolerate lower voltage swings at the circuit level, reducing power consumption dramatically. Some systems have begun to put this tradeoff under the control of the operating system, by including both a high power, high performance multiprocessor and a low power, lower performance uniprocessor on the same chip. High power is appropriate when response time is at a premium and low power when power consumption is more important. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Processor usage.</B> For systems with multiple processor chips, or multiple cores on a single chip, lightly used processors can be disabled to save power. Processors will typically draw much less power when they are completely idle, but as we mentioned above, many parallel programs achieve some benefit from using extra processors, yet also reach a point of diminishing returns. Thus, there is a tradeoff between somewhat faster execution (e.g., by using all available resources) and lower energy use (e.g., by turning off some processors even when using them would slightly decrease response time). </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>I/O device power.</B> Devices not in use can be powered off. Although this is most obvious in terms of the display, devices such as the WiFi or cellphone network interface also consume large amounts of power. Power-constrained embedded systems such as sensors will turn on their network interface hardware periodically to send or receive data, and then go back to quiescence. For this to work, the senders and receivers need to synchronize their periods of transmission, or the hardware needs to have a low power listening mode. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Heat dissipation</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A closely related topic to energy use is heat dissipation. In laptop computers, you can save weight by not including a fan to cool the processor. However, a modern multicore chip will consume up to 150 Watts, or more than a very bright incandescent light bulb. Just as with a light bulb, the heat generated has to go somewhere. Making things significantly more complicated, the processor will also break permanently if it runs at too high a temperature. Thus, the operating system increasingly must monitor and manage the temperature of the processor to ensure it stays within its operating region. Much like a cheetah, portable computers are now capable of running at very fast speeds for short periods of time, before they need to take a break to cool down. Or they can amble at much slower speeds for a longer period of time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The laptop one of us used to write this book illustrates this. Formatting this textbook takes about a minute when the computer is cold, but the same formatting request will stall in the middle of the build for several minutes if run immediately after a previous build request. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">At times, different power optimizations interact in subtle ways. For example, running application code quickly can sometimes improve power efficiency, by enabling the network interface hardware to be turned off more quickly once the application finishes. Because context switching consumes both time and energy to reload processor caches, affinity scheduling improves both performance and energy efficiency. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In most cases, however, there is a tradeoff: how should the operating system balance between competing demands for timeliness and energy efficiency? If the user has requested maximum responsiveness or maximum battery life, the choice is easy, but often the user wants a reasonable tradeoff between the two. </FONT><A id=x1-12100114 name=x1-12100114></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00429.gif" data-calibre-src="OEBPS/Images/image00429.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.14: </B>Example relationship between response time and user-perceived value. For most applications, faster response time is valuable within a range. Below some threshold, users will not be able to perceive the difference. Above some threshold, users will perform other activities while waiting for the result.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One approach would be to consider the value that the user places on fast response time for a particular application: quickly updating the display after a user interface command is probably more important than transferring files quickly in the background. We can capture the relationship between response time and value in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12100114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Although the precise shape and magnitude will vary from user to user and application to application, the curve will head down and to the right &#8212; the longer something takes, the less useful it is. Often, the curve is S-shaped. Human perception is unable to tell the difference between a few tens of milliseconds, so adding a short delay will not matter that much for most tasks; likewise, if a protein folding computation has already taken a few minutes, it won&#8217;t matter much if it takes a few more seconds. Not everything will be S-shaped: in high frequency stock trading, value starts high and plummets to zero within a few milliseconds. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Response time predictability affects this relationship as well. An online video that cuts out for a few seconds every minute is much less watchable than one that is lower quality on average but more predictable. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If we combine Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12100114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> with the fact that increased energy use often provides diminishing returns in terms of improved performance, this suggests a three prong strategy to spend the system&#8217;s energy budget where it will make the most difference: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Below the threshold of human perception.</B> Optimize for energy use, to the extent that tasks can be executed with greater energy efficiency without the user noticing. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Above the threshold of human perception.</B> Optimize for response time if the user will notice any slowdown. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Long-running or background tasks.</B> Balance energy use and responsiveness depending on the available battery resources. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Battery life and the kernel-user boundary</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An emerging issue on smartphones is that application behavior can have a significant impact on battery life, e.g., by more intensive use of the network or other power-hungry features of the architecture. If a user runs a mix of applications, how can she know which was most responsible for their smartphone running out of power? Among the resources we will discuss in this book, energy is almost unique in being a <EM>non-virtualizable</EM> resource. When an application drains the battery, the energy lost is no longer available to any other applications. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How can we prevent a misbehaving or greedy application from using more than its share of the battery? One model is to let the user decide: for the kernel to measure and record how much energy was used by each application, so the user can determine if each application is worth it. Apple has taken a different approach with the iPhone. Because Apple controls which applications can run on the system, it can and has barred applications that (in its view) unnecessarily drain the battery. It will be interesting to see which of these models wins out over time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-121002r210 name=x1-121002r210></A><A id=x1-1220004 name=x1-1220004>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.4 Real-Time Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">On some systems, the operating system scheduler must account for process deadlines. For example, the sensor and control software to manage an airplane&#8217;s flight path must be executed in a timely fashion, if it is to be useful at all. Similarly, the software to control anti-lock brakes or anti-skid traction control on an automobile must occur at a precise time if it is to be effective. In a less life critical domain, when playing a movie on a computer, the next frame must be rendered in time or the user will perceive the video quality as poor. </FONT><A id=x1-12200115 name=x1-12200115></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00430.gif" data-calibre-src="OEBPS/Images/image00430.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.15: </B>With real-time constraints, the value of completing some task drops to zero if the deadline is not met.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These systems have <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:real-time constraint"}'>real-time constraints</A></EM>: computation that must be completed by a deadline if it is to have value. Real-time constraints are a special case of Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12100114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12200115"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.15</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, where the value of completing a task is uniform up to the deadline, and then drops to zero. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How do we design a scheduler to ensure deadlines are met? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We might start by assigning real-time tasks a higher priority than any less time critical tasks. We could then run the system under a variety of different of different workloads, and see if the system continues to comfortably meet its deadlines in all cases. If not, the system may need a faster processor or other hardware resources to speed up the real-time tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, testing alone is insufficient for guaranteeing real-time constraints. Recall that the specific ordering of execution events can sometimes lead to different execution sequences &#8212; e.g., sometimes a thread will need to wait for a lock held another thread, and other times the lock will be FREE. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One option is that, instead of threads, we should use a completely deterministic and repeatable schedule that ensures that the deadlines are met. This can work if the real-time tasks are periodic and fixed in advance. However, in dynamic systems, it is difficult to account for all possible variations affecting how long different parts of the computation will take. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are three widely used techniques for increasing the likelihood that threads meet their deadlines. These approaches are also useful whenever timeliness matters without a strict deadline, e.g., to ensure responsiveness of a user interface. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Over-provisioning.</B> A simple step is to ensure that the real-time tasks, in aggregate, use only a fraction of the system&#8217;s processing power. This way, the real-time tasks will be scheduled quickly, without having to wait for higher-priority, compute-intensive tasks. The equivalent step in college is to avoid signing up for too many hard courses in the same semester! </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Earliest deadline first.</B> Careful choice of the scheduling policy can also help meet deadlines. If you have a pile of homework to do, neither shortest job first nor round robin will ensure that the assignment due tomorrow gets done in time. Instead, real-time schedulers, mimicking real life, use a policy called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:earliest deadline first"}'>earliest deadline first</A></EM> (EDF). EDF sorts tasks by their deadline and performs them in that order. If it is possible to schedule the required work to meet their deadlines, and the tasks only need the processor (and not I/O, locks or other resources), EDF will ensure that all tasks are done in time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For complex tasks, however, EDF can produce anomalous behavior. Consider two tasks. Task A is I/O-bound with a deadline at 12 ms, needing 1 ms of computation followed by 10 ms of I/O. Task B is compute-bound with a deadline at 10 ms, but needing 5 ms of computation. Although there is a schedule that will meet both deadlines (run task A first), EDF will run the compute-bound task first, causing the I/O-bound task to miss its deadline. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This limitation can be addressed by breaking tasks into shorter units, each with its own deadline. In the example, the true deadline for the compute portion of the I/O-bound task is at 2 ms, because if it is not completed by then, the overall task deadline will be missed. If your homework next week needs a book from the library, you need to put that on hold first, even if that slightly delays the homework you have due tomorrow. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Priority donation.</B> Another problem can occur through the interaction of shared data structures, priorities, and deadlines. Suppose we have three tasks, each with a different priority level. The real-time task runs at the highest priority, and it has sufficient processing resources to meet its deadline, with some time to spare. However, the three tasks also access a shared data structure, protected by a lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose the low priority acquires the lock to modify the data structure, but it is then preempted by the medium priority task. The relative priorities imply that we should run the medium priority task first, even though the low priority task is in the middle of a critical section. Next, suppose the real-time task preempts the medium task and proceeds to access the shared data structure. It will find the lock busy and wait. Normally, the wait would be short, and the real-time task would be able to meet its deadline despite the delay. However, in this case, when the high priority task waits for the lock, the scheduler will pick the medium priority task to run next, causing an indefinite delay. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:priority inversion"}'>priority inversion</A></EM>; it can occur whenever a high priority task must wait for a lower priority task to complete its work. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A commonly used solution, implemented in most commercial operating systems, is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:priority donation"}'>priority donation</A></EM>: when a high priority task waits on a shared lock, it temporarily donates its priority to the task holding the lock. This allows the low priority task to be scheduled to complete the critical section, at which point its priority reverts to its original state, and the processor is re-assigned to the high priority, waiting, task. </FONT></P></LI></UL><A id=x1-122002r212 name=x1-122002r212></A><A id=x1-1230005 name=x1-1230005>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5 Queueing Theory</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you build a new web service, and the week before you are to take it live, you test it to see whether it will have reasonable response time. If your tests show that the performance is terrible, what then? Is it because the implementation is too slow? Perhaps you have the wrong scheduler? Quick, let&#8217;s re-implement that linked list with a hash table! And add more levels to the multi-level feedback queue! Our advice: don&#8217;t panic. In this section, we consider a third possibility, an effect that often trumps all of the others: response time depends non-linearly on the rate that tasks arrive at a system. Understanding this relationship is the topic of <EM>queueing theory</EM>. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, if you have ever waited in line (and who hasn&#8217;t?), you have an intuitive understanding of queueing theory. Its concepts apply whenever there is a queue waiting for a turn, whether it is tasks waiting for a processor, web requests waiting for a turn at a web server, restaurant patrons waiting for a table, cars waiting at a busy intersection, or people waiting in line at the supermarket. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While queueing theory is capable of providing precise predictions for complex systems, our interest is providing you the tools to be able to do back of the envelope calculations for where the time goes in a real system. For performance debugging, coarse estimates are often enough. For this reason, we make two simplifying assumptions for this discussion. First, we assume the system is work-conserving, so that all tasks that arrive are eventually serviced; this will normally be the case except in extreme overload conditions, a topic we will discuss in the next section of this chapter. Second, although the scheduling policy can affect a system&#8217;s queueing behavior, we will keep things simple and assume FIFO scheduling. </FONT><A id=x1-123001r199 name=x1-123001r199></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.1 </FONT><A id=x1-1240001 name=x1-1240001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Definitions</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Because queueing theory is concerned with the root causes of system performance, and not just its observable effects, we need to introduce a bit more terminology. A simple abstract queueing system is illustrated by Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12400116"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. In any queueing system, tasks arrive, wait their turn, get service, and leave. If tasks arrive faster than they can be serviced, the queue grows. The queue shrinks when the service rate exceeds the arrival rate. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To begin, we will consider single-queue, single-server, work-conserving systems. Later, we will introduce more complexity such as multiple queues, multiple servers, and finite queues that can discard some requests. </FONT><A id=x1-12400116 name=x1-12400116></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00431.gif" data-calibre-src="OEBPS/Images/image00431.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.16: </B>An abstract queueing system. Tasks arrive, wait their turn in the queue, get service, and leave.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Server.</B> A server is anything that performs tasks. A web server is obviously a server, performing web requests, but so is the processor on a client machine, since it executes application tasks. The cashier at a supermarket and a waiter in a restaurant are also servers. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Queueing delay (W) and number of tasks queued (Q).</B> The queueing delay, or wait time, is the total time a task must wait to be scheduled. In a time slicing system, a task might need to wait multiple times for the same server to complete its task; in this case the queueing delay includes all of the time a task spends waiting until it is completed. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Service time (S).</B> The service time S, or execution time, is the time to complete a task assuming no waiting. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Response time (R).</B> The response time is the queueing delay (how long you wait in line) plus the service time (how long it takes once you get to the front of the line). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; W + S </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the web server example we started with, the poor performance can be due to either factor &#8212; the system could be too slow even when no one is waiting, or the system could be too slow because each request spends most of its time waiting for service. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can improve the response time by improving either factor. We can reduce the queueing delay by buying more servers (for example, by having more processors than ready threads or more cashiers than customers), and we can reduce service time by buying a faster server or by engineering a faster implementation. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Arrival rate (&#955;) and arrival process.</B> The arrival rate &#955; is the average rate at which new tasks arrive. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">More generally, the arrival process describes when tasks arrive including both the average arrival rate and the pattern of those arrivals such as whether arrivals are bursty or spread evenly over time. As we will see, burstiness can have a large impact on queueing behavior. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Service rate (&#956;).</B> The service rate &#956; is the number of tasks the server can complete per unit of time when there is work to do. Notice that the service rate &#956; is the inverse of the service time S. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Utilization (U).</B> The utilization is the fraction of time the server is busy (0 &#8804; U &#8804; 1). In a work-conserving system, utilization is determined by the ratio of the average arrival rate to the service rate: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">U </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#955; / &#956; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if &#955; &lt; &#956; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if &#955; &#8805; &#956;) </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice that if &#955; &gt; &#956;, tasks arrive more quickly than they can be serviced. Such an overload condition is unstable; in a work-conserving system, the queue length and queueing delay grow without bound. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Throughput (X).</B> Throughput is the number of tasks processed by the system per unit of time. When the system is busy, the server processes tasks at the rate of &#956;, so we have: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">X </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; U &#956; </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Combining this equation with the previous one, we can see that when the average arrival rate &#955; is less than the service rate &#956;, the system throughput matches the arrival rate. We can also see that the throughput can never exceed &#956; no matter how quickly tasks arrive. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">X </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#955; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if U &lt; 1 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#956; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if U = 1 </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Number of tasks in the system (N).</B> The average number of tasks in the system is just the number queued plus the number receiving service: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; Q + U </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR></LI></UL><A id=x1-124002r215 name=x1-124002r215></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.2 </FONT><A id=x1-1250002 name=x1-1250002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Little&#8217;s Law</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Little"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Little&#8217;s Law</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> is a theorem proved by John Little in 1961 that applies to any <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:stable system"}'>stable system</A></EM> where the arrival rate matches the departure rate. It defines a very general relationship between the average throughput, response time, and the number of tasks in the system: </FONT>
<P><BR></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although this relationship is simple and intuitive, it is powerful because the &#8220;system&#8221; can be anything with arriving and departing tasks, provided the system is stable &#8212; regardless of the arrival process, number of servers, or queueing order. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose we have a queueing system like the one shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12400116"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and we observe over the course of an hour that an average of 100 requests arrive and depart each second and that the average request is completed 50 ms after it arrives. On average, how many requests are being handled by the system? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Since the arrival rate matches the departure rate, the system is stable and we can use Little&#8217;s Law. We have a throughput X = 100 requests/second and a response time R = 50 ms = 0.05 seconds: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 100 requests/second &#215; 0.05 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>5 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this system there are, on average, 5 requests waiting in the queue or being served. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can also zoom in to see what is happening at the server, ignoring the queue. The server itself is a system, and Little&#8217;s Law applies there, too. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose we have a server that processes one request at a time and we observe that an average of 100 requests arrive and depart each second and that the average request completes 5 ms after it arrives. What is the average utilization of the server? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The utilization of the server is the fraction of time the server is busy processing a request. Because the server handles one request at a time, its utilization equals the average number of requests in the server-only system. Using Little&#8217;s Law: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">U </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 100 requests/second &#215; 0.005 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>0.5 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR><FONT style="BACKGROUND-COLOR: #7be1e1">The average utilization is 0.5 or 50%. &#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can also look at the subsystem comprising just the queue. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>For the system described in the previous two examples, how long does an average request spend in the queue, and on average how many requests are in the queue? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>We know that an average task takes 50 ms to get through the queue and server and that it spends 5 ms at the server, so it must spend 45 ms in the queue. Similarly, we know that on average the system holds 5 tasks with 0.5 of them in the server, so the average queue length is 4.5 tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can get the same result with Little&#8217;s Law. One hundred tasks pass through the queue per second and spend an average of 45 ms in the queue, so the average number of tasks in the queue is: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 100 requests/second &#215; 0.045 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>4.5 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although Little&#8217;s Law is useful, remember that it only provides information about the system&#8217;s averages over time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>One thing might puzzle you. In the previous example, if the average number of tasks in the queue is 4.5 and processing a request takes 5 ms, how can the average queueing delay for a request be 45 ms rather than 4.5 &#215; 5 ms = 22.5 ms? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The <EM>average</EM> number of requests in the queue is 4.5. Sometimes there are more; sometimes there are fewer. Queues will grow during bursts of arrivals, and they will shrink when tasks are arriving slowly. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In fact, from the 0.5 server utilization rate calculated above, we know that the queue is empty half the time. To make up for the empty periods, there <EM>must</EM> be periods with longer-than-average queue lengths. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, the queues tend to be full during busy periods and they tend to be empty during idle periods, so relatively few requests enjoy short or empty queues and relatively many suffer long queues. So, the average request sees a longer queue than the average queue length over time might suggest, and the (per-request) average queueing delay exceeds the (time) average queue length times the (per-request) average service time. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Not only can we apply Little&#8217;s Law to a simple queueing system or its subcomponents, we can apply it to more complex systems, even those whose internal structure we do not fully understand. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose there is a complex web service like Google, Facebook, or Amazon, and we know that the average request takes 100 milliseconds and that the service handles an average of 10,000 queries per second. How many requests are pending in the system on average? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Applying Little&#8217;s Law: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 10000 requests/second &#215; 0.1 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>1000 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Note that this is true regardless of the internal structure of the web service. It may have many load balancers, processors, network switches, and databases, each with separate queues, and each with different queueing policies, but in aggregate in steady state the number of requests being handled must be equal to the product of the response time and the throughput. &#9633; </FONT><A id=x1-125001r217 name=x1-125001r217></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.3 </FONT><A id=x1-1260003 name=x1-1260003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Response Time Versus Utilization</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Because having more servers (whether processors on chip or cashiers in a supermarket) or faster servers is costly, you might think that the goal of the system designer is to maximize utilization. However, in most cases, there is no free lunch: as we will see, higher utilization normally implies higher queueing delay and higher response times. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Operating a system at high utilization also increases the risk of overload. Suppose you plan to minimize costs by operating a web site at 95% utilization, but your service turns out to be a little more popular than you expected. You can quickly find yourself operating in the unstable regime where requests are arriving faster than you can service them (&#955; &gt; &#956;) and where your queues and waiting times are growing without bound. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As a designer, you need to find an appropriate tradeoff between higher utilization and better response time. Fifty years ago, computer designers made the tradeoff in favor of higher utilization: when computers are wildly expensive, it is annoying but understandable to make people wait for the computer. Now that computers are much cheaper, our lives are better! We now usually make the computer do the waiting. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can predict a queueing system&#8217;s average response time from its arrival process and service time, but the relationship is more complex than the relationships discussed so far. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To provide intuition, we start with some extreme scenarios that bound the behavior of a queueing system; we will introduce more realistic scenarios as we proceed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Broadly speaking, higher arrival rates and burstier arrival patterns tend to yield longer queue lengths and response times than lower arrival rates and smoother arrival patterns. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Best case: Evenly spaced arrivals.</B> Suppose we have a set of fixed-sized tasks that arrive equally spaced from one another. For As long as the rate at which tasks arrive is less than the rate at which the server completes the tasks, there will be no queueing at all. Perfection! Each server finishes the previous customer in time for the next arrival. </FONT><A id=x1-12600117 name=x1-12600117></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00432.gif" data-calibre-src="OEBPS/Images/image00432.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.17: </B>Best case response time and throughput as a function of the task arrival rate relative to the service rate. These graphs assume arrivals are evenly spaced and service times are fixed-size.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the relationship between arrival rate and response time for this best case scenario of evenly spaced arrivals. There are three regimes: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>&#955; &lt; &#956;.</B> If the arrival rate is below the service rate, there is no queueing and the response time equals the service time.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, suppose we have a server that can handle 1000 requests per second, and one request arrives every 1000, 100, or 10 milliseconds. The server finishes processing request i- 1 before request i arrives, and request i completes 1 ms after it arrives, clearing the way for request i + 1. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The situation remains the same if arrivals are more closely spaced at 1.1, 1.01, 1.001, and so on down to 1.0 ms, where each request arrives at the moment the previous request completes. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>&#955; = &#956;.</B> If the arrival rate matches the service rate, the system is in a precarious equilibrium. If the queues are initially empty, they will stay empty, but if the queues are initially full, they will remain full.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose arrivals are coming every 1.0 ms, and at some point during the day a single extra request arrives; that request must wait until the previous one completes, but the server will then be busy when the next request arrives. That single extra request produces queueing delay for every subsequent request. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>&#955; &gt; &#956;.</B> If the arrival rate exceeds the service rate, queues will grow without bound. In this case, the system is not in equilibrium, and the steady state response time is undefined.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose the task arrival rate is one per 0.999 ms so that tasks arrive slightly faster than they can be processed? If a system&#8217;s arrival rate exceeds its service rate, then under our simple model its queues will grow without bound, and its queueing delay is undefined. In practice, memory is finite; once the queue&#8217;s capacity is reached, the system must discard some of the arriving requests. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> also shows the relationship between arrival rate and throughput. When the arrival rate is less than the service rate, increasing the arrival rate increases throughput. Once the arrival rate matches or exceeds the service rate, faster arrivals just grow the queues more quickly, they do not increase useful throughput. </FONT><A id=x1-12600218 name=x1-12600218></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00433.gif" data-calibre-src="OEBPS/Images/image00433.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.18: </B>Response time for a server that can handle 10 requests per second as we vary arrival rate of fixed-size tasks in two scenarios: evenly spaced arrivals and bursty arrivals where all of a second&#8217;s requests arrive in a group at the start of the second.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Worst case: Bursty arrivals.</B> Now consider the opposite case. Suppose a group of tasks arrive at exactly the same time. The average wait time increases linearly as more tasks arrive together &#8212; one task in a group can be serviced right away, but others must wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600218"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> considers a hypothetical server with a maximum throughput of 10 tasks per second as we vary the number of tasks that arrive per second. The graph shows two cases: one where requests are evenly spaced as in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and the other where requests arrive in a burst at the start of each second. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even when the request rate is below the server&#8217;s service rate, bursty arrivals suffer queueing delays. For example, when five requests arrive as a group at the start of each second, the first request is served immediately and finishes 0.1 seconds later. The server can then start processing the second request, finishing it 0.2 seconds after the start of the interval. The third, fourth, and fifth requests finish at 0.3, 0.4, and 0.5 seconds after the start of the second, giving an average response time of (0.1 + 0.2 + 0.3 + 0.4 + 0.5)/5 = 0.3 seconds. By the same logic, if ten requests arrive as a group, the average response time is (0.1 + 0.2 + 0.3 + 0.4 + 0.5 + 0.6 + 0.7 + 0.8 + 0.9 + 1.0)/10 = 0.55 seconds. If the same requests had arrived evenly spaced, their average response time would have been over five times better! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Exponential arrivals.</B> Most systems are somewhere in between this best case and worst case. Rather than being perfectly synchronized or perfectly desynchronized, task arrivals in many systems are random. For example, different customers in a supermarket do not coordinate with each other as to when they arrive. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Likewise, service times are not perfectly equal &#8212; there is randomness there as well. At a doctor&#8217;s office, everyone has an appointment, so it may seem like that should be the best case scenario, and no one should ever have to wait. Even so, there is often queueing! Why? If the amount of time the doctor takes with each patient is sometimes shorter and sometimes longer than the appointment length, then random chance will cause queueing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A particularly useful model for understanding queueing behavior is to use an exponential distribution to describe the time between tasks arriving and the time it takes to service each task. Once you get past a bit of math, the exponential provides a stunningly simple <EM>approximate</EM> description of most real-life queueing systems. We do not claim that all real systems always obey the exponential model in detail; in fact, most do not. However, the model is often accurate enough to provide insight on system behaviors, and as we will discuss, it is easy to understand the circumstances under which it is inaccurate. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Model vs. reality</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When trying to understand a complex system, it is often useful to construct a model of its behavior. A model is a simplification that tries to capture the most important aspects of a more complex system&#8217;s behavior. Models are neither true nor false, but they can be useful or not for a particular purpose. It is often the case that a more complex model will yield a closer approximation; whether the added complexity is useful or gets in the way depends on how the model is being used. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We often find it useful to use simple workload models when debugging early system implementations. Using the types of analysis described in this chapter and an understanding of the system being built, it is usually possible to predict how the system should behave under simple workloads. If measured behavior deviates from these predictions, there is a bug in our understanding or implementation of the system. Simple workloads can help us improve our understanding if it is the former and track down the bug if it is the latter. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We could, instead, evaluate early implementations by feeding them more realistic workloads. For example, if we are building a new web server, we could feed it a workload trace captured at some other server. However, this approach is often more complex. For example, to test our system under a range of conditions, we need to gather a range of traces &#8212; some with low load, some with high; some with bursty loads, some with smooth; etc. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Worst, even though this approach is more complex, it may yield less insight because it is harder to predict the expected system behavior. If we run a simulation with a trace and get worse performance than we expected, is it because we do not understand our system or because we do not understand the trace? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This is not to suggest that simple models are always superior to more complex, more realistic ones. Once we are satisfied with our new system&#8217;s behavior for workloads we understand, we should test it for workloads we do not understand or control. There may be (and probably are) important behaviors not captured in our simple models. We might find, for example, that bursts of interest in particular topics create &#8220;hot spots&#8221; of load that we did not anticipate. Evaluation under more realistic models might make us realize that we need to implement more aggressive caching of recently popular pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Selecting the right model for system evaluation is a delicate balance between complexity and accuracy. If after abstracting away detail, we can still provide approximately correct predictions of system behavior under a variety of scenarios, then it is likely the model captures the most important aspects of the system. If the model is inaccurate in some important respect, then it means our explanation for how the system behaves is too coarse, and to improve the prediction we need to revise the model. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-12600319 name=x1-12600319></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00434.gif" data-calibre-src="OEBPS/Images/image00434.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.19: </B>Exponential probability distribution.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">First, the math. An <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:exponential distribution"}'>exponential distribution</A></EM> of a continuous random variable with a mean of 1 / &#955; has the probability density function, shown in Figure </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600319"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.19</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">f(x) </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#955;e<SUP>-&#955;x</SUP> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, you need not understand that equation in any detail, except for the following. A useful property of an exponential distribution is that it is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:memoryless property"}'>memoryless</A></EM>. A memoryless distribution for the time between two events means that the likelihood of an event occurring remains the same, <EM>no matter how long we have already waited</EM> for the event, or what other events may have already happened. For example, on a web server, web requests from different users (usually) arrive independently. Sometimes, two requests will arrive close together in time; sometimes there will be more of a delay. For example, suppose a web server receives a request from a new user on average every 10 ms. If you want to predict how long until the next request arrives, it probably does not matter when the <EM>last</EM> request arrived: 0, 1, 5, or 50 ms ago. The expected time to the next request is still probably about 10 ms. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Not every distribution is memoryless. A Gaussian, or normal, distribution for the time between events is closer to the best case scenario described above &#8212; arrivals occur randomly, but they tend to occur at regular intervals, give or take a bit. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some probability distributions work the other way. With a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:heavy-tailed distribution"}'>heavy-tailed distribution</A></EM>, the longer you have waited for some event, the longer you are likely to still need to wait. This is closer to the worst case behavior above, as it means that most events are clustered together. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, a ticket seller&#8217;s web site might see bursty workloads. For long periods of time the site might see little traffic, but when tickets for a popular concert of sporting event go on sale, the traffic may be overwhelming. Here, external factors introduce synchronization across different users&#8217; activities so that requests from different users do not arrive independently. Such a workload is unlikely to be memoryless; if you look at a ticket seller&#8217;s web site at a random moment and see that it has been a long time since the last request arrived, you probably arrived during a lull, and you can predict that it will likely be a long time until the next request arrives. On the other hand, if the last request just arrived, you probably arrived during a burst, and the next request will arrive soon. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">With a memoryless distribution, the behavior of queueing systems becomes simple to understand. One can think of the queue as a finite state machine: with some probability, a new task arrives, increasing the queue by one. If the queue length is non-zero, with some other probability, a task completes, decreasing the queue by one. With a memoryless distribution of arrivals and departures, the probability of each transition is constant and independent of the other transitions, as illustrated in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600420"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.20</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-12600420 name=x1-12600420></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00435.gif" data-calibre-src="OEBPS/Images/image00435.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.20: </B>State machine representing a queue with exponentially distributed arrivals and departures. &#955; is the rate of arrivals; &#956; is the rate at which the server completes each task. With an exponential distribution, the probability of a state transition is independent of how long the system has been in any given state.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming that &#955; &lt; &#956;, the system is stable Assuming stability and exponential distributions for the arrival and departure processes, we can solve the model to determine the average response time R as a function of the utilization U and service time S: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - U) </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Recall that the utilization, the fraction of time that the server is busy, is simply the ratio between &#955; and &#956;. </FONT><A id=x1-12600521 name=x1-12600521></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00436.gif" data-calibre-src="OEBPS/Images/image00436.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.21: </B>Relationship between response time and utilization, assuming exponentially distributed arrivals and departures. Average response time goes to infinity as the system approaches full utilization.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This equation is graphed in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600521"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. When utilization is low, there is little queueing delay and response time is close to the service time. Furthermore, when utilization is low, small increases in the arrival rate result in small increases in queueing delay and response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As utilization increases, queueing and response time also increase, and the relationship is non-linear. At high utilizations, the queueing delay is high, and small increases in the arrival rate can drastically increase queueing delay and response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose a queueing system with exponentially distributed arrivals and task sizes is 20% utilized and the load increases by 5%, by how much does the response time increase? How does that increase compare to the case when utilization goes from 90% to 95%? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>At 20% utilization, the response time is: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - U) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - 0.2) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1.25 S </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">At 25% utilization, the response time is: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - U) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - 0.25) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1.33 S </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The 5% increase in load increases response time by about 8%.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using the same equation, at 90% utilization we have R = 10S and at 95% we have R = 20S, <B>the 5% increase in load increases response time by a factor of two.</B> &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The response time of a system becomes unbounded as the system approaches full utilization. Although it might seem that full utilization is an achievable goal, if there is any randomness in arrivals <EM>or</EM> any randomness in service times, full utilization cannot be achieved in steady state without making some tasks wait unbounded amounts of time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In most systems, well before a system reaches full utilization, average response time will become unbearably long. In the next section, we discuss some of the steps system designers can take in response to overload. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Variance in the response time increases even faster as the system approaches full utilization, proportional to 1 / (1 - U)<SUP>2</SUP>. Even with 99% utilization, 1% of the time there is no queue at all; random chance means that while sometimes a large number of customers arrive at nearly the same time, at other times the server will be able to work through all of the backlog. If you are lucky enough to arrive at just that moment, you can receive service without waiting. If you are unlucky enough to arrive immediately after a burst of other customers, your wait will be quite long. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Exponential arrivals are burstier than the evenly spaced ones we considered in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and less bursty than the ones we considered in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600218"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. The response time line for the exponential arrivals is higher than the one for evenly spaced arrivals, which was flat across the entire stable range form U = 0 to U = 1, and the line is lower than the one for more bursty arrivals, which rose rapidly even when utilization was low. In general burstier arrivals will produce worse response time for a given level of load. </FONT><A id=x1-126006r218 name=x1-126006r218></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.4 </FONT><A id=x1-1270004 name=x1-1270004></A><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;What if?&#8221; Questions</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Queueing theory is particularly useful for answering &#8220;what if?&#8221; questions: what happens if we change some design parameter of the system. In this section, we consider a selection of these questions, as a way of providing you a bit more intuition. </FONT>
<H5 class=subsubsectionHead><A id=x1-1280004 name=x1-1280004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Policy</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">What happens to the response time curve for other scheduling policies? It depends on the burstiness and predictability of the workload. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the distribution of arrivals or service times is less bursty than an exponential (e.g., evenly spaced or Gaussian), FIFO will deliver nearly optimal response times, while Round Robin will perform worse than FIFO. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If task service times are exponentially distributed but individual task times are unpredictable, the average response time is the exactly the same for Round Robin as for FIFO. With a memoryless distribution, every queued task has the same expected remaining service time, so switching among tasks has no impact other than to increase overhead. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On the other hand, if task lengths can be predicted and there is variability of service times, Shortest Job First can improve average response time, particularly if arrivals are bursty. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Many real-world systems exhibit more bursty arrivals or service times than an exponential distribution. A bursty distribution is sometimes called <EM>heavy-tailed</EM> because it has more very long tasks; since the mean rate is the same, this also implies that the distribution has even more very short tasks. For example, web page size is heavy-tailed; so is the processing time per web page. Process execution times on desktop computers are also heavy-tailed. For these types of systems, burstiness results in worse average response time than would be predicted by an exponential distribution. That said, for these types of systems, there is an even greater benefit to approximating SJF to avoid stalling small requests behind long ones, and Round Robin will outperform FIFO. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using SJF (or an approximation) to improve average response time comes at a cost of an increase in response time for long tasks. At low utilization, this increase is small, but at high utilization SJF can result in a massive increase in average response time for long tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To see this, note that any server alternates between periods of being idle (when the queue is empty) and periods of being busy (when the queue is non-empty). If we ignore switching overhead, the scheduling discipline has no impact on these periods &#8212; they are only affected by when tasks arrive. Scheduling can only affect which tasks the server handles first. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">With SJF, a long task will only complete immediately before an idle period; it is always the last thing in the queue to complete. As utilization increases, these idle periods become increasingly rare. For example, if the server is 99% busy, the server will be idle only 1% of the time. Further, idle periods are <EM>not</EM> evenly distributed &#8212; a server is much more likely to be idle if it was idle a second ago. This means that the long jobs are likely to wait for a long time under SJF under high load. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1290004 name=x1-1290004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Workloads That Vary With the Queueing Delay</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">So far, we have assumed that arrival rates and service times are independent of queueing delay. This is not always the case. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, suppose a system has 10 users. Each repeatedly issues one request, waits for the result, thinks about the results, and issues the next request. In such a system, the arrival rate will generally be lower during periods when many tasks are queued than during periods when few are. In the limit, during periods when 10 tasks are queued, no new tasks can arrive and the arrival rate is zero. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Or, consider an online store that becomes overloaded and sluggish during a holiday shopping season. Rather than continuing to browse, some customers may get fed up and leave, reducing the number of active browsing sessions and thereby reducing the arrival rate of requests for individual web pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another example is a system with a finite queue. If there is a burst of load that fills the queue, subsequent requests will be turned away until there is space. This heavy-load behavior can be modeled as either a reduced arrival rate or a reduced average service time (some tasks are &#8220;processed&#8221; by being discarded). </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1300004 name=x1-1300004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Multiple Servers</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Many real systems have not just one but multiple servers. Does it matter whether there is a single queue for everyone or a separate queue per server? Real systems take both approaches: supermarkets tend to have a separate queue per cashier; banks tend to have a single shared queue for bank tellers. Some systems do both: airports often have a single queue at security but have separate queues for the parking garage. Which is better for response time? </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Clearly, there are often efficiency gains from having separate queues. Multiprocessor schedulers use separate queues for affinity scheduling and to reduce switching costs; in a supermarket, it may not be practical to have a single queue. On the other hand, users often consider a single (FIFO) queue to be fairer than separate queues. It often seems that we always end up in the slowest line at the supermarket, even if that cannot possibly be true for everyone. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If we focus on average response time, however, a single queue is always better than separate queues, provided that users are not allowed to jump lanes. The reason is simple: because of variations in how long each task takes to service, one server can be idle while another server has multiple queued tasks. Likewise, a single fast server is always better for response time than a large number of slower servers of equal aggregate capacity to the fast server. There is no difference when all servers are busy, but the single fast server will process requests faster when there are fewer active tasks than servers. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1310004 name=x1-1310004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Secondary Bottlenecks</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">If a processor is 90% busy serving web requests, and we add another processor to reduce its load, how much will that improve average response time? Unfortunately, there is not enough information to say. You might like to believe that it will reduce response time by a considerable amount, from R = S / (1 - 0.9) = 10S to R = S / (1 - 0.45) = 1.8S. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, suppose each web request needs not only processing time, but also disk I/O and network bandwidth. If the disk was 80% busy beforehand, it will appear that the processor utilization was the primary problem. Once you add an extra processor, however, the disk becomes the new limiting factor to good performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In some cases, queueing theory can make a specific prediction as to the impact of improving one part of a system in isolation. For example, if arrival times are exponentially distributed and independent of the system response time, and if the service times at the processor, disk, and network are also exponentially distributed and independent of one another, then the overall response time for the system is just the sum of the response times of the components: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#8721; <SUB>i</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; S<SUB>i</SUB> / (1 - U<SUB>i</SUB>) </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this case, improving one part of the system will affect just its contribution to the aggregate system response time. Even though these conditions may not always hold, this is often useful as an approximation to what will occur in real life. </FONT><A id=x1-131001r224 name=x1-131001r224></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.5 </FONT><A id=x1-1320005 name=x1-1320005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Lessons</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">To summarize, almost all real-world systems exhibit some randomness in their arrival process or their service times, or both. For these systems: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Response time increases with increased load. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">System performance is predictable across a range of load factors if we can estimate the average service time per request. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Burstiness increases average response time. It is mathematically convenient to assume an exponential distribution, but many real-world systems exhibit more burstiness and therefore worse user performance. </FONT></P></LI></UL><A id=x1-132001r214 name=x1-132001r214></A><A id=x1-1330006 name=x1-1330006>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.6 Overload Management</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Many systems operate without any direct control over their workload. In the previous section, we explained that good response time and low variance in the response time are both predicated on operating well below peak utilization. If your web service generates interest on Slashdot, however, you can suddenly receive a ton of traffic from new users. Success! Except that the new users discover your service has horrible performance. Disaster! </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">More sophisticated scheduling can help at low to moderate load, but if the load is more than system can handle, response time will spike, even for short tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The key idea in overload management is to design your system to do less work when overloaded. This will seem strange! After all, you want your system to work a particular way; how can you cripple the user&#8217;s experience just when your system becomes popular? Under overload conditions, however, your system is incapable of serving all of the requests in the normal way. The only question is: do you choose what to disable, or do you let events choose for you? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An obvious step is to simply reject some requests in order to preserve reasonable response time for the remaining ones. While this can seem harsh, it is also pragmatic. Under overload, the only way to give anyone good service is to reduce or eliminate service for others. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The approach of turning away requests under overload conditions is common in streaming video applications. An overloaded movie service will reject requests to start new streams so that it can continue to provide good streaming service to users that have already started. Likewise, during the NCAA basketball tournament or during the Olympics, the broadcaster will turn requests away, rather than giving everyone poor service. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An apt analogy, perhaps, is that of a popular restaurant. Why not set out acres of tables so that everyone who shows up at the restaurant can be seated? If the waiters Round Robin among the various tables, you can be seated, but wait an hour to get a menu, then wait another hour to make an order, and so forth. That is one way of dealing with a persistent overload situation &#8212; by making the user experience so unpleasant that none of your customers will return! As absurd as this scenario is, however, it is close to how we allocate scarce space on congested highways &#8212; by making everyone wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A less obvious step is to somehow reduce the service time per request under overload conditions. A good example of this happened on September 11, 2001 when CNN&#8217;s web page was overwhelmed with people trying to get updates about the terrorist attacks. To make the site usable, CNN shifted to a static page that was less personalized and sophisticated but that was faster to serve. As another example, when experiencing unexpected load, EBay will update its auction listings less frequently, saving work that can be used for processing other requests. Finally, an overloaded movie service can reduce the bit rate for everyone in order to serve more simultaneous requests at slightly lower quality. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Amazon has designed its web site to always return a result quickly, even when the requested data is unavailable due to overload conditions. Every backend service has both a normal interface and a fallback to use if its results are not ready in time. For example, this means a user can be told that their purchase will be shipped shortly, even when the book is actually out of stock. This is a strategic decision that it is better to give a wrong answer quickly, and apologize later, rather than to wait to give the right answer more slowly. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, many systems have the opposite problem: they do more work per request as load increases. A simple example of this would be using a linked list to manage a queue of requests: as more requests are queued, more processing time is used maintaining the queue and not getting useful work done. If amount of work per task increases as the load increases, then response times will soar even faster with increased utilization, and throughput can decrease as we add load. This makes overload management even more important. </FONT><A id=x1-13300122 name=x1-13300122></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00437.gif" data-calibre-src="OEBPS/Images/image00437.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.22: </B>Measured throughput (cars per hour) versus occupancy (percentage of the road covered with vehicles). Each data point represents a separate observation. At low load, throughput increases linearly; once load passes a critical point, adding vehicles decreases average throughput. As each vehicle moves more slowly, it takes more time on the highway to complete its journey, increasing load. Data reprinted from Nagel and Schreckenberg&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Xnagel"}'><FONT style="BACKGROUND-COLOR: #7be1e1">122</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">].</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A real-life example of this phenomenon is with highway traffic. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-13300122"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.22</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> provides measured data of throughput versus load for one stretch of highway. As you add cars to an empty highway, it increases the rate that cars traverse a given point on the highway. However, at very high loads, the density of cars causes a transition to stop and go traffic, where the rate of progress is much slower than when there were fewer cars. A common solution for highways is to use onramp limiters &#8212; to limit the rate that new cars can enter the highway if the system is close to overload. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Time-slicing in the presence of caches has similar behavior. When load is low, there are few time slices, and every task uses its cache efficiently. As more tasks are added to the system, there are more time slices and fewer cache hits, slowing down the processor just when we need it to be running at peak efficiency. In networks, packets are dropped when the network is overloaded. Without careful protocol design, this can cause the sender to retransmit packets, further overloading the network. TCP congestion control, now a common part of almost every Internet connection, was developed precisely to deal with this effect. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You may have even experienced this issue. Some students, as homework piles up, become less, rather than more, efficient. After all, it is hard to concentrate on one project if you know that you really ought to be also working on a different one. But if you decide to take the lessons of this textbook to heart and decide to blow off some of your homework to get the rest of your assignments done, let us suggest that you choose some class other than operating systems! </FONT><A id=x1-133002r230 name=x1-133002r230></A></P><A id=x1-1340007 name=x1-1340007>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.7 Case Study: Servers in a Data Center</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">We can illustrate the application of the ideas discussed in this chapter, by considering how we should manage a collection of servers in a data center to provide responsive web service. Many web services, such as Google, Facebook, and Amazon, are organized as a set of front-end machines that redirect incoming requests to a larger set of back-end machines. We illustrate this in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-13500123"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.23</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. This architecture isolates clients from the architecture of the back-end systems, so that more capacity can be added to the back-end simply by changing the configuration of the front-end systems. Back-end servers can also be taken off-line, have their software upgraded, and so forth, completely transparently to clients. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To provide good response time to the clients of the web service: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When clients first connect to the service, the front-end node assigns each customer to a back-end server to balance load. Customers can be spread evenly across the back-end servers or they can be assigned to a node with low current load, much as customers at a supermarket select the shortest line for a cashier. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Additional requests from the same client can be assigned to the same back-end server, as a form of affinity scheduling. Once a server has fetched client data, it will be faster for it to handle additional requests. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We need to prevent individual users from hogging resources, because that can disrupt performance for other users. A back-end server can favor short tasks over long ones; they can also keep track of the total resources used by each client, reducing the scheduling priority of any client consuming more than their fair share of resources. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is often crucial to the usability of a web service to keep response time small. This requires monitoring both the rate of arrivals and the average amount of computing, disk, and network resources consumed by each request. Back-end servers should be added before average server utilization gets too high. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since it often takes considerable time to bring new servers online, we need to predict future load and have a backup plan for overload conditions.</FONT></P></LI></UL><A id=x1-134001r232 name=x1-134001r232></A><A id=x1-1350008 name=x1-1350008>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.8 Summary and Future Directions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Resource scheduling is an ancient topic in computer science. Almost from the moment that computers were first multiprogrammed, operating system designers have had to decide which tasks to do first and which to leave for later. This decision &#8212; the system&#8217;s scheduling policy &#8212; can have a significant impact on system responsiveness and usability. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, the cumulative effect of Moore&#8217;s Law has shifted the balance towards a focus on improving response time for users, rather than on efficient utilization of resources for the computer. At the same time, the massive scale of the Internet means that many services need to be designed to provide good response time across a wide range of load conditions. Our goal in this chapter is to give you the conceptual basis for making those design choices. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several ongoing trends pose new and interesting challenges to effective resource scheduling. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multicore systems.</B> Although almost all new servers, desktops, laptops and smartphones are multicore systems, relatively few widely used applications have been redesigned to take full advantage of multiple processors. This is likely to change over the next few years as multicore systems become ubiquitous and as they scale to larger numbers of processors per chip. Although we have the concepts in place to manage resource sharing among multiple parallel applications, commercial systems are only just now starting to deploy these ideas. It will be interesting to see how the theory works out in practice. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Cache affinity.</B> Over the past twenty years, processor architects have radically increased both the size and number of levels of on-chip caches. There is little reason to believe that this trend will reverse. Although processor clock rates are improving slowly, transistor density is still increasing at a rapid rate. This will make it both possible and desirable to have even larger, multi-level on-chip caches to achieve good performance. Thus, it is likely that scheduling for cache affinity will be an even larger factor in the future than it is today. Balancing when to respect affinity and when to migrate is still somewhat of an open question, as is deciding how to spread or coalesce application threads across caches. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Energy-aware scheduling.</B> The number of energy-constrained computers such as smartphones, tablets, and laptops, now far outstrips powered computers such as desktops and servers. As a result, we are likely to see the development of hardware to monitor and manage energy use by applications, and the operating system will need to make use of that hardware support. We are likely to see operating systems sandbox application energy use to prevent faulty or malicious applications from running down the battery. Likewise, just as applications can adapt to changing numbers of processors, we are likely to see applications that adapt their behavior to energy availability. </FONT></P></LI></UL><A id=x1-13500123 name=x1-13500123></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="about:../Images/image00438.gif" data-calibre-src="OEBPS/Images/image00438.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.23: </B>A web service often consists of a number of front-end servers who redirect incoming client requests to a larger set of back-end servers.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=Q1-1-235 name=Q1-1-235></A><A id=Q1-1-236 name=Q1-1-236></A><A id=x1-1360008 name=x1-1360008>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=problems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For shortest job first, if the scheduler assigns a task to the processor, and no other task becomes schedulable in the meantime, will the scheduler ever preempt the current task? Why or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Devise a workload where FIFO is pessimal &#8212; it does the worst possible choices &#8212; for average response time. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you do your homework assignments in SJF-order. After all, you feel like you are making a lot of progress! What might go wrong? </FONT>
<P></P>
<LI>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Given the following mix of tasks, task lengths, and arrival times, compute the completion and response time for each task, along with the average response time for the FIFO, RR, and SJF algorithms. Assume a time slice of 10 milliseconds and that all times are in milliseconds. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Task</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Length</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Arrival Time</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Completion Time</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Response Time</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 85 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 30 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 10 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 35 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 15 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 20 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 80 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 50 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 85 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Average:</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Is it possible for an application to run slower when assigned 10 processors than when assigned 8? Why or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose your company is considering using one of two candidate scheduling algorithms. One is Round Robin, with an overhead of 1% of the processing power of the system. The second is a wizzy new system that predicts the future and so it can closely approximate SJF, but it takes an overhead of 10% of the processing power of the system. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assume randomized arrivals and random task lengths. Under what conditions will the simpler algorithm outperform the more complex, and vice versa? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Are there non-trivial workloads for which Multi-level Feedback Queue is an optimal policy? Why or why not? (A trivial workload is one with only one or a few tasks or tasks that last a single instruction.) </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">If a queueing system with one server has a workload of 1000 tasks arriving per second, and the average number of tasks waiting or getting service is 5, what is the average response time per task? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Is it possible for a system in equilibrium to have both bounded average response time and 100% utilization? Why or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For a queueing system with random arrivals and service times, how does the variance in the service time affect the system response time? Briefly explain. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Most round-robin schedulers use a fixed size quantum. Give an argument in favor of and against a small quantum. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Which provides the best average response time when there are multiple servers (e.g., bank tellers, supermarket cash registers, airline ticket takers): a single FIFO queue or a FIFO queue per server? Why? Assume that you cannot predict how long any customer is going to take at the server, and that once you have picked a queue to wait in, you are stuck and cannot change queues. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Three tasks, A, B, and C are run concurrently on a computer system. </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Task A arrives first at time 0, and uses the CPU for 100 ms before finishing. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Task B arrives shortly after A, still at time 0. Task B loops ten times; for each iteration of the loop, B uses the CPU for 2 ms and then it does I/O for 8 ms. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Task C is identical to B, but arrives shortly after B, still at time 0.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming there is no overhead to doing a context switch, identify when A, B and C will finish for each of the following CPU scheduling disciplines: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin with a 1 ms time slice </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin with a 100 ms time slice </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Multilevel feedback with four levels, and a time slice for the highest priority level is 1 ms. </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Shortest job first </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For each of the following processor scheduling policies, describe the set of workloads under which that policy is optimal in terms of minimizing average response time (does the same thing as shortest job first) and the set of workloads under which the policy is pessimal (does the same thing as longest job first). If there are no workloads under which a policy is optimal or pessimal, indicate that. </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Multilevel feedback queues </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Explain how you would set up a valid experimental comparison between two scheduling policies, one of which can starve some jobs. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">As system administrator of a popular social networking website, you notice that usage peaks during working hours (10am &#8211; 5pm) and the evening (7 &#8211; 10pm) on the US east coast. The CEO asks you to design a system where during these peak hours there will be three levels of users. Users in level 1 are the center of the social network, and so they are to enjoy better response time than users in level 2, who in turn will enjoy better response time than users in level 3. You are to design such a system so that all users will still get some progress, but with the indicated preferences in place. </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Will a fixed priority scheme with pre-emption and three fixed priorities work? Why, or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Will a UNIX-style multi-feedback queue work? Why, or why not? </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the following preemptive priority-scheduling algorithm based on dynamically changing priorities. Larger numbers imply higher priority. Tasks are preempted whenever there is a higher priority task. When a task is waiting for CPU (in the ready queue, but not running), its priority changes at a rate of a: </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">P(t) = P<SUB>0</SUB> + a &#215; (t - t<SUB>0</SUB>) </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">where t<SUB>0</SUB> is the time at which the task joins the ready queue and P<SUB>0</SUB> is its initial priority, assigned when the task enters the ready queue or is preempted. Similarly, when it is running, the task&#8217;s priority changes at a rate b. The parameters a, b and P<SUB>0</SUB> can be used to obtain many different scheduling algorithms. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">What is the algorithm that results from P<SUB>0</SUB> = 0 and b &gt; a &gt; 0? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">What is the algorithm that results from P<SUB>0</SUB> = 0 and a &lt; b &lt; 0? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose tasks are assigned a priority 0 when they arrive, but they retain their priority when they are preempted. What happens if two tasks arrive at nearly the same time and a &gt; 0 &gt; b? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">How should we adjust the algorithm to eliminate this pathology? </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For a computer with two cores and a hyperthreading level of two, draw a graph of the rate of progress of a compute-intensive task as a function of time, depending on whether it is running alone, or with 1, 2, 3, or 4 other tasks. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Implement a test on your computer to see if your answer to the previous problem is correct. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">A countermeasure is a strategy by which a user (or an application) exploits the characteristics of the processor scheduling policy to get as much of the processing time as possible. For example, if the scheduler trusts users to give accurate estimates of how long each task will take, it can give higher priority to short tasks. However, a countermeasure would be for the user to tell the system that the user&#8217;s tasks are short even when they are not. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Devise a countermeasure strategy for each of the following processor scheduling policies; your strategy should minimize an individual application&#8217;s response time (even if it hurts overall system performance). You may assume perfect knowledge &#8212; for example, your strategy can be based on which jobs will arrive in the future, where your application is in the queue, and how long the tasks ahead of you will run before blocking. Your strategy should also be robust &#8212; it should work properly even if there are no other tasks in the system, there are only short tasks, or there are only long running tasks. If no strategy will improve your application&#8217;s response time, then explain why. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Last in first out </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin, assuming tasks are put at the end of the ready list when they become ready to run </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Multilevel feedback queues, where tasks are put on the highest priority queue when they become ready to run </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a computer system running a general-purpose workload. Measured utilizations (in terms of time, not space) are given in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-13600124"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.24</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-13600124 name=x1-13600124></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Processor utilization </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 20.0% </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Disk </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 99.7%</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Network </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 5.0% </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.24: </B>Measured utilizations of a computer system.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For each of the following changes, say what its likely impact will be on processor utilization, and explain why. Is it likely to significantly increase, marginally increase, significantly decrease, marginally decrease, or have no effect on the processor utilization? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Get a faster CPU </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Get a faster disk </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Increase the degree of multiprogramming </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Get a faster network </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></LI></OL><A id=Q1-1-239 name=Q1-1-239></A><A id=Q1-1-240 name=Q1-1-240></A><A id=Q1-1-241 name=Q1-1-241></A><A id=Q1-1-242 name=Q1-1-242></A><A id=Q1-1-243 name=Q1-1-243></A><A id=Q1-1-244 name=Q1-1-244></A><A id=Q1-1-245 name=Q1-1-245></A><A id=Q1-1-246 name=Q1-1-246></A><A id=Q1-1-247 name=Q1-1-247></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-1370008 name=x1-1370008>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">References</FONT></I></H2></A>
<TABLE cellSpacing=0 cellPadding=0 border=0>
<TBODY>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[1]</FONT><A id=XAdams:2006:CSH:1168857.1168860 name=XAdams:2006:CSH:1168857.1168860></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Keith Adams and Ole Agesen. A comparison of software and hardware techniques for x86 virtualization. In Proceedings of the 12th International conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS-XII, pages 2&#8211;13, 2006. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[2]</FONT><A id=XAnderson:1992:SAE:146941.146944 name=XAnderson:1992:SAE:146941.146944></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Thomas&nbsp;E. Anderson, Brian&nbsp;N. Bershad, Edward&nbsp;D. Lazowska, and Henry&nbsp;M. Levy. Scheduler activations: effective kernel support for the user-level management of parallelism. ACM Trans. Comput. Syst., 10(1):53&#8211;79, February 1992. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[3]</FONT><A id=XAnderson:1991:IAO:106972.106985 name=XAnderson:1991:IAO:106972.106985></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Thomas&nbsp;E. Anderson, Henry&nbsp;M. Levy, Brian&nbsp;N. Bershad, and Edward&nbsp;D. Lazowska. The interaction of architecture and operating system design. In Proceedings of the fourth International conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS-IV, pages 108&#8211;120, 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[4]</FONT><A id=XAppel:1991:VMP:106972.106984 name=XAppel:1991:VMP:106972.106984></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Andrew&nbsp;W. Appel and Kai Li. Virtual memory primitives for user programs. In Proceedings of the fourth International conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS-IV, pages 96&#8211;107, 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[5]</FONT><A id=XAviram:2010:ESD:1924943.1924957 name=XAviram:2010:ESD:1924943.1924957></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Amittai Aviram, Shu-Chun Weng, Sen Hu, and Bryan Ford. Efficient system-enforced deterministic parallelism. In Proceedings of the 9th USENIX conference on Operating Systems Design and Implementation, OSDI&#8217;10, pages 1&#8211;16, 2010. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[6]</FONT><A id=XBabaoglu:1981:CSS:800216.806595 name=XBabaoglu:1981:CSS:800216.806595></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">&#214;zalp Babaoglu and William Joy. Converting a swap-based system to do paging in an architecture lacking page-referenced bits. In Proceedings of the eighth ACM Symposium on Operating Systems Principles, SOSP &#8217;81, pages 78&#8211;86, 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[7]</FONT><A id=XBacon name=XBacon></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David Bacon, Joshua Bloch, Jeff Bogda, Cliff Click, Paul Haahr, Doug Lea, Tom May, Jan-Willem Maessen, Jeremy Manson, John&nbsp;D. Mitchell, Kelvin Nilsen, Bill Pugh, and Emin&nbsp;Gun Sirer. The &#8220;double-checked locking is broken" declaration. </FONT><A href="http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html"><FONT style="BACKGROUND-COLOR: #7be1e1">http://www.cs.umd. edu/~pugh/java/memoryModel/DoubleCheckedLocking.html</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[8]</FONT><A id=XBanga:1999:RCN:296806.296810 name=XBanga:1999:RCN:296806.296810></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Gaurav Banga, Peter Druschel, and Jeffrey&nbsp;C. Mogul. Resource containers: a new facility for resource management in server systems. In Proceedings of the third USENIX symposium on Operating Systems Design and Implementation, OSDI &#8217;99, pages 45&#8211;58, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[9]</FONT><A id=XBarham:2003:XAV:945445.945462 name=XBarham:2003:XAV:945445.945462></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, and Andrew Warfield. Xen and the art of virtualization. In Proceedings of the nineteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;03, pages 164&#8211;177, 2003. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[10]</FONT><A id=Xbarney name=Xbarney></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Blaise Barney. POSIX threads programming. http://computing.llnl.gov/tutorials/pthreads/, 2013. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[11]</FONT><A id=XBartlett:1981:NK:800216.806587 name=XBartlett:1981:NK:800216.806587></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Joel&nbsp;F. Bartlett. A nonstop kernel. In Proceedings of the eighth ACM Symposium on Operating Systems Principles, SOSP &#8217;81, pages 22&#8211;29, 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[12]</FONT><A id=XBaumann:2009:MNO:1629575.1629579 name=XBaumann:2009:MNO:1629575.1629579></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Andrew Baumann, Paul Barham, Pierre-Evariste Dagand, Tim Harris, Rebecca Isaacs, Simon Peter, Timothy Roscoe, Adrian Sch&#252;pbach, and Akhilesh Singhania. The multikernel: a new OS architecture for scalable multicore systems. In Proceedings of the 22nd ACM Symposium on Operating Systems Principles, SOSP &#8217;09, pages 29&#8211;44, 2009. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[13]</FONT><A id=XBensoussan:1972:MVM:355602.361306 name=XBensoussan:1972:MVM:355602.361306></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">A.&nbsp;Bensoussan, C.&nbsp;T. Clingen, and R.&nbsp;C. Daley. The multics virtual memory: concepts and design. Commun. ACM, 15(5):308&#8211;318, May 1972. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[14]</FONT><A id=XBergan:2010:DPG:1924943.1924956 name=XBergan:2010:DPG:1924943.1924956></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Tom Bergan, Nicholas Hunt, Luis Ceze, and Steven&nbsp;D. Gribble. Deterministic process groups in dOS. In Proceedings of the 9th USENIX conference on Operating Systems Design and Implementation, OSDI&#8217;10, pages 1&#8211;16, 2010. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[15]</FONT><A id=XBershad:1995:ESP:224056.224077 name=XBershad:1995:ESP:224056.224077></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">B.&nbsp;N. Bershad, S.&nbsp;Savage, P.&nbsp;Pardyak, E.&nbsp;G. Sirer, M.&nbsp;E. Fiuczynski, D.&nbsp;Becker, C.&nbsp;Chambers, and S.&nbsp;Eggers. Extensibility safety and performance in the SPIN operating system. In Proceedings of the fifteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;95, pages 267&#8211;283, 1995. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[16]</FONT><A id=XBershad:1990:LRP:77648.77650 name=XBershad:1990:LRP:77648.77650></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Brian&nbsp;N. Bershad, Thomas&nbsp;E. Anderson, Edward&nbsp;D. Lazowska, and Henry&nbsp;M. Levy. Lightweight remote procedure call. ACM Trans. Comput. Syst., 8(1):37&#8211;55, February 1990. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[17]</FONT><A id=XBershad:1991:UIC:103720.114701 name=XBershad:1991:UIC:103720.114701></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Brian&nbsp;N. Bershad, Thomas&nbsp;E. Anderson, Edward&nbsp;D. Lazowska, and Henry&nbsp;M. Levy. User-level interprocess communication for shared memory multiprocessors. ACM Trans. Comput. Syst., 9(2):175&#8211;198, May 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[18]</FONT><A id=XBirrellThreads name=XBirrellThreads></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Andrew Birrell. An introduction to programming with threads. Technical Report&nbsp;35, Digital Equipment Corporation Systems Research Center, 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[19]</FONT><A id=XBirrell:1984:IRP:2080.357392 name=XBirrell:1984:IRP:2080.357392></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Andrew&nbsp;D. Birrell and Bruce&nbsp;Jay Nelson. Implementing remote procedure calls. ACM Trans. Comput. Syst., 2(1):39&#8211;59, February 1984. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[20]</FONT><A id=XBoyd-Wickizer:2010:ALS:1924943.1924944 name=XBoyd-Wickizer:2010:ALS:1924943.1924944></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Silas Boyd-Wickizer, Austin&nbsp;T. Clements, Yandong Mao, Aleksey Pesterev, M.&nbsp;Frans Kaashoek, Robert Morris, and Nickolai Zeldovich. An analysis of Linux scalability to many cores. In Proceedings of the 9th USENIX conference on Operating Systems Design and Implementation, OSDI&#8217;10, pages 1&#8211;8, 2010. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[21]</FONT><A id=XDBLP:conf/infocom/BreslauCFPS99 name=XDBLP:conf/infocom/BreslauCFPS99></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Lee Breslau, Pei Cao, Li&nbsp;Fan, Graham Phillips, and Scott Shenker. Web caching and Zipf-like distributions: evidence and implications. In INFOCOM, pages 126&#8211;134, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[22]</FONT><A id=XBressoud:1996:HFT:225535.225538 name=XBressoud:1996:HFT:225535.225538></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Thomas&nbsp;C. Bressoud and Fred&nbsp;B. Schneider. Hypervisor-based fault tolerance. ACM Trans. Comput. Syst., 14(1):80&#8211;107, February 1996. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[23]</FONT><A id=XBrin:1998:ALH:297805.297827 name=XBrin:1998:ALH:297805.297827></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine. In Proceedings of the seventh International conference on the World Wide Web, WWW7, pages 107&#8211;117, 1998. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[24]</FONT><A id=XBruning name=XBruning></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Max Bruning. ZFS on-disk data walk (or: Where&#8217;s my data?). In OpenSolaris Developer Conference, 2008. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[25]</FONT><A id=XBugnion:1997:DRC:265924.265930 name=XBugnion:1997:DRC:265924.265930></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Edouard Bugnion, Scott Devine, Kinshuk Govil, and Mendel Rosenblum. Disco: running commodity operating systems on scalable multiprocessors. ACM Trans. Comput. Syst., 15(4):412&#8211;447, November 1997. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[26]</FONT><A id=XCarrier name=XCarrier></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Brian Carrier. File System Forensic Analysis. Addison Wesley Professional, 2005. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[27]</FONT><A id=XCastro:2009:FBS:1629575.1629581 name=XCastro:2009:FBS:1629575.1629581></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Miguel Castro, Manuel Costa, Jean-Philippe Martin, Marcus Peinado, Periklis Akritidis, Austin Donnelly, Paul Barham, and Richard Black. Fast byte-granularity software fault isolation. In Proceedings of the 22nd ACM Symposium on Operating Systems Principles, SOSP &#8217;09, pages 45&#8211;58, 2009. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[28]</FONT><A id=XChapin:1995:HFC:224056.224059 name=XChapin:1995:HFC:224056.224059></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">J.&nbsp;Chapin, M.&nbsp;Rosenblum, S.&nbsp;Devine, T.&nbsp;Lahiri, D.&nbsp;Teodosiu, and A.&nbsp;Gupta. Hive: fault containment for shared-memory multiprocessors. In Proceedings of the fifteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;95, pages 12&#8211;25, 1995. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[29]</FONT><A id=XChase:1994:SPS:195792.195795 name=XChase:1994:SPS:195792.195795></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jeffrey&nbsp;S. Chase, Henry&nbsp;M. Levy, Michael&nbsp;J. Feeley, and Edward&nbsp;D. Lazowska. Sharing and protection in a single-address-space operating system. ACM Trans. Comput. Syst., 12(4):271&#8211;307, November 1994. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[30]</FONT><A id=XChen:1993:IOS:168619.168629 name=XChen:1993:IOS:168619.168629></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">J.&nbsp;Bradley Chen and Brian&nbsp;N. Bershad. The impact of operating system structure on memory system performance. In Proceedings of the fourteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;93, pages 120&#8211;133, 1993. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[31]</FONT><A id=XChen:2001:VBR:874075.876409 name=XChen:2001:VBR:874075.876409></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Peter&nbsp;M. Chen and Brian&nbsp;D. Noble. When virtual is better than real. In Proceedings of the Eighth Workshop on Hot Topics in Operating Systems, HOTOS &#8217;01, 2001. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[32]</FONT><A id=XCheriton:1988:VDS:42392.42400 name=XCheriton:1988:VDS:42392.42400></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David Cheriton. The V distributed system. Commun. ACM, 31(3):314&#8211;333, March 1988. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[33]</FONT><A id=XCheriton:1994:CMO:1267638.1267652 name=XCheriton:1994:CMO:1267638.1267652></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David&nbsp;R. Cheriton and Kenneth&nbsp;J. Duda. A caching model of operating system kernel functionality. In Proceedings of the 1st USENIX conference on Operating Systems Design and Implementation, OSDI &#8217;94, 1994. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[34]</FONT><A id=XClark:1985:SSU:323647.323645 name=XClark:1985:SSU:323647.323645></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David&nbsp;D. Clark. The structuring of systems using upcalls. In Proceedings of the tenth ACM Symposium on Operating Systems Principles, SOSP &#8217;85, pages 171&#8211;180, 1985. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[35]</FONT><A id=XCondit:2009:BIT:1629575.1629589 name=XCondit:2009:BIT:1629575.1629589></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jeremy Condit, Edmund&nbsp;B. Nightingale, Christopher Frost, Engin Ipek, Benjamin Lee, Doug Burger, and Derrick Coetzee. Better I/O through byte-addressable, persistent memory. In Proceedings of the 22nd ACM Symposium on Operating Systems Principles, SOSP &#8217;09, pages 133&#8211;146, 2009. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[36]</FONT><A id=XCorbato:1991:BSF:114669.114686 name=XCorbato:1991:BSF:114669.114686></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Fernando&nbsp;J. Corbat&#243;. On building systems that will fail. Commun. ACM, 34(9):72&#8211;81, September 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[37]</FONT><A id=Xmultics name=Xmultics></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Fernando&nbsp;J. Corbat&#243; and Victor&nbsp;A. Vyssotsky. Introduction and overview of the Multics system. AFIPS Fall Joint Computer Conference, 27(1):185&#8211;196, 1965. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[38]</FONT><A id=XCreasy:1981:OVT:1664853.1664863 name=XCreasy:1981:OVT:1664853.1664863></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">R.&nbsp;J. Creasy. The origin of the VM/370 time-sharing system. IBM J. Res. Dev., 25(5):483&#8211;490, September 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[39]</FONT><A id=XDahlin:1994:CCU:1267638.1267657 name=XDahlin:1994:CCU:1267638.1267657></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Michael&nbsp;D. Dahlin, Randolph&nbsp;Y. Wang, Thomas&nbsp;E. Anderson, and David&nbsp;A. Patterson. Cooperative caching: using remote client memory to improve file system performance. In Proceedings of the 1st USENIX conference on Operating Systems Design and Implementation, OSDI &#8217;94, 1994. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[40]</FONT><A id=XDaley:1968:VMP:363095.363139 name=XDaley:1968:VMP:363095.363139></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Robert&nbsp;C. Daley and Jack&nbsp;B. Dennis. Virtual memory, processes, and sharing in Multics. Commun. ACM, 11(5):306&#8211;312, May 1968. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[41]</FONT><A id=XdeJonge:1993:LDN:168619.168621 name=XdeJonge:1993:LDN:168619.168621></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Wiebren de&nbsp;Jonge, M.&nbsp;Frans Kaashoek, and Wilson&nbsp;C. Hsieh. The logical disk: a new approach to improving file systems. In Proceedings of the fourteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;93, pages 15&#8211;28, 1993. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[42]</FONT><A id=XDean:2004:MSD:1251254.1251264 name=XDean:2004:MSD:1251254.1251264></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jeffrey Dean and Sanjay Ghemawat. MapReduce: simplified data processing on large clusters. In Proceedings of the 6th USENIX Symposium on Operating Systems Design &amp; Implementation, OSDI&#8217;04, 2004. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[43]</FONT><A id=XDenning:1968:WSM:363095.363141 name=XDenning:1968:WSM:363095.363141></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Peter&nbsp;J. Denning. The working set model for program behavior. Commun. ACM, 11(5):323&#8211;333, May 1968. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[44]</FONT><A id=X1702696 name=X1702696></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">P.J. Denning. Working sets past and present. Software Engineering, IEEE Transactions on, SE-6(1):64 &#8211; 84, jan. 1980. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[45]</FONT><A id=XDennis:1965:SDM:321296.321310 name=XDennis:1965:SDM:321296.321310></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jack&nbsp;B. Dennis. Segmentation and the design of multiprogrammed computer systems. J. ACM, 12(4):589&#8211;602, October 1965. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[46]</FONT><A id=XDennis:1966:PSM:365230.365252 name=XDennis:1966:PSM:365230.365252></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jack&nbsp;B. Dennis and Earl&nbsp;C. Van&nbsp;Horn. Programming semantics for multiprogrammed computations. Commun. ACM, 9(3):143&#8211;155, March 1966. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[47]</FONT><A id=XDijkstra:1965:SPC:365559.365617 name=XDijkstra:1965:SPC:365559.365617></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">E.&nbsp;W. Dijkstra. Solution of a problem in concurrent programming control. Commun. ACM, 8(9):569&#8211;, September 1965. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[48]</FONT><A id=XDijkstra:1968:SLS:363095.363143 name=XDijkstra:1968:SLS:363095.363143></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Edsger&nbsp;W. Dijkstra. The structure of the &#8220;THE&#8221;-multiprogramming system. Commun. ACM, 11(5):341&#8211;346, May 1968. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[49]</FONT><A id=XDobrescu:2009:REP:1629575.1629578 name=XDobrescu:2009:REP:1629575.1629578></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Mihai Dobrescu, Norbert Egi, Katerina Argyraki, Byung-Gon Chun, Kevin Fall, Gianluca Iannaccone, Allan Knies, Maziar Manesh, and Sylvia Ratnasamy. Routebricks: exploiting parallelism to scale software routers. In Proceedings of the 22nd ACM Symposium on Operating Systems Principles, SOSP &#8217;09, pages 15&#8211;28, 2009. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[50]</FONT><A id=XDonovanPNaCL name=XDonovanPNaCL></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Alan Donovan, Robert Muth, Brad Chen, and David Sehr. Portable Native Client executables. Technical report, Google, 2012. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[51]</FONT><A id=XDouglis:1991:TPM:116715.116729 name=XDouglis:1991:TPM:116715.116729></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Fred Douglis and John Ousterhout. Transparent process migration: design alternatives and the Sprite implementation. Softw. Pract. Exper., 21(8):757&#8211;785, July 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[52]</FONT><A id=XDraves:1991:UCI:121132.121155 name=XDraves:1991:UCI:121132.121155></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Richard&nbsp;P. Draves, Brian&nbsp;N. Bershad, Richard&nbsp;F. Rashid, and Randall&nbsp;W. Dean. Using continuations to implement thread management and communication in operating systems. In Proceedings of the thirteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;91, pages 122&#8211;136, 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[53]</FONT><A id=XDruschel:1993:FHC:173668.168634 name=XDruschel:1993:FHC:173668.168634></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Peter Druschel and Larry&nbsp;L. Peterson. Fbufs: a high-bandwidth cross-domain transfer facility. SIGOPS Oper. Syst. Rev., 27(5):189&#8211;202, December 1993. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[54]</FONT><A id=XDunlap:2002:REI:844128.844148 name=XDunlap:2002:REI:844128.844148></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">George&nbsp;W. Dunlap, Samuel&nbsp;T. King, Sukru Cinar, Murtaza&nbsp;A. Basrai, and Peter&nbsp;M. Chen. ReVirt: enabling intrusion analysis through virtual-machine logging and replay. SIGOPS Oper. Syst. Rev., 36(SI):211&#8211;224, December 2002. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[55]</FONT><A id=XEfstathopoulos:2005:LEP:1095810.1095813 name=XEfstathopoulos:2005:LEP:1095810.1095813></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Petros Efstathopoulos, Maxwell Krohn, Steve VanDeBogart, Cliff Frey, David Ziegler, Eddie Kohler, David Mazi&#232;res, Frans Kaashoek, and Robert Morris. Labels and event processes in the Asbestos operating system. In Proceedings of the twentieth ACM Symposium on Operating Systems Principles, SOSP &#8217;05, pages 17&#8211;30, 2005. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[56]</FONT><A id=XEngler:1995:EOS:224056.224076 name=XEngler:1995:EOS:224056.224076></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">D.&nbsp;R. Engler, M.&nbsp;F. Kaashoek, and J.&nbsp;O&#8217;Toole, Jr. Exokernel: an operating system architecture for application-level resource management. In Proceedings of the fifteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;95, pages 251&#8211;266, 1995. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[57]</FONT><A id=XEngler:2001:BDB:502034.502041 name=XEngler:2001:BDB:502034.502041></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Dawson Engler, David&nbsp;Yu Chen, Seth Hallem, Andy Chou, and Benjamin Chelf. Bugs as deviant behavior: a general approach to inferring errors in systems code. In Proceedings of the eighteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;01, pages 57&#8211;72, 2001. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[58]</FONT><A id=XFabry:1974:CA:361011.361070 name=XFabry:1974:CA:361011.361070></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">R.&nbsp;S. Fabry. Capability-based addressing. Commun. ACM, 17(7):403&#8211;412, July 1974. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[59]</FONT><A id=XFlinn:1999:EAM:319151.319155 name=XFlinn:1999:EAM:319151.319155></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jason Flinn and M.&nbsp;Satyanarayanan. Energy-aware adaptation for mobile applications. In Proceedings of the seventeenth ACM Symposium on Operating Systems Principles, SOSP &#8217;99, pages 48&#8211;63, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[60]</FONT><A id=XFrost:2007:GFS:1294261.1294291 name=XFrost:2007:GFS:1294261.1294291></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Christopher Frost, Mike Mammarella, Eddie Kohler, Andrew de&nbsp;los Reyes, Shant Hovsepian, Andrew Matsuoka, and Lei Zhang. Generalized file system dependencies. In Proceedings of twenty-first ACM Symposium on Operating Systems Principles, SOSP &#8217;07, pages 307&#8211;320, 2007. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[61]</FONT><A id=XGanger:2000:SUS:350853.350863 name=XGanger:2000:SUS:350853.350863></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Gregory&nbsp;R. Ganger, Marshall&nbsp;Kirk McKusick, Craig A.&nbsp;N. Soules, and Yale&nbsp;N. Patt. Soft updates: a solution to the metadata update problem in file systems. ACM Trans. Comput. Syst., 18(2):127&#8211;153, May 2000. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[62]</FONT><A id=XGarfinkel:1996:PUI:227419 name=XGarfinkel:1996:PUI:227419></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Simson Garfinkel and Gene Spafford. Practical Unix and Internet security (2nd ed.). O&#8217;Reilly &amp; Associates, Inc., 1996. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[63]</FONT><A id=XGarfinkel:2003:TVM:945445.945464 name=XGarfinkel:2003:TVM:945445.945464></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Tal Garfinkel, Ben Pfaff, Jim Chow, Mendel Rosenblum, and Dan Boneh. Terra: a virtual machine-based platform for trusted computing. In Proceedings of the nineteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;03, pages 193&#8211;206, 2003. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[64]</FONT><A id=XGlerum:2009:DLT:1629575.1629586 name=XGlerum:2009:DLT:1629575.1629586></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Kirk Glerum, Kinshuman Kinshumann, Steve Greenberg, Gabriel Aul, Vince Orgovan, Greg Nichols, David Grant, Gretchen Loihle, and Galen Hunt. Debugging in the (very) large: ten years of implementation and experience. In Proceedings of the 22nd ACM Symposium on Operating Systems Principles, SOSP &#8217;09, pages 103&#8211;116, 2009. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[65]</FONT><A id=XGoldbergSurvey name=XGoldbergSurvey></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">R.P. Goldberg. Survey of virtual machine research. IEEE Computer, 7(6):34&#8211;45, June 1974. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[66]</FONT><A id=XGovil:1999:CDR:319151.319162 name=XGovil:1999:CDR:319151.319162></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Kinshuk Govil, Dan Teodosiu, Yongqiang Huang, and Mendel Rosenblum. Cellular Disco: resource management using virtual clusters on shared-memory multiprocessors. In Proceedings of the seventeenth ACM Symposium on Operating Systems Principles, SOSP &#8217;99, pages 154&#8211;169, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[67]</FONT><A id=XGray:1981:TCV:1286831.1286846 name=XGray:1981:TCV:1286831.1286846></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jim Gray. The transaction concept: virtues and limitations (invited paper). In Proceedings of the seventh International conference on Very Large Data Bases, VLDB &#8217;81, pages 144&#8211;154, 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[68]</FONT><A id=XGrayStop name=XGrayStop></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jim Gray. Why do computers stop and what can be done about it? Technical Report TR-85.7, HP Labs, 1985. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[69]</FONT><A id=XGray:1981:RMS:356842.356847 name=XGray:1981:RMS:356842.356847></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jim Gray, Paul McJones, Mike Blasgen, Bruce Lindsay, Raymond Lorie, Tom Price, Franco Putzolu, and Irving Traiger. The recovery manager of the System R database manager. ACM Comput. Surv., 13(2):223&#8211;242, June 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[70]</FONT><A id=XDBLP:books/mk/GrayR93 name=XDBLP:books/mk/GrayR93></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jim Gray and Andreas Reuter. Transaction Processing: Concepts and Techniques. Morgan Kaufmann, 1993. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[71]</FONT><A id=XGray:1991:HCS:125810.125815 name=XGray:1991:HCS:125810.125815></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jim Gray and Daniel&nbsp;P. Siewiorek. High-availability computer systems. Computer, 24(9):39&#8211;48, September 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[72]</FONT><A id=XGupta:2008:DEH:1855741.1855763 name=XGupta:2008:DEH:1855741.1855763></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Diwaker Gupta, Sangmin Lee, Michael Vrable, Stefan Savage, Alex&nbsp;C. Snoeren, George Varghese, Geoffrey&nbsp;M. Voelker, and Amin Vahdat. Difference engine: harnessing memory redundancy in virtual machines. In Proceedings of the 8th USENIX conference on Operating Systems Design and Implementation, OSDI&#8217;08, pages 309&#8211;322, 2008. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[73]</FONT><A id=XHadoop name=XHadoop></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Hadoop. http://hadoop.apache.org. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[74]</FONT><A id=XHand:1999:SNO:296806.296812 name=XHand:1999:SNO:296806.296812></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Steven&nbsp;M. Hand. Self-paging in the Nemesis operating system. In Proceedings of the third USENIX Symposium on Operating Systems Design and Implementation, OSDI &#8217;99, pages 73&#8211;86, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[75]</FONT><A id=XHansen:1970:NMS:362258.362278 name=XHansen:1970:NMS:362258.362278></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Per&nbsp;Brinch Hansen. The nucleus of a multiprogramming system. Commun. ACM, 13(4):238&#8211;241, April 1970. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[76]</FONT><A id=XHarchol-Balter:1995:EPL:224056.225838 name=XHarchol-Balter:1995:EPL:224056.225838></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Mor Harchol-Balter and Allen&nbsp;B. Downey. Exploiting process lifetime distributions for dynamic load balancing. In Proceedings of the fifteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;95, pages 236&#8211;, 1995. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[77]</FONT><A id=XHarty:1992:APM:143365.143511 name=XHarty:1992:APM:143365.143511></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Kieran Harty and David&nbsp;R. Cheriton. Application-controlled physical memory using external page-cache management. In Proceedings of the fifth International conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS-V, pages 187&#8211;197, 1992. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[78]</FONT><A id=XHaskin:1988:RMQ:35037.35060 name=XHaskin:1988:RMQ:35037.35060></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Rober Haskin, Yoni Malachi, and Gregory Chan. Recovery management in QuickSilver. ACM Trans. Comput. Syst., 6(1):82&#8211;108, February 1988. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[79]</FONT><A id=XDBLP:books/daglib/0028244 name=XDBLP:books/daglib/0028244></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">John&nbsp;L. Hennessy and David&nbsp;A. Patterson. Computer Architecture - A Quantitative Approach (5. ed.). Morgan Kaufmann, 2012. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[80]</FONT><A id=XHerlihy:1991:WS:114005.102808 name=XHerlihy:1991:WS:114005.102808></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Maurice Herlihy. Wait-free synchronization. ACM Trans. Program. Lang. Syst., 13(1):124&#8211;149, January 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[81]</FONT><A id=XArtOfMpP name=XArtOfMpP></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Maurice Herlihy and Nir Shavit. The Art of Multiprocessor Programming. Morgan Kaufmann, 2008. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[82]</FONT><A id=XHitzWAFL name=XHitzWAFL></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Dave Hitz, James Lau, and Michael Malcolm. File system design for an NFS file server appliance. Technical Report 3002, Network Appliance, 1995. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[83]</FONT><A id=XHoare74monitors:an name=XHoare74monitors:an></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">C.&nbsp;A.&nbsp;R. Hoare. Monitors: An operating system structuring concept. Communications of the ACM, 17:549&#8211;557, 1974. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[84]</FONT><A id=XHoare:1978:CSP:359576.359585 name=XHoare:1978:CSP:359576.359585></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">C.&nbsp;A.&nbsp;R. Hoare. Communicating sequential processes. Commun. ACM, 21(8):666&#8211;677, August 1978. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[85]</FONT><A id=XHoare:1981:EOC:358549.358561 name=XHoare:1981:EOC:358549.358561></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">C.&nbsp;A.&nbsp;R. Hoare. The emperor&#8217;s old clothes. Commun. ACM, 24(2):75&#8211;83, February 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[86]</FONT><A id=XHorsley:1979:PSE:800091.802927 name=XHorsley:1979:PSE:800091.802927></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Thomas&nbsp;R. Horsley and William&nbsp;C. Lynch. Pilot: A software engineering case study. In Proceedings of the 4th International conference on Software engineering, ICSE &#8217;79, pages 94&#8211;99, 1979. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[87]</FONT><A id=XJain name=XJain></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Raj Jain. The Art of Computer Systems Performance Analysis. John Wiley &amp; Sons, 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[88]</FONT><A id=XKadav:2012:UMD:2150976.2150987 name=XKadav:2012:UMD:2150976.2150987></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Asim Kadav and Michael&nbsp;M. Swift. Understanding modern device drivers. In Proceedings of the seventeenth international conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &#8217;12, pages 87&#8211;98, New York, NY, USA, 2012. ACM. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[89]</FONT><A id=XKarger:1991:RVV:123453.123459 name=XKarger:1991:RVV:123453.123459></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Paul&nbsp;A. Karger, Mary&nbsp;Ellen Zurko, Douglas&nbsp;W. Bonin, Andrew&nbsp;H. Mason, and Clifford&nbsp;E. Kahn. A retrospective on the VAX VMM security kernel. IEEE Trans. Softw. Eng., 17(11):1147&#8211;1165, November 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[90]</FONT><A id=XKhalidi:1993:EFS:168619.168620 name=XKhalidi:1993:EFS:168619.168620></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Yousef&nbsp;A. Khalidi and Michael&nbsp;N. Nelson. Extensible file systems in Spring. In Proceedings of the fourteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;93, pages 1&#8211;14, 1993. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[91]</FONT><A id=XKlein:2009:SFV:1629575.1629596 name=XKlein:2009:SFV:1629575.1629596></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, Thomas Sewell, Harvey Tuch, and Simon Winwood. sel4: formal verification of an OS kernel. In Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles, SOSP &#8217;09, pages 207&#8211;220, 2009. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[92]</FONT><A id=XKleinrock:1972:PSQ:321707.321717 name=XKleinrock:1972:PSQ:321707.321717></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">L.&nbsp;Kleinrock and R.&nbsp;R. Muntz. Processor sharing queueing models of mixed scheduling disciplines for time shared system. J. ACM, 19(3):464&#8211;482, July 1972. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[93]</FONT><A id=X286 name=X286></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Leonard Kleinrock. Queueing Systems, Volume II: Computer Applications. Wiley Interscience, 1976. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[94]</FONT><A id=XKung:1981:OMC:319566.319567 name=XKung:1981:OMC:319566.319567></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">H.&nbsp;T. Kung and John&nbsp;T. Robinson. On optimistic methods for concurrency control. ACM Trans. Database Syst., 6(2):213&#8211;226, June 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[95]</FONT><A id=XLamport:1987:FME:7351.7352 name=XLamport:1987:FME:7351.7352></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Leslie Lamport. A fast mutual exclusion algorithm. ACM Trans. Comput. Syst., 5(1):1&#8211;11, January 1987. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[96]</FONT><A id=XLampson:1984:HCS:1308434.1308739 name=XLampson:1984:HCS:1308434.1308739></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">B.&nbsp;W. Lampson. Hints for computer system design. IEEE Softw., 1(1):11&#8211;28, January 1984. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[97]</FONT><A id=XLampsonCrash name=XLampsonCrash></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Butler Lampson and Howard Sturgis. Crash recovery in a distributed data storage system. Technical report, Xerox Palo Alto Research Center, 1979. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[98]</FONT><A id=XLampson:1980:EPM:358818.358824 name=XLampson:1980:EPM:358818.358824></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Butler&nbsp;W. Lampson and David&nbsp;D. Redell. Experience with processes and monitors in Mesa. Commun. ACM, 23(2):105&#8211;117, February 1980. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[99]</FONT><A id=XLampson:1976:ROS:360051.360074 name=XLampson:1976:ROS:360051.360074></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Butler&nbsp;W. Lampson and Howard&nbsp;E. Sturgis. Reflections on an operating system design. Commun. ACM, 19(5):251&#8211;265, May 1976. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[100]</FONT><A id=XLarus:2010:SS:1787234.1787253 name=XLarus:2010:SS:1787234.1787253></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">James Larus and Galen Hunt. The Singularity system. Commun. ACM, 53(8):72&#8211;79, August 2010. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[101]</FONT><A id=XLauer79onthe name=XLauer79onthe></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Hugh&nbsp;C. Lauer and Roger&nbsp;M. Needham. On the duality of operating system structures. In Operating Systems Review, pages 3&#8211;19, 1979. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[102]</FONT><A id=XLazowska:1984:QSP:2971 name=XLazowska:1984:QSP:2971></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Edward&nbsp;D. Lazowska, John Zahorjan, G.&nbsp;Scott Graham, and Kenneth&nbsp;C. Sevcik. Quantitative system performance: computer system analysis using queueing network models. Prentice-Hall, Inc., 1984. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[103]</FONT><A id=XLeland:1994:SNE:178221.178222 name=XLeland:1994:SNE:178221.178222></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Will&nbsp;E. Leland, Murad&nbsp;S. Taqqu, Walter Willinger, and Daniel&nbsp;V. Wilson. On the self-similar nature of Ethernet traffic (extended version). IEEE/ACM Trans. Netw., 2(1):1&#8211;15, February 1994. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[104]</FONT><A id=XLeveson:1993:ITA:161477.161479 name=XLeveson:1993:ITA:161477.161479></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">N.&nbsp;G. Leveson and C.&nbsp;S. Turner. An investigation of the Therac-25 accidents. Computer, 26(7):18&#8211;41, July 1993. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[105]</FONT><A id=XLevy:1982:VMM:1318724.1319149 name=XLevy:1982:VMM:1318724.1319149></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">H.&nbsp;M. Levy and P.&nbsp;H. Lipman. Virtual memory management in the VAX/VMS operating system. Computer, 15(3):35&#8211;41, March 1982. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[106]</FONT><A id=XLiedtke:1995:MC:224056.224075 name=XLiedtke:1995:MC:224056.224075></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">J.&nbsp;Liedtke. On micro-kernel construction. In Proceedings of the fifteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;95, pages 237&#8211;250, 1995. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[107]</FONT><A id=XLions:1996:LCU name=XLions:1996:LCU></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">John Lions. Lions&#8217; Commentary on UNIX 6th Edition, with Source Code. Peer-to-Peer Communications, 1996. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[108]</FONT><A id=XLiptay:1968:SAS:1663411.1663413 name=XLiptay:1968:SAS:1663411.1663413></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">J.&nbsp;S. Liptay. Structural aspects of the System/360 model 85: ii the cache. IBM Syst. J., 7(1):15&#8211;21, March 1968. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[109]</FONT><A id=XLowell:2000:EFT:1251229.1251249 name=XLowell:2000:EFT:1251229.1251249></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David&nbsp;E. Lowell, Subhachandra Chandra, and Peter&nbsp;M. Chen. Exploring failure transparency and the limits of generic recovery. In Proceedings of the 4th conference on Symposium on Operating Systems Design and Implementation, OSDI&#8217;00, pages 20&#8211;20, 2000. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[110]</FONT><A id=XLowell:1997:FTR:268998.266665 name=XLowell:1997:FTR:268998.266665></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David&nbsp;E. Lowell and Peter&nbsp;M. Chen. Free transactions with Rio Vista. In Proceedings of the sixteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;97, pages 92&#8211;101, 1997. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[111]</FONT><A id=XmckenneyHard name=XmckenneyHard></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">P.&nbsp;McKenney. Is parallel programming hard, and, if so, what can be done about it? http://kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.2011.05.30a.pdf. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[112]</FONT><A id=XRCU name=XRCU></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Paul&nbsp;E. McKenney, Dipankar Sarma, Andrea Arcangeli, Andi Kleen, Orran Krieger, and Rusty Russell. Read-copy update. In Ottawa Linux Symposium, pages 338&#8211;367, June 2002. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[113]</FONT><A id=XMcKusick:1984:FFS:989.990 name=XMcKusick:1984:FFS:989.990></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Marshall&nbsp;K. McKusick, William&nbsp;N. Joy, Samuel&nbsp;J. Leffler, and Robert&nbsp;S. Fabry. A fast file system for UNIX. ACM Trans. Comput. Syst., 2(3):181&#8211;197, August 1984. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[114]</FONT><A id=XMcKusick:1996:DIO:231070 name=XMcKusick:1996:DIO:231070></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Marshall&nbsp;Kirk McKusick, Keith Bostic, Michael&nbsp;J. Karels, and John&nbsp;S. Quarterman. The design and implementation of the 4.4BSD operating system. Addison Wesley Longman Publishing Co., Inc., 1996. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[115]</FONT><A id=XMellor-Crummey:1991:ASS:103727.103729 name=XMellor-Crummey:1991:ASS:103727.103729></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">John&nbsp;M. Mellor-Crummey and Michael&nbsp;L. Scott. Algorithms for scalable synchronization on shared-memory multiprocessors. ACM Trans. Comput. Syst., 9(1):21&#8211;65, February 1991. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[116]</FONT><A id=XMeyers:2004 name=XMeyers:2004></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Scott Meyers and Andrei Alexandrescu. C++ and the perils of double-checked locking. Dr. Dobbs Journal, 2004. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[117]</FONT><A id=XMogul:1997:ERL:263326.263335 name=XMogul:1997:ERL:263326.263335></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jeffrey&nbsp;C. Mogul and K.&nbsp;K. Ramakrishnan. Eliminating receive livelock in an interrupt-driven kernel. ACM Trans. Comput. Syst., 15(3):217&#8211;252, August 1997. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[118]</FONT><A id=XMogul87thepacket name=XMogul87thepacket></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jeffrey&nbsp;C. Mogul, Richard&nbsp;F. Rashid, and Michael&nbsp;J. Accetta. The packet filter: An efficient mechanism for user-level network code. In In the Proceedings of the eleventh ACM Symposium on Operating Systems Principles, pages 39&#8211;51, 1987. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[119]</FONT><A id=XMohan:1992:ATR:128765.128770 name=XMohan:1992:ATR:128765.128770></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">C.&nbsp;Mohan, Don Haderle, Bruce Lindsay, Hamid Pirahesh, and Peter Schwarz. ARIES: a transaction recovery method supporting fine-granularity locking and partial rollbacks using write-ahead logging. ACM Trans. Database Syst., 17(1):94&#8211;162, March 1992. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[120]</FONT><A id=XMooresLaw name=XMooresLaw></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Gordon&nbsp;E. Moore. Cramming more components onto integrated circuits. Electronics, 38(8):114&#8211;117, 1965. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[121]</FONT><A id=XMusuvathi:2008:FRH:1855741.1855760 name=XMusuvathi:2008:FRH:1855741.1855760></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Madanlal Musuvathi, Shaz Qadeer, Thomas Ball, Gerard Basler, Piramanayagam&nbsp;Arumuga Nainar, and Iulian Neamtiu. Finding and reproducing Heisenbugs in concurrent programs. In Proceedings of the 8th USENIX conference on Operating Systems Design and Implementation, OSDI&#8217;08, pages 267&#8211;280, 2008. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[122]</FONT><A id=Xnagel name=Xnagel></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Kai Nagel and Michael Schreckenberg. A cellular automaton model for freeway traffic. J. Phys. I France, 1992. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[123]</FONT><A id=XNecula:1996:SKE:238721.238781 name=XNecula:1996:SKE:238721.238781></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">George&nbsp;C. Necula and Peter Lee. Safe kernel extensions without run-time checking. In Proceedings of the second USENIX Symposium on Operating Systems Design and Implementation, OSDI &#8217;96, pages 229&#8211;243, 1996. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[124]</FONT><A id=XNightingale:2008:RS:1394441.1394442 name=XNightingale:2008:RS:1394441.1394442></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Edmund&nbsp;B. Nightingale, Kaushik Veeraraghavan, Peter&nbsp;M. Chen, and Jason Flinn. Rethink the sync. ACM Trans. Comput. Syst., 26(3):6:1&#8211;6:26, September 2008. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[125]</FONT><A id=XOrganick:1972:MSE:1095599 name=XOrganick:1972:MSE:1095599></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Elliott&nbsp;I. Organick. The Multics system: an examination of its structure. MIT Press, 1972. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[126]</FONT><A id=XOsman:2002:DIZ:1060289.1060323 name=XOsman:2002:DIZ:1060289.1060323></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Steven Osman, Dinesh Subhraveti, Gong Su, and Jason Nieh. The design and implementation of Zap: a system for migrating computing environments. In Proceedings of the fifth USENIX Symposium on Operating Systems Design and Implementation, OSDI &#8217;02, pages 361&#8211;376, 2002. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[127]</FONT><A id=XOusterhoutCo name=XOusterhoutCo></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">John Ousterhout. Scheduling techniques for concurrent systems. In Proceedings of Third International Conference on Distributed Computing Systems, pages 22&#8211;30, 1982. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[128]</FONT><A id=XOusterhoutFast name=XOusterhoutFast></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">John Ousterhout. Why aren&#8217;t operating systems getting faster as fast as hardware? In Proceedings USENIX Conference, pages 247&#8211;256, 1990. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[129]</FONT><A id=Xousterhoutbadthreads name=Xousterhoutbadthreads></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">John Ousterhout. Why threads are a bad idea (for most purposes). In USENIX Winter Technical Conference, 1996. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[130]</FONT><A id=XPai:1999:FEP:1268708.1268723 name=XPai:1999:FEP:1268708.1268723></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Vivek&nbsp;S. Pai, Peter Druschel, and Willy Zwaenepoel. Flash: an efficient and portable web server. In Proceedings of the annual conference on USENIX Annual Technical Conference, ATEC &#8217;99, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[131]</FONT><A id=XPai:1999:IUI:296806.296808 name=XPai:1999:IUI:296806.296808></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Vivek&nbsp;S. Pai, Peter Druschel, and Willy Zwaenepoel. IO-lite: a unified I/O buffering and caching system. In Proceedings of the third USENIX Symposium on Operating Systems Design and Implementation, OSDI &#8217;99, pages 15&#8211;28, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[132]</FONT><A id=XPatterson:1988:CRA:50202.50214 name=XPatterson:1988:CRA:50202.50214></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David&nbsp;A. Patterson, Garth Gibson, and Randy&nbsp;H. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1988 ACM SIGMOD International conference on Management of Data, SIGMOD &#8217;88, pages 109&#8211;116, 1988. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[133]</FONT><A id=XPeterson:1989:RXE:74850.74860 name=XPeterson:1989:RXE:74850.74860></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">L.&nbsp;Peterson, N.&nbsp;Hutchinson, S.&nbsp;O&#8217;Malley, and M.&nbsp;Abbott. RPC in the x-Kernel: evaluating new design techniques. In Proceedings of the twelfth ACM Symposium on Operating Systems Principles, SOSP &#8217;89, pages 91&#8211;101, 1989. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[134]</FONT><A id=XPincus:2004:BSS:1018027.1018271 name=XPincus:2004:BSS:1018027.1018271></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jonathan Pincus and Brandon Baker. Beyond stack smashing: recent advances in exploiting buffer overruns. IEEE Security and Privacy, 2(4):20&#8211;27, July 2004. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[135]</FONT><A id=XPinheiro:2007:FTL:1267903.1267905 name=XPinheiro:2007:FTL:1267903.1267905></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Eduardo Pinheiro, Wolf-Dietrich Weber, and Luiz&nbsp;Andr&#233; Barroso. Failure trends in a large disk drive population. In Proceedings of the 5th USENIX conference on File and Storage Technologies, FAST &#8217;07, pages 2&#8211;2, 2007. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[136]</FONT><A id=XPrabhakaran:2005:IFS:1095810.1095830 name=XPrabhakaran:2005:IFS:1095810.1095830></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Vijayan Prabhakaran, Lakshmi&nbsp;N. Bairavasundaram, Nitin Agrawal, Haryadi&nbsp;S. Gunawi, Andrea&nbsp;C. Arpaci-Dusseau, and Remzi&nbsp;H. Arpaci-Dusseau. IRON file systems. In Proceedings of the twentieth ACM Symposium on Operating Systems Principles, SOSP &#8217;05, pages 206&#8211;220, 2005. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[137]</FONT><A id=XRashidMach name=XRashidMach></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Richard Rashid, Robert Baron, Alessandro Forin, David Golub, Michael Jones, Daniel Julin, Douglas Orr, and Richard Sanzi. Mach: A foundation for open systems. In Proceedings of the Second Workshop on Workstation Operating Systems(WWOS2), 1989. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[138]</FONT><A id=XDBLP:journals/tc/RashidTYGBBBC88 name=XDBLP:journals/tc/RashidTYGBBBC88></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Richard&nbsp;F. Rashid, Avadis Tevanian, Michael Young, David&nbsp;B. Golub, Robert&nbsp;V. Baron, David&nbsp;L. Black, William&nbsp;J. Bolosky, and Jonathan Chew. Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures. IEEE Trans. Computers, 37(8):896&#8211;907, 1988. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[139]</FONT><A id=Xraymond2001cathedral name=Xraymond2001cathedral></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">E.S. Raymond. The Cathedral and the Bazaar: Musings On Linux And Open Source By An Accidental Revolutionary. O&#8217;Reilly Series. O&#8217;Reilly, 2001. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[140]</FONT><A id=XRedell:1980:POS:358818.358822 name=XRedell:1980:POS:358818.358822></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">David&nbsp;D. Redell, Yogen&nbsp;K. Dalal, Thomas&nbsp;R. Horsley, Hugh&nbsp;C. Lauer, William&nbsp;C. Lynch, Paul&nbsp;R. McJones, Hal&nbsp;G. Murray, and Stephen&nbsp;C. Purcell. Pilot: an operating system for a personal computer. Commun. ACM, 23(2):81&#8211;92, February 1980. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[141]</FONT><A id=XRitchie:1974:UTS:361011.361061 name=XRitchie:1974:UTS:361011.361061></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Dennis&nbsp;M. Ritchie and Ken Thompson. The UNIX time-sharing system. Commun. ACM, 17(7):365&#8211;375, July 1974. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[142]</FONT><A id=XRosenblum:1992:DIL:146941.146943 name=XRosenblum:1992:DIL:146941.146943></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Mendel Rosenblum and John&nbsp;K. Ousterhout. The design and implementation of a log-structured file system. ACM Trans. Comput. Syst., 10(1):26&#8211;52, February 1992. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[143]</FONT><A id=XRuemmler:1994:IDD:176756.176761 name=XRuemmler:1994:IDD:176756.176761></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Chris Ruemmler and John Wilkes. An introduction to disk drive modeling. Computer, 27(3):17&#8211;28, March 1994. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[144]</FONT><A id=XSaltzer:1984:EAS:357401.357402 name=XSaltzer:1984:EAS:357401.357402></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">J.&nbsp;H. Saltzer, D.&nbsp;P. Reed, and D.&nbsp;D. Clark. End-to-end arguments in system design. ACM Trans. Comput. Syst., 2(4):277&#8211;288, November 1984. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[145]</FONT><A id=XSaltzer:1974:PCI:361011.361067 name=XSaltzer:1974:PCI:361011.361067></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Jerome&nbsp;H. Saltzer. Protection and the control of information sharing in Multics. Commun. ACM, 17(7):388&#8211;402, July 1974. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[146]</FONT><A id=XSatyanarayanan:1994:LRV:174613.174615 name=XSatyanarayanan:1994:LRV:174613.174615></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">M.&nbsp;Satyanarayanan, Henry&nbsp;H. Mashburn, Puneet Kumar, David&nbsp;C. Steere, and James&nbsp;J. Kistler. Lightweight recoverable virtual memory. ACM Trans. Comput. Syst., 12(1):33&#8211;57, February 1994. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[147]</FONT><A id=XSavage:1997:EDD:265924.265927 name=XSavage:1997:EDD:265924.265927></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Stefan Savage, Michael Burrows, Greg Nelson, Patrick Sobalvarro, and Thomas Anderson. Eraser: a dynamic data race detector for multithreaded programs. ACM Trans. Comput. Syst., 15(4):391&#8211;411, November 1997. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[148]</FONT><A id=XSchroeder:2007:DFR:1267903.1267904 name=XSchroeder:2007:DFR:1267903.1267904></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Bianca Schroeder and Garth&nbsp;A. Gibson. Disk failures in the real world: what does an MTTF of 1,000,000 hours mean to you? In Proceedings of the 5th USENIX conference on File and Storage Technologies, FAST &#8217;07, 2007. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[149]</FONT><A id=XSchroeder:2006:WSU:1125274.1125276 name=XSchroeder:2006:WSU:1125274.1125276></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Bianca Schroeder and Mor Harchol-Balter. Web servers under overload: How scheduling can help. ACM Trans. Internet Technol., 6(1):20&#8211;52, February 2006. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[150]</FONT><A id=XSchroeder:1977:MKD:800214.806546 name=XSchroeder:1977:MKD:800214.806546></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Michael&nbsp;D. Schroeder, David&nbsp;D. Clark, and Jerome&nbsp;H. Saltzer. The Multics kernel design project. In Proceedings of the sixth ACM Symposium on Operating Systems Principles, SOSP &#8217;77, pages 43&#8211;56, 1977. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[151]</FONT><A id=XSchroeder:1972:HAI:361268.361275 name=XSchroeder:1972:HAI:361268.361275></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Michael&nbsp;D. Schroeder and Jerome&nbsp;H. Saltzer. A hardware architecture for implementing protection rings. Commun. ACM, 15(3):157&#8211;170, March 1972. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[152]</FONT><A id=XSiewiorek:1984:AFC:1319725.1320039 name=XSiewiorek:1984:AFC:1319725.1320039></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">D.&nbsp;P. Siewiorek. Architecture of fault-tolerant computers. Computer, 17(8):9&#8211;18, August 1984. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[153]</FONT><A id=XSpafford:1989:CA:63526.63527 name=XSpafford:1989:CA:63526.63527></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">E.&nbsp;H. Spafford. Crisis and aftermath. Commun. ACM, 32(6):678&#8211;687, June 1989. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[154]</FONT><A id=XSQL name=XSQL></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Structured Query Language (SQL). http://en.wikipedia.org/wiki/SQL. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[155]</FONT><A id=XStonebraker:1981:OSS:358699.358703 name=XStonebraker:1981:OSS:358699.358703></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Michael Stonebraker. Operating system support for database management. Commun. ACM, 24(7):412&#8211;418, July 1981. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[156]</FONT><A id=XSwift:2006:RDD:1189256.1189257 name=XSwift:2006:RDD:1189256.1189257></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Michael&nbsp;M. Swift, Muthukaruppan Annamalai, Brian&nbsp;N. Bershad, and Henry&nbsp;M. Levy. Recovering device drivers. ACM Trans. Comput. Syst., 24(4):333&#8211;360, November 2006. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[157]</FONT><A id=XThompson78uniximplementation name=XThompson78uniximplementation></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">K.&nbsp;Thompson. Unix implementation. Bell System Technical Journal, 57:1931&#8211;1946, 1978. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[158]</FONT><A id=XThompson:1984:RTT:358198.358210 name=XThompson:1984:RTT:358198.358210></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Ken Thompson. Reflections on trusting trust. Commun. ACM, 27(8):761&#8211;763, August 1984. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[159]</FONT><A id=Xtyma name=Xtyma></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Paul Tyma. Thousands of threads and blocking i/o. http://www.mailinator.com/tymaPaulMultithreaded.pdf, 2008. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[160]</FONT><A id=XvanRenesse:1998:GPC:319195.319208 name=XvanRenesse:1998:GPC:319195.319208></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Robbert van Renesse. Goal-oriented programming, or composition using events, or threads considered harmful. In ACM SIGOPS European Workshop on Support for Composing Distributed Applications, pages 82&#8211;87, 1998. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[161]</FONT><A id=XVerhofstad:1978:RTD:356725.356730 name=XVerhofstad:1978:RTD:356725.356730></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Joost S.&nbsp;M. Verhofstad. Recovery techniques for database systems. ACM Comput. Surv., 10(2):167&#8211;195, June 1978. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[162]</FONT><A id=XVrable:2005:SFC:1095810.1095825 name=XVrable:2005:SFC:1095810.1095825></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Michael Vrable, Justin Ma, Jay Chen, David Moore, Erik Vandekieft, Alex&nbsp;C. Snoeren, Geoffrey&nbsp;M. Voelker, and Stefan Savage. Scalability, fidelity, and containment in the Potemkin virtual honeyfarm. In Proceedings of the twentieth ACM Symposium on Operating Systems Principles, SOSP &#8217;05, pages 148&#8211;162, 2005. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[163]</FONT><A id=XWahbe:1993:ESF:168619.168635 name=XWahbe:1993:ESF:168619.168635></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Robert Wahbe, Steven Lucco, Thomas&nbsp;E. Anderson, and Susan&nbsp;L. Graham. Efficient software-based fault isolation. In Proceedings of the fourteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;93, pages 203&#8211;216, 1993. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[164]</FONT><A id=XWaldspurger:2002:MRM:844128.844146 name=XWaldspurger:2002:MRM:844128.844146></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Carl&nbsp;A. Waldspurger. Memory resource management in VMware ESX server. SIGOPS Oper. Syst. Rev., 36(SI):181&#8211;194, December 2002. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[165]</FONT><A id=XWhitaker:2002:SPD:1060289.1060308 name=XWhitaker:2002:SPD:1060289.1060308></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Andrew Whitaker, Marianne Shaw, and Steven&nbsp;D. Gribble. Scale and performance in the Denali isolation kernel. In Proceedings of the fifth USENIX Symposium on Operating Systems Design and Implementation, OSDI &#8217;02, pages 195&#8211;209, 2002. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[166]</FONT><A id=XWilkes:1995:HAH:224056.224065 name=XWilkes:1995:HAH:224056.224065></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">J.&nbsp;Wilkes, R.&nbsp;Golding, C.&nbsp;Staelin, and T.&nbsp;Sullivan. The HP AutoRAID hierarchical storage system. In Proceedings of the fifteenth ACM Symposium on Operating Systems Principles, SOSP &#8217;95, pages 96&#8211;108, 1995. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[167]</FONT><A id=XWolman:1999:SPC:319151.319153 name=XWolman:1999:SPC:319151.319153></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Alec Wolman, M.&nbsp;Voelker, Nitin Sharma, Neal Cardwell, Anna Karlin, and Henry&nbsp;M. Levy. On the scale and performance of cooperative web proxy caching. In Proceedings of the seventeenth ACM Symposium on Operating Systems Principles, SOSP &#8217;99, pages 16&#8211;31, 1999. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[168]</FONT><A id=XWulf:1974:HKM:355616.364017 name=XWulf:1974:HKM:355616.364017></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">W.&nbsp;Wulf, E.&nbsp;Cohen, W.&nbsp;Corwin, A.&nbsp;Jones, R.&nbsp;Levin, C.&nbsp;Pierson, and F.&nbsp;Pollack. Hydra: the kernel of a multiprocessor operating system. Commun. ACM, 17(6):337&#8211;345, June 1974. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[169]</FONT><A id=XYee:2009:NCS:1607723.1608126 name=XYee:2009:NCS:1607723.1608126></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Bennet Yee, David Sehr, Gregory Dardyk, J.&nbsp;Bradley Chen, Robert Muth, Tavis Ormandy, Shiki Okasaka, Neha Narula, and Nicholas Fullagar. Native Client: a sandbox for portable, untrusted x86 native code. In Proceedings of the 2009 30th IEEE Symposium on Security and Privacy, SP &#8217;09, pages 79&#8211;93, 2009. </FONT></TD></TR>
<TR vAlign=top>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">[170]</FONT><A id=XZeldovich:2011:MIF:2018396.2018419 name=XZeldovich:2011:MIF:2018396.2018419></A><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;</FONT></TD>
<TD><FONT style="BACKGROUND-COLOR: #7be1e1">Nickolai Zeldovich, Silas Boyd-Wickizer, Eddie Kohler, and David Mazi&#232;res. Making information flow explicit in HiStar. Commun. ACM, 54(11):93&#8211;101, November 2011. </FONT></TD></TR></TBODY></TABLE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-1380008 name=x1-1380008>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">Glossary</FONT></I></H2></A>
<DL class=description>
<DT class=description><A id="glo:absolute path" name="glo:absolute path"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>absolute path</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file path name interpreted relative to the root directory. </FONT>
<DT class=description><A id="glo:abstract virtual machine" name="glo:abstract virtual machine"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>abstract virtual machine</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The interface provided by an operating system to its applications, including the system call interface, the memory abstraction, exceptions, and signals. </FONT>
<DT class=description><A id="glo:ACID properties" name="glo:ACID properties"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ACID properties</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A mnemonic for the properties of a transaction: atomicity, consistency, isolation, and durability. </FONT>
<DT class=description><A id=glo:acquire-all/release-all name=glo:acquire-all/release-all></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>acquire-all/release-all</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A design pattern to provide atomicity of a request consisting of multiple operations. A thread acquires all of the locks it might need before starting to process a request; it releases the locks once the request is done. </FONT>
<DT class=description><A id="glo:address translation" name="glo:address translation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>address translation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The conversion from the memory address the program thinks it is referencing to the physical location of the memory. </FONT>
<DT class=description><A id="glo:affinity scheduling" name="glo:affinity scheduling"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>affinity scheduling</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling policy where tasks are preferentially scheduled onto the same processor they had previously been assigned, to improve cache reuse. </FONT>
<DT class=description><A id="glo:annual disk failure rate" name="glo:annual disk failure rate"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>annual disk failure rate</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The fraction of disks expected to failure each year. </FONT>
<DT class=description><A id=glo:API name=glo:API></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>API</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:application programming interface"}'>application programming interface</A></EM>. </FONT>
<DT class=description><A id="glo:application programming interface" name="glo:application programming interface"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>application programming interface</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The system call interface provided by an operating system to applications. </FONT>
<DT class=description><A id=glo:arm name=glo:arm></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>arm</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An attachment allowing the motion of the disk head across a disk surface. </FONT>
<DT class=description><A id="glo:arm assembly" name="glo:arm assembly"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>arm assembly</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A motor plus the set of disk arms needed to position a disk head to read or write each surface of the disk. </FONT>
<DT class=description><A id="glo:arrival rate" name="glo:arrival rate"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>arrival rate</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The rate at which tasks arrive for service. </FONT>
<DT class=description><A id="glo:asynchronous I/O" name="glo:asynchronous I/O"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>asynchronous I/O</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A design pattern for system calls to allow a single-threaded process to make multiple concurrent I/O requests. When the process issues an I/O request, the system call returns immediately. The process later on receives a notification when the I/O completes. </FONT>
<DT class=description><A id="glo:asynchronous procedure call" name="glo:asynchronous procedure call"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>asynchronous procedure call</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A procedure call where the caller starts the function, continues execution concurrently with the called function, and later waits for the function to complete. </FONT>
<DT class=description><A id="glo:atomic commit" name="glo:atomic commit"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>atomic commit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The moment when a transaction commits to apply all of its updates. </FONT>
<DT class=description><A id="glo:atomic memory" name="glo:atomic memory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>atomic memory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The value stored in memory is the last value stored by one of the processors, not a mixture of the updates of different processors. </FONT>
<DT class=description><A id="glo:atomic operations" name="glo:atomic operations"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>atomic operations</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Indivisible operations that cannot be interleaved with or split by other operations. </FONT>
<DT class=description><A id="glo:atomic read-modify-write instruction" name="glo:atomic read-modify-write instruction"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>atomic read-modify-write instruction</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A processor-specific instruction that lets one thread temporarily have exclusive and atomic access to a memory location while the instruction executes. Typically, the instruction (atomically) reads a memory location, does some simple arithmetic operation to the value, and stores the result. </FONT>
<DT class=description><A id="glo:attribute record" name="glo:attribute record"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>attribute record</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In NTFS, a variable-size data structure containing either file data or file metadata. </FONT>
<DT class=description><A id=glo:availability name=glo:availability></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>availability</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The percentage of time that a system is usable. </FONT>
<DT class=description><A id="glo:average seek time" name="glo:average seek time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>average seek time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The average time across seeks between each possible pair of tracks on a disk. </FONT>
<DT class=description><A id=glo:AVM name=glo:AVM></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>AVM</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:abstract virtual machine"}'>abstract virtual machine</A></EM>. </FONT>
<DT class=description><A id=glo:backup name=glo:backup></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>backup</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A logically or physically separate copy of a system&#8217;s main storage. </FONT>
<DT class=description><A id="glo:base and bound memory protection" name="glo:base and bound memory protection"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>base and bound memory protection</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An early system for memory protection where each process is limited to a specific range of physical memory. </FONT>
<DT class=description><A id="glo:batch operating system" name="glo:batch operating system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>batch operating system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An early type of operating system that efficiently ran a queue of tasks. While one program was running, another was being loaded into memory. </FONT>
<DT class=description><A id="glo:bathtub model" name="glo:bathtub model"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bathtub model</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A model of disk device failure combining device infant mortality and wear out. </FONT>
<DT class=description><A id="glo:Belady's anomaly" name="glo:Belady's anomaly"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Belady&#8217;s anomaly</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">For some cache replacement policies and some reference patterns, adding space to a cache can hurt the cache hit rate. </FONT>
<DT class=description><A id="glo:best fit" name="glo:best fit"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>best fit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A storage allocation policy that attempts to place a newly allocated file in the smallest free region that is large enough to hold it. </FONT>
<DT class=description><A id=glo:BIOS name=glo:BIOS></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>BIOS</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The initial code run when an Intel x86 computer boots; acronym for Basic Input/Output System. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Boot ROM"}'>Boot ROM</A></EM>. </FONT>
<DT class=description><A id="glo:bit error rate" name="glo:bit error rate"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bit error rate</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The non-recoverable read error rate. </FONT>
<DT class=description><A id=glo:bitmap name=glo:bitmap></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bitmap</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A data structure for block allocation where each block is represented by one bit. </FONT>
<DT class=description><A id="glo:block device" name="glo:block device"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>block device</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An I/O device that allows data to be read or written in fixed-sized blocks. </FONT>
<DT class=description><A id="glo:block group" name="glo:block group"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>block group</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A set of nearby disk tracks. </FONT>
<DT class=description><A id="glo:block integrity metadata" name="glo:block integrity metadata"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>block integrity metadata</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Additional data stored with a block to allow the software to validate that the block has not been corrupted. </FONT>
<DT class=description><A id="glo:blocking bounded queue" name="glo:blocking bounded queue"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>blocking bounded queue</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A bounded queue where a thread trying to remove an item from an empty queue will wait until an item is available, and a thread trying to put an item into a full queue will wait until there is room. </FONT>
<DT class=description><A id=glo:Bohrbugs name=glo:Bohrbugs></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bohrbugs</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Bugs that are deterministic and reproducible, given the same program input. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Heisenbugs"}'>Heisenbugs</A></EM>. </FONT>
<DT class=description><A id="glo:Boot ROM" name="glo:Boot ROM"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Boot ROM</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Special read-only memory containing the initial instructions for booting a computer. </FONT>
<DT class=description><A id=glo:bootloader name=glo:bootloader></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bootloader</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Program stored at a fixed position on disk (or flash RAM) to load the operating system into memory and start it executing. </FONT>
<DT class=description><A id="glo:bounded queue" name="glo:bounded queue"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bounded queue</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A queue with a fixed size limit on the number of items stored in the queue. </FONT>
<DT class=description><A id="glo:bounded resources" name="glo:bounded resources"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bounded resources</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A necessary condition for deadlock: there are a finite number of resources that threads can simultaneously use. </FONT>
<DT class=description><A id="glo:buffer overflow attack" name="glo:buffer overflow attack"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>buffer overflow attack</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An attack that exploits a bug where input can overflow the buffer allocated to hold it, overwriting other important program data structures with data provided by the attacker. One common variation overflows a buffer allocated on the stack (e.g., a local, automatic variable) and replaces the function&#8217;s return address with a return address specified by the attacker, possibly to code &#8220;pushed&#8221; onto the stack with the overflowing input. </FONT>
<DT class=description><A id="glo:bulk synchronous" name="glo:bulk synchronous"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bulk synchronous</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A type of parallel application where work is split into independent tasks and where each task completes before the results of any of the tasks can be used. </FONT>
<DT class=description><A id="glo:bulk synchronous parallel programming" name="glo:bulk synchronous parallel programming"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bulk synchronous parallel programming</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:data parallel programming"}'>data parallel programming</A></EM>. </FONT>
<DT class=description><A id="glo:bursty distribution" name="glo:bursty distribution"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>bursty distribution</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A probability distribution that is less evenly distributed around the mean value than an exponential distribution. See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:exponential distribution"}'>exponential distribution</A></EM>. Compare: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:heavy-tailed distribution"}'>heavy-tailed distribution</A></EM>. </FONT>
<DT class=description><A id=glo:busy-waiting name=glo:busy-waiting></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>busy-waiting</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A thread spins in a loop waiting for a concurrent event to occur, consuming CPU cycles while it is waiting. </FONT>
<DT class=description><A id=glo:cache name=glo:cache></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A copy of data that can be accessed more quickly than the original. </FONT>
<DT class=description><A id="glo:cache hit" name="glo:cache hit"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cache hit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The cache contains the requested item. </FONT>
<DT class=description><A id="glo:cache miss" name="glo:cache miss"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cache miss</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The cache does not contain the requested item. </FONT>
<DT class=description><A id=glo:checkpoint name=glo:checkpoint></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>checkpoint</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A consistent snapshot of the entire state of a process, including the contents of memory and processor registers. </FONT>
<DT class=description><A id="glo:child process" name="glo:child process"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>child process</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A process created by another process. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:parent process"}'>parent process</A></EM>. </FONT>
<DT class=description><A id="glo:Circular SCAN" name="glo:Circular SCAN"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Circular SCAN</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:CSCAN"}'>CSCAN</A></EM>. </FONT>
<DT class=description><A id="glo:circular waiting" name="glo:circular waiting"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>circular waiting</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A necessary condition for deadlock to occur: there is a set of threads such that each thread is waiting for a resource held by another. </FONT>
<DT class=description><A id="glo:client-server communication" name="glo:client-server communication"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>client-server communication</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Two-way communication between processes, where the client sends a request to the server to do some task, and when the operation is complete, the server replies back to the client. </FONT>
<DT class=description><A id="glo:clock algorithm" name="glo:clock algorithm"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>clock algorithm</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A method for identifying a not recently used page to evict. The algorithm sweeps through each page frame: if the page use bit is set, it is cleared; if the use bit is not set, the page is reclaimed. </FONT>
<DT class=description><A id="glo:cloud computing" name="glo:cloud computing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cloud computing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A model of computing where large-scale applications run on shared computing and storage infrastructure in data centers instead of on the user&#8217;s own computer. </FONT>
<DT class=description><A id=glo:commit name=glo:commit></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>commit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The outcome of a transaction where all of its updates occur. </FONT>
<DT class=description><A id=glo:compare-and-swap name=glo:compare-and-swap></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>compare-and-swap</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An atomic read-modify-write instruction that first tests the value of a memory location, and if the value has not been changed, sets it to a new value. </FONT>
<DT class=description><A id="glo:compute-bound task" name="glo:compute-bound task"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>compute-bound task</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A task that primarily uses the processor and does little I/O. </FONT>
<DT class=description><A id="glo:computer virus" name="glo:computer virus"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>computer virus</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A computer program that modifies an operating system or application to copy itself from computer to computer without the computer owner&#8217;s permission or knowledge. Once installed on a computer, a virus often provides the attacker control over the system&#8217;s resources or data. </FONT>
<DT class=description><A id=glo:concurrency name=glo:concurrency></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>concurrency</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Multiple activities that can happen at the same time. </FONT>
<DT class=description><A id="glo:condition variable" name="glo:condition variable"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>condition variable</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A synchronization variable that enables a thread to efficiently wait for a change to shared state protected by a lock. </FONT>
<DT class=description><A id=glo:continuation name=glo:continuation></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>continuation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A data structure used in event-driven programming that keeps track of a task&#8217;s current state and its next step. </FONT>
<DT class=description><A id="glo:cooperating threads" name="glo:cooperating threads"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cooperating threads</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Threads that read and write shared state. </FONT>
<DT class=description><A id="glo:cooperative caching" name="glo:cooperative caching"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cooperative caching</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Using the memory of nearby nodes over a network as a cache to avoid the latency of going to disk. </FONT>
<DT class=description><A id="glo:cooperative multi-threading" name="glo:cooperative multi-threading"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cooperative multi-threading</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Each thread runs without interruption until it explicitly relinquishes control of the processor, e.g., by exiting or calling thread_yield. </FONT>
<DT class=description><A id=glo:copy-on-write name=glo:copy-on-write></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>copy-on-write</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A method of sharing physical memory between two logically distinct copies (e.g., in different processes). Each shared page is marked as read-only so that the operating system kernel is invoked and can make a copy of the page if either process tries to write it. The process can then modify the copy and resume normal execution. </FONT>
<DT class=description><A id="glo:copy-on-write file system" name="glo:copy-on-write file system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>copy-on-write file system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file system where an update to the file system is made by writing new versions of modified data and metadata blocks to free disk blocks. The new blocks can point to unchanged blocks in the previous version of the file system. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:COW file system"}'>COW file system</A></EM>. </FONT>
<DT class=description><A id="glo:core map" name="glo:core map"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>core map</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A data structure used by the memory management system to keep track of the state of physical page frames, such as which processes reference the page frame. </FONT>
<DT class=description><A id="glo:COW file system" name="glo:COW file system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>COW file system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:copy-on-write file system"}'>copy-on-write file system</A></EM>. </FONT>
<DT class=description><A id="glo:critical path" name="glo:critical path"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>critical path</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The minimum sequence of steps for a parallel application to compute its result, even with infinite resources. </FONT>
<DT class=description><A id="glo:critical section" name="glo:critical section"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>critical section</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A sequence of code that operates on shared state. </FONT>
<DT class=description><A id="glo:cross-site scripting" name="glo:cross-site scripting"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cross-site scripting</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An attack against a client computer that works by compromising a server visited by the client. The compromised server then provides scripting code to the client that accesses and downloads the client&#8217;s sensitive data. </FONT>
<DT class=description><A id="glo:cryptographic signature" name="glo:cryptographic signature"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>cryptographic signature</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A specially designed function of a data block and a private cryptographic key that allows someone with the corresponding public key to verify that an authorized entity produced the data block. It is computationally intractable for an attacker without the private key to create a different data block with a valid signature. </FONT>
<DT class=description><A id=glo:CSCAN name=glo:CSCAN></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>CSCAN</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A variation of the SCAN disk scheduling policy in which the disk only services requests when the head is traveling in one direction. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Circular SCAN"}'>Circular SCAN</A></EM>. </FONT>
<DT class=description><A id="glo:current working directory" name="glo:current working directory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>current working directory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The current directory of the process, used for interpreting relative path names. </FONT>
<DT class=description><A id="glo:data breakpoint" name="glo:data breakpoint"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>data breakpoint</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A request to stop the execution of a program when it references or modifies a particular memory location. </FONT>
<DT class=description><A id="glo:data parallel programming" name="glo:data parallel programming"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>data parallel programming</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A programming model where the computation is performed in parallel across all items in a data set. </FONT>
<DT class=description><A id=glo:deadlock name=glo:deadlock></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>deadlock</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A cycle of waiting among a set of threads, where each thread waits for some other thread in the cycle to take some action. </FONT>
<DT class=description><A id="glo:deadlocked state" name="glo:deadlocked state"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>deadlocked state</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The system has at least one deadlock. </FONT>
<DT class=description><A id=glo:declustering name=glo:declustering></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>declustering</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A technique for reducing the recovery time after a disk failure in a RAID system by spreading redundant disk blocks across many disks. </FONT>
<DT class=description><A id="glo:defense in depth" name="glo:defense in depth"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>defense in depth</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Improving security through multiple layers of protection. </FONT>
<DT class=description><A id=glo:defragment name=glo:defragment></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>defragment</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Coalesce scattered disk blocks to improve spatial locality, by reading data from its present storage location and rewriting it to a new, more compact, location. </FONT>
<DT class=description><A id="glo:demand paging" name="glo:demand paging"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>demand paging</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Using address translation hardware to run a process without all of its memory physically present. When the process references a missing page, the hardware traps to the kernel, which brings the page into memory from disk. </FONT>
<DT class=description><A id="glo:deterministic debugging" name="glo:deterministic debugging"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>deterministic debugging</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The ability to re-execute a concurrent process with the same schedule and sequence of internal and external events. </FONT>
<DT class=description><A id="glo:device driver" name="glo:device driver"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>device driver</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Operating system code to initialize and manage a particular I/O device. </FONT>
<DT class=description><A id="glo:direct mapped cache" name="glo:direct mapped cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>direct mapped cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Only one entry in the cache can hold a specific memory location, so on a lookup, the system must check the address against only that entry to determine if there is a cache hit. </FONT>
<DT class=description><A id="glo:direct memory access" name="glo:direct memory access"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>direct memory access</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Hardware I/O devices transfer data directly into/out of main memory at a location specified by the operating system. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:DMA"}'>DMA</A></EM>. </FONT>
<DT class=description><A id="glo:dirty bit" name="glo:dirty bit"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>dirty bit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A status bit in a page table entry recording whether the contents of the page have been modified relative to what is stored on disk. </FONT>
<DT class=description><A id="glo:disk buffer memory" name="glo:disk buffer memory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>disk buffer memory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Memory in the disk controller to buffer data being read or written to the disk. </FONT>
<DT class=description><A id="glo:disk infant mortality" name="glo:disk infant mortality"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>disk infant mortality</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The device failure rate is higher than normal during the first few weeks of use. </FONT>
<DT class=description><A id="glo:disk wear out" name="glo:disk wear out"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>disk wear out</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The device failure rate rises after the device has been in operation for several years. </FONT>
<DT class=description><A id=glo:DMA name=glo:DMA></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>DMA</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:direct memory access"}'>direct memory access</A></EM>. </FONT>
<DT class=description><A id=glo:dnode name=glo:dnode></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>dnode</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In ZFS, a file is represented by variable-depth tree whose root is a dnode and whose leaves are its data blocks. </FONT>
<DT class=description><A id="glo:double indirect block" name="glo:double indirect block"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>double indirect block</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A storage block containing pointers to indirect blocks. </FONT>
<DT class=description><A id="glo:double-checked locking" name="glo:double-checked locking"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>double-checked locking</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A pitfall in concurrent code where a data structure is lazily initialized by first, checking without a lock if it has been set, and if not, acquiring a lock and checking again, before calling the initialization function. With instruction re-ordering, double-checked locking can fail unexpectedly. </FONT>
<DT class=description><A id="glo:dual redundancy array" name="glo:dual redundancy array"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>dual redundancy array</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A RAID storage algorithm using two redundant disk blocks per array to tolerate two disk failures. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:RAID 6"}'>RAID 6</A></EM>. </FONT>
<DT class=description><A id="glo:dual-mode operation" name="glo:dual-mode operation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>dual-mode operation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Hardware processor that has (at least) two privilege levels: one for executing the kernel with complete access to the capabilities of the hardware and a second for executing user code with restricted rights. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:kernel-mode operation"}'>kernel-mode operation</A></EM>. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:user-mode operation"}'>user-mode operation</A></EM>. </FONT>
<DT class=description><A id="glo:dynamically loadable device driver" name="glo:dynamically loadable device driver"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>dynamically loadable device driver</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Software to manage a specific device, interface, or chipset, added to the operating system kernel after the kernel starts running. </FONT>
<DT class=description><A id="glo:earliest deadline first" name="glo:earliest deadline first"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>earliest deadline first</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling policy that performs the task that needs to be completed first, but only if it can be finished in time. </FONT>
<DT class=description><A id=glo:EDF name=glo:EDF></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EDF</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:earliest deadline first"}'>earliest deadline first</A></EM>. </FONT>
<DT class=description><A id=glo:efficiency name=glo:efficiency></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>efficiency</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The lack of overhead in implementing an abstraction. </FONT>
<DT class=description><A id="glo:erasure block" name="glo:erasure block"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>erasure block</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The unit of erasure in a flash memory device. Before any portion of an erasure block can be over-written, every cell in the entire erasure block must be set to a logical &#8220;1.&#8221; </FONT>
<DT class=description><A id="glo:error correcting code" name="glo:error correcting code"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>error correcting code</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A technique for storing data redundantly to allow for the original data to be recovered even though some bits in a disk sector or flash memory page are corrupted. </FONT>
<DT class=description><A id="glo:event-driven programming" name="glo:event-driven programming"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>event-driven programming</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A coding design pattern where a thread spins in a loop; each iteration gets and processes the next I/O event. </FONT>
<DT class=description><A id=glo:exception name=glo:exception></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>exception</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:processor exception"}'>processor exception</A></EM>. </FONT>
<DT class=description><A id="glo:executable image" name="glo:executable image"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>executable image</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">File containing a sequence of machine instructions and initial data values for a program. </FONT>
<DT class=description><A id="glo:execution stack" name="glo:execution stack"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>execution stack</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Space to store the state of local variables during procedure calls. </FONT>
<DT class=description><A id="glo:exponential distribution" name="glo:exponential distribution"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>exponential distribution</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A convenient probability distribution for use in queueing theory because it has the property of being memoryless. For a continuous random variable with a mean of 1&#8725;&#955;, the probability density function is f(x) = &#955; times e raised to the -&#955;x. </FONT>
<DT class=description><A id=glo:extent name=glo:extent></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>extent</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A variable-sized region of a file that is stored in a contiguous region on the storage device. </FONT>
<DT class=description><A id="glo:external fragmentation" name="glo:external fragmentation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>external fragmentation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In a system that allocates memory in contiguous regions, the unusable memory between valid contiguous allocations. A new request for memory may find no single free region that is both contiguous and large enough, even though there is enough free memory in aggregate. </FONT>
<DT class=description><A id=glo:fairness name=glo:fairness></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>fairness</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Partitioning of shared resources between users or applications either equally or balanced according to some desired priorities. </FONT>
<DT class=description><A id="glo:false sharing" name="glo:false sharing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>false sharing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Extra inter-processor communication required because a single cache entry contains portions of two different data structures with different sharing patterns. </FONT>
<DT class=description><A id="glo:fate sharing" name="glo:fate sharing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>fate sharing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When a crash in one module implies a crash in another. For example, a library shares fate with the application it is linked with; if either crashes, the process exits. </FONT>
<DT class=description><A id="glo:fault isolation" name="glo:fault isolation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>fault isolation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An error in one application should not disrupt other applications, or even the operating system itself. </FONT>
<DT class=description><A id=glo:file name=glo:file></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A named collection of data in a file system. </FONT>
<DT class=description><A id="glo:file allocation table" name="glo:file allocation table"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file allocation table</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An array of entries in the FAT file system stored in a reserved area of the volume, where each entry corresponds to one file data block, and points to the next block in the file. </FONT>
<DT class=description><A id="glo:file data" name="glo:file data"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file data</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Contents of a file. </FONT>
<DT class=description><A id="glo:file descriptor" name="glo:file descriptor"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file descriptor</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A handle to an open file, device, or channel. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:file handle"}'>file handle</A></EM>. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:file stream"}'>file stream</A></EM>. </FONT>
<DT class=description><A id="glo:file directory" name="glo:file directory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file directory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A list of human-readable names plus a mapping from each name to a specific file or sub-directory. </FONT>
<DT class=description><A id="glo:file handle" name="glo:file handle"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file handle</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:file descriptor"}'>file descriptor</A></EM>. </FONT>
<DT class=description><A id="glo:file index structure" name="glo:file index structure"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file index structure</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A persistently stored data structure used to locate the blocks of the file. </FONT>
<DT class=description><A id="glo:file metadata" name="glo:file metadata"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file metadata</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Information about a file that is managed by the operating system, but not including the file contents. </FONT>
<DT class=description><A id="glo:file stream" name="glo:file stream"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file stream</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:file descriptor"}'>file descriptor</A></EM>. </FONT>
<DT class=description><A id="glo:file system" name="glo:file system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operating system abstraction that provides persistent, named data. </FONT>
<DT class=description><A id="glo:file system fingerprint" name="glo:file system fingerprint"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>file system fingerprint</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A checksum across the entire file system. </FONT>
<DT class=description><A id=glo:fill-on-demand name=glo:fill-on-demand></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>fill-on-demand</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A method for starting a process before all of its memory is brought in from disk. If the first access to the missing memory triggers a trap to the kernel, the kernel can fill the memory and then resume. </FONT>
<DT class=description><A id="glo:fine-grained locking" name="glo:fine-grained locking"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>fine-grained locking</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A way to increase concurrency by partitioning an object&#8217;s state into different subsets each protected by a different lock. </FONT>
<DT class=description><A id="glo:finished list" name="glo:finished list"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>finished list</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The set of threads that are complete but not yet de-allocated, e.g., because a join may read the return value from the thread control block. </FONT>
<DT class=description><A id=glo:first-in-first-out name=glo:first-in-first-out></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>first-in-first-out</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling policy that performs each task in the order in which it arrives. </FONT>
<DT class=description><A id="glo:flash page failure" name="glo:flash page failure"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>flash page failure</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A flash memory device failure where the data stored on one or more individual pages of flash are lost, but the rest of the flash continues to operate correctly. </FONT>
<DT class=description><A id="glo:flash translation layer" name="glo:flash translation layer"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>flash translation layer</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A layer that maps logical flash pages to different physical pages on the flash device. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:FTL"}'>FTL</A></EM>. </FONT>
<DT class=description><A id="glo:flash wear out" name="glo:flash wear out"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>flash wear out</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">After some number of program-erase cycles, a given flash storage cell may no longer be able to reliably store information. </FONT>
<DT class=description><A id="glo:fork-join parallelism" name="glo:fork-join parallelism"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>fork-join parallelism</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A type of parallel programming where threads can be created (forked) to do work in parallel with a parent thread; a parent may asynchronously wait for a child thread to finish (join). </FONT>
<DT class=description><A id="glo:free space map" name="glo:free space map"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>free space map</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file system data structure used to track which storage blocks are free and which are in use. </FONT>
<DT class=description><A id=glo:FTL name=glo:FTL></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>FTL</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:flash translation layer"}'>flash translation layer</A></EM>. </FONT>
<DT class=description><A id="glo:full disk failure" name="glo:full disk failure"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>full disk failure</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When a disk device stops being able to service reads or writes to all sectors. </FONT>
<DT class=description><A id="glo:full flash drive failure" name="glo:full flash drive failure"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>full flash drive failure</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When a flash device stops being able to service reads or writes to all memory pages. </FONT>
<DT class=description><A id="glo:fully associative cache" name="glo:fully associative cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>fully associative cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Any entry in the cache can hold any memory location, so on a lookup, the system must check the address against all of the entries in the cache to determine if there is a cache hit. </FONT>
<DT class=description><A id="glo:gang scheduling" name="glo:gang scheduling"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>gang scheduling</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling policy for multiprocessors that performs all of the runnable tasks for a particular process at the same time. </FONT>
<DT class=description><A id="glo:Global Descriptor Table" name="glo:Global Descriptor Table"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Global Descriptor Table</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The x86 terminology for a segment table for shared segments. A Local Descriptor Table is used for segments that are private to the process. </FONT>
<DT class=description><A id="glo:grace period" name="glo:grace period"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>grace period</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">For a shared object protected by a read-copy-update lock, the time from when a new version of a shared object is published until the last reader of the old version is guaranteed to be finished. </FONT>
<DT class=description><A id="glo:green threads" name="glo:green threads"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>green threads</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A thread system implemented entirely at user-level without any reliance on operating system kernel services, other than those designed for single-threaded processes. </FONT>
<DT class=description><A id="glo:group commit" name="glo:group commit"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>group commit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A technique that batches multiple transaction commits into a single disk operation. </FONT>
<DT class=description><A id="glo:guest operating system" name="glo:guest operating system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>guest operating system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operating system running in a virtual machine. </FONT>
<DT class=description><A id="glo:hard link" name="glo:hard link"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>hard link</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The mapping between a file name and the underlying file, typically when there are multiple path names for the same underlying file. </FONT>
<DT class=description><A id="glo:hardware abstraction layer" name="glo:hardware abstraction layer"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>hardware abstraction layer</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A module in the operating system that hides the specifics of different hardware implementations. Above this layer, the operating system is portable. </FONT>
<DT class=description><A id="glo:hardware timer" name="glo:hardware timer"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>hardware timer</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware device that can cause a processor interrupt after some delay, either in time or in instructions executed. </FONT>
<DT class=description><A id=glo:head name=glo:head></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>head</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The component that writes the data to or reads the data from a spinning disk surface. </FONT>
<DT class=description><A id="glo:head crash" name="glo:head crash"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>head crash</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An error where the disk head physically scrapes the magnetic surface of a spinning disk surface. </FONT>
<DT class=description><A id="glo:head switch time" name="glo:head switch time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>head switch time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time it takes to re-position the disk arm over the corresponding track on a different surface, before a read or write can begin. </FONT>
<DT class=description><A id=glo:heap name=glo:heap></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>heap</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Space to store dynamically allocated data structures. </FONT>
<DT class=description><A id="glo:heavy-tailed distribution" name="glo:heavy-tailed distribution"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>heavy-tailed distribution</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A probability distribution such that events far from the mean value (in aggregate) occur with significant probability. When used for the distribution of time between events, the remaining time to the next event is positively related to the time already spent waiting &#8212; you expect to wait longer the longer you have already waited. </FONT>
<DT class=description><A id=glo:Heisenbugs name=glo:Heisenbugs></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Heisenbugs</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Bugs in concurrent programs that disappear or change behavior when you try to examine them. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Bohrbugs"}'>Bohrbugs</A></EM>. </FONT>
<DT class=description><A id=glo:hint name=glo:hint></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>hint</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A result of some computation whose results may no longer be valid, but where using an invalid hint will trigger an exception. </FONT>
<DT class=description><A id="glo:home directory" name="glo:home directory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>home directory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The sub-directory containing a user&#8217;s files. </FONT>
<DT class=description><A id="glo:host operating system" name="glo:host operating system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>host operating system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operating system that provides the abstraction of a virtual machine, to run another operating system as an application. </FONT>
<DT class=description><A id="glo:host transfer time" name="glo:host transfer time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>host transfer time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time to transfer data between the host&#8217;s memory and the disk&#8217;s buffer. </FONT>
<DT class=description><A id=glo:hyperthreading name=glo:hyperthreading></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>hyperthreading</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:simultaneous multi-threading"}'>simultaneous multi-threading</A></EM>. </FONT>
<DT class=description><A id="glo:I/O-bound task" name="glo:I/O-bound task"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>I/O-bound task</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A task that primarily does I/O, and does little processing. </FONT>
<DT class=description><A id=glo:idempotent name=glo:idempotent></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>idempotent</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operation that has the same effect whether executed once or many times. </FONT>
<DT class=description><A id="glo:incremental checkpoint" name="glo:incremental checkpoint"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>incremental checkpoint</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A consistent snapshot of the portion of process memory that has been modified since the previous checkpoint. </FONT>
<DT class=description><A id="glo:independent threads" name="glo:independent threads"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>independent threads</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Threads that operate on completely separate subsets of process memory. </FONT>
<DT class=description><A id="glo:indirect block" name="glo:indirect block"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>indirect block</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A storage block containing pointers to file data blocks. </FONT>
<DT class=description><A id=glo:inode name=glo:inode></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>inode</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In the Unix Fast File System (FFS) and related file systems, an inode stores a file&#8217;s metadata, including an array of pointers that can be used to find all of the file&#8217;s blocks. The term inode is sometimes used more generally to refer to any file system&#8217;s per-file metadata data structure. </FONT>
<DT class=description><A id="glo:inode array" name="glo:inode array"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>inode array</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The fixed location on disk containing all of the file system&#8217;s inodes. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:inumber"}'>inumber</A></EM>. </FONT>
<DT class=description><A id=glo:intentions name=glo:intentions></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>intentions</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The set of writes that a transaction will perform if the transaction commits. </FONT>
<DT class=description><A id="glo:internal fragmentation" name="glo:internal fragmentation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>internal fragmentation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">With paged allocation of memory, the unusable memory at the end of a page because a process can only be allocated memory in page-sized chunks. </FONT>
<DT class=description><A id=glo:interrupt name=glo:interrupt></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>interrupt</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An asynchronous signal to the processor that some external event has occurred that may require its attention. </FONT>
<DT class=description><A id="glo:interrupt disable" name="glo:interrupt disable"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>interrupt disable</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A privileged hardware instruction to temporarily defer any hardware interrupts, to allow the kernel to complete a critical task. </FONT>
<DT class=description><A id="glo:interrupt enable" name="glo:interrupt enable"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>interrupt enable</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A privileged hardware instruction to resume hardware interrupts, after a non-interruptible task is completed. </FONT>
<DT class=description><A id="glo:interrupt handler" name="glo:interrupt handler"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>interrupt handler</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A kernel procedure invoked when an interrupt occurs. </FONT>
<DT class=description><A id="glo:interrupt stack" name="glo:interrupt stack"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>interrupt stack</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A region of memory for holding the stack of the kernel&#8217;s interrupt handler. When an interrupt, processor exception, or system call trap causes a context switch into the kernel, the hardware changes the stack pointer to point to the base of the kernel&#8217;s interrupt stack. </FONT>
<DT class=description><A id="glo:interrupt vector table" name="glo:interrupt vector table"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>interrupt vector table</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A table of pointers in the operating system kernel, indexed by the type of interrupt, with each entry pointing to the first instruction of a handler procedure for that interrupt. </FONT>
<DT class=description><A id=glo:inumber name=glo:inumber></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>inumber</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The index into the inode array for a particular file. </FONT>
<DT class=description><A id="glo:inverted page table" name="glo:inverted page table"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>inverted page table</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hash table used for translation between virtual page numbers and physical page frames. </FONT>
<DT class=description><A id="glo:kernel thread" name="glo:kernel thread"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>kernel thread</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A thread that is implemented inside the operating system kernel. </FONT>
<DT class=description><A id="glo:kernel-mode operation" name="glo:kernel-mode operation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>kernel-mode operation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The processor executes in an unrestricted mode that gives the operating system full control over the hardware. Compare: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:user-mode operation"}'>user-mode operation</A></EM>. </FONT>
<DT class=description><A id=glo:LBA name=glo:LBA></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>LBA</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:logical block address"}'>logical block address</A></EM>. </FONT>
<DT class=description><A id="glo:least frequently used" name="glo:least frequently used"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>least frequently used</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A cache replacement policy that evicts whichever block has been used the least often, over some period of time. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:LFU"}'>LFU</A></EM>. </FONT>
<DT class=description><A id="glo:least recently used" name="glo:least recently used"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>least recently used</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A cache replacement policy that evicts whichever block has not been used for the longest period of time. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:LRU"}'>LRU</A></EM>. </FONT>
<DT class=description><A id=glo:LFU name=glo:LFU></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>LFU</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:least frequently used"}'>least frequently used</A></EM>. </FONT>
<DT class=description><A id="glo:Little's Law" name="glo:Little's Law"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Little&#8217;s Law</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In a stable system where the arrival rate matches the departure rate, the number of tasks in the system equals the system&#8217;s throughput multiplied by the average time a task spends in the system: N = XR. </FONT>
<DT class=description><A id="glo:liveness property" name="glo:liveness property"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>liveness property</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A constraint on program behavior such that it always produces a result. Compare: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:safety property"}'>safety property</A></EM>. </FONT>
<DT class=description><A id="glo:locality heuristic" name="glo:locality heuristic"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>locality heuristic</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file system block allocation policy that places files in nearby disk sectors if they are likely to be read or written at the same time. </FONT>
<DT class=description><A id=glo:lock name=glo:lock></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>lock</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A type of synchronization variable used for enforcing atomic, mutually exclusive access to shared data. </FONT>
<DT class=description><A id="glo:lock ordering" name="glo:lock ordering"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>lock ordering</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A widely used approach to prevent deadlock, where locks are acquired in a pre-determined order. </FONT>
<DT class=description><A id="glo:lock-free data structures" name="glo:lock-free data structures"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>lock-free data structures</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrent data structure that guarantees progress for some thread: some method will finish in a finite number of steps, regardless of the state of other threads executing in the data structure. </FONT>
<DT class=description><A id=glo:log name=glo:log></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>log</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An ordered sequence of steps saved to persistent storage. </FONT>
<DT class=description><A id="glo:logical block address" name="glo:logical block address"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>logical block address</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A unique identifier for each disk sector or flash memory block, typically numbered from 1 to the size of the disk/flash device. The disk interface converts this identifier to the physical location of the sector/block. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:LBA"}'>LBA</A></EM>. </FONT>
<DT class=description><A id="glo:logical separation" name="glo:logical separation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>logical separation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A backup storage policy where the backup is stored at the same location as the primary storage, but with restricted access, e.g., to prevent updates. </FONT>
<DT class=description><A id=glo:LRU name=glo:LRU></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>LRU</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:least recently used"}'>least recently used</A></EM>. </FONT>
<DT class=description><A id="glo:master file table" name="glo:master file table"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>master file table</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In NTFS, an array of records storing metadata about each file. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:MFT"}'>MFT</A></EM>. </FONT>
<DT class=description><A id="glo:maximum seek time" name="glo:maximum seek time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>maximum seek time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time it takes to move the disk arm from the innermost track to the outermost one or vice versa. </FONT>
<DT class=description><A id="glo:max-min fairness" name="glo:max-min fairness"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>max-min fairness</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling objective to maximize the minimum resource allocation given to each task. </FONT>
<DT class=description><A id="glo:MCS lock" name="glo:MCS lock"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MCS lock</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An efficient spinlock implementation where each waiting thread spins on a separate memory location. </FONT>
<DT class=description><A id="glo:mean time to data loss" name="glo:mean time to data loss"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>mean time to data loss</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The expected time until a RAID system suffers an unrecoverable error. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:MTTDL"}'>MTTDL</A></EM>. </FONT>
<DT class=description><A id="glo:mean time to failure" name="glo:mean time to failure"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>mean time to failure</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The average time that a system runs without failing. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:MTTF"}'>MTTF</A></EM>. </FONT>
<DT class=description><A id="glo:mean time to repair" name="glo:mean time to repair"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>mean time to repair</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The average time that it takes to repair a system once it has failed. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:MTTR"}'>MTTR</A></EM>. </FONT>
<DT class=description><A id="glo:memory address alias" name="glo:memory address alias"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>memory address alias</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Two or more virtual addresses that refer to the same physical memory location. </FONT>
<DT class=description><A id="glo:memory barrier" name="glo:memory barrier"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>memory barrier</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An instruction that prevents the compiler and hardware from reordering memory accesses across the barrier &#8212; no accesses before the barrier are moved after the barrier and no accesses after the barrier are moved before the barrier. </FONT>
<DT class=description><A id="glo:memory protection" name="glo:memory protection"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>memory protection</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Hardware or software-enforced limits so that each application process can read and write only its own memory and not the memory of the operating system or any other process. </FONT>
<DT class=description><A id="glo:memoryless property" name="glo:memoryless property"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>memoryless property</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">For a probability distribution for the time between events, the remaining time to the next event does not depend on the amount of time already spent waiting. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:exponential distribution"}'>exponential distribution</A></EM>. </FONT>
<DT class=description><A id="glo:memory-mapped file" name="glo:memory-mapped file"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>memory-mapped file</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file whose contents appear to be a memory segment in a process&#8217;s virtual address space. </FONT>
<DT class=description><A id="glo:memory-mapped I/O" name="glo:memory-mapped I/O"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>memory-mapped I/O</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Each I/O device&#8217;s control registers are mapped to a range of physical addresses on the memory bus. </FONT>
<DT class=description><A id=glo:memristor name=glo:memristor></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>memristor</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A type of solid-state persistent storage using a circuit element whose resistance depends on the amounts and directions of currents that have flowed through it in the past. </FONT>
<DT class=description><A id=glo:MFQ name=glo:MFQ></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MFQ</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multi-level feedback queue"}'>multi-level feedback queue</A></EM>. </FONT>
<DT class=description><A id=glo:MFT name=glo:MFT></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MFT</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:master file table"}'>master file table</A></EM>. </FONT>
<DT class=description><A id=glo:microkernel name=glo:microkernel></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>microkernel</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operating system design where the kernel itself is kept small, and instead most of the functionality of a traditional operating system kernel is put into a set of user-level processes, or servers, accessed from user applications via interprocess communication. </FONT>
<DT class=description><A id="glo:MIN cache replacement" name="glo:MIN cache replacement"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MIN cache replacement</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:optimal cache replacement"}'>optimal cache replacement</A></EM>. </FONT>
<DT class=description><A id="glo:minimum seek time" name="glo:minimum seek time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>minimum seek time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time to move the disk arm to the next adjacent track. </FONT>
<DT class=description><A id=glo:MIPS name=glo:MIPS></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MIPS</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An early measure of processor performance: millions of instructions per second. </FONT>
<DT class=description><A id=glo:mirroring name=glo:mirroring></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>mirroring</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system for redundantly storing data on disk where each block of data is stored on two disks and can be read from either. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:RAID 1"}'>RAID 1</A></EM>. </FONT>
<DT class=description><A id=glo:model name=glo:model></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>model</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A simplification that tries to capture the most important aspects of a more complex system&#8217;s behavior. </FONT>
<DT class=description><A id="glo:monolithic kernel" name="glo:monolithic kernel"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>monolithic kernel</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operating system design where most of the operating system functionality is linked together inside the kernel. </FONT>
<DT class=description><A id="glo:Moore's Law" name="glo:Moore's Law"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Moore&#8217;s Law</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Transistor density increases exponentially over time. Similar exponential improvements have occurred in many other component technologies; in the popular press, these often go by the same term. </FONT>
<DT class=description><A id=glo:mount name=glo:mount></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>mount</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A mapping of a path in the existing file system to the root directory of another file system volume. </FONT>
<DT class=description><A id=glo:MTTDL name=glo:MTTDL></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MTTDL</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:mean time to data loss"}'>mean time to data loss</A></EM>. </FONT>
<DT class=description><A id=glo:MTTF name=glo:MTTF></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MTTF</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:mean time to failure"}'>mean time to failure</A></EM>. </FONT>
<DT class=description><A id=glo:MTTR name=glo:MTTR></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MTTR</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:mean time to repair"}'>mean time to repair</A></EM>. </FONT>
<DT class=description><A id="glo:multi-level feedback queue" name="glo:multi-level feedback queue"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multi-level feedback queue</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling algorithm with multiple priority levels managed using round robin queues, where a task is moved between priority levels based on how much processing time it has used. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:MFQ"}'>MFQ</A></EM>. </FONT>
<DT class=description><A id="glo:multi-level index" name="glo:multi-level index"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multi-level index</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A tree data structure to keep track of the disk location of each data block in a file. </FONT>
<DT class=description><A id="glo:multi-level paged segmentation" name="glo:multi-level paged segmentation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multi-level paged segmentation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A virtual memory mechanism where physical memory is allocated in page frames, virtual addresses are segmented, and each segment is translated to physical addresses through multiple levels of page tables. </FONT>
<DT class=description><A id="glo:multi-level paging" name="glo:multi-level paging"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multi-level paging</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A virtual memory mechanism where physical memory is allocated in page frames, and virtual addresses are translated to physical addresses through multiple levels of page tables. </FONT>
<DT class=description><A id="glo:multiple independent requests" name="glo:multiple independent requests"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multiple independent requests</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A necessary condition for deadlock to occur: a thread first acquires one resource and then tries to acquire another. </FONT>
<DT class=description><A id="glo:multiprocessor scheduling policy" name="glo:multiprocessor scheduling policy"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multiprocessor scheduling policy</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A policy to determine how many processors to assign each process. </FONT>
<DT class=description><A id=glo:multiprogramming name=glo:multiprogramming></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multiprogramming</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multitasking"}'>multitasking</A></EM>. </FONT>
<DT class=description><A id=glo:multitasking name=glo:multitasking></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multitasking</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The ability of an operating system to run multiple applications at the same time, also called multiprogramming. </FONT>
<DT class=description><A id="glo:multi-threaded process" name="glo:multi-threaded process"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multi-threaded process</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A process with multiple threads. </FONT>
<DT class=description><A id="glo:multi-threaded program" name="glo:multi-threaded program"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>multi-threaded program</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A generalization of a single-threaded program. Instead of only one logical sequence of steps, the program has multiple sequences, or threads, executing at the same time. </FONT>
<DT class=description><A id="glo:mutual exclusion" name="glo:mutual exclusion"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>mutual exclusion</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When one thread uses a lock to prevent concurrent access to a shared data structure. </FONT>
<DT class=description><A id="glo:mutually recursive locking" name="glo:mutually recursive locking"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>mutually recursive locking</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A deadlock condition where two shared objects call into each other while still holding their locks. Deadlock occurs if one thread holds the lock on the first object and calls into the second, while the other thread holds the lock on the second object and calls into the first. </FONT>
<DT class=description><A id="glo:named data" name="glo:named data"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>named data</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Data that can be accessed by a human-readable identifier, such as a file name. </FONT>
<DT class=description><A id="glo:native command queueing" name="glo:native command queueing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>native command queueing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:tagged command queueing"}'>tagged command queueing</A></EM>. </FONT>
<DT class=description><A id=glo:NCQ name=glo:NCQ></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>NCQ</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:native command queueing"}'>native command queueing</A></EM>. </FONT>
<DT class=description><A id="glo:nested waiting" name="glo:nested waiting"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>nested waiting</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A deadlock condition where one shared object calls into another shared object while holding the first object&#8217;s lock, and then waits on a condition variable. Deadlock results if the thread that can signal the condition variable needs the first lock to make progress. </FONT>
<DT class=description><A id="glo:network effect" name="glo:network effect"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>network effect</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The increase in value of a product or service based on the number of other people who have adopted that technology and not just its intrinsic capabilities. </FONT>
<DT class=description><A id="glo:no preemption" name="glo:no preemption"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>no preemption</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A necessary condition for deadlock to occur: once a thread acquires a resource, its ownership cannot be revoked until the thread acts to release it. </FONT>
<DT class=description><A id="glo:non-blocking data structure" name="glo:non-blocking data structure"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>non-blocking data structure</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrent data structure where a thread is never required to wait for another thread to complete its operation. </FONT>
<DT class=description><A id="glo:non-recoverable read error" name="glo:non-recoverable read error"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>non-recoverable read error</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When sufficient bit errors occur within a disk sector or flash memory page, such that the original data cannot be recovered even after error correction. </FONT>
<DT class=description><A id="glo:non-resident attribute" name="glo:non-resident attribute"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>non-resident attribute</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In NTFS, an attribute record whose contents are addressed indirectly, through extent pointers in the master file table that point to the contents in those extents. </FONT>
<DT class=description><A id="glo:non-volatile storage" name="glo:non-volatile storage"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>non-volatile storage</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Unlike DRAM, memory that is durable and retains its state across crashes and power outages. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:persistent storage"}'>persistent storage</A></EM>. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:stable storage"}'>stable storage</A></EM>. </FONT>
<DT class=description><A id="glo:not recently used" name="glo:not recently used"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>not recently used</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A cache replacement policy that evicts some block that has not been referenced recently, rather than the least recently used block. </FONT>
<DT class=description><A id="glo:oblivious scheduling" name="glo:oblivious scheduling"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>oblivious scheduling</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling policy where the operating system assigns threads to processors without knowledge of the intent of the parallel application. </FONT>
<DT class=description><A id="glo:open system" name="glo:open system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>open system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system whose source code is available to the public for modification and reuse, or a system whose interfaces are defined by a public standards process. </FONT>
<DT class=description><A id="glo:operating system" name="glo:operating system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>operating system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A layer of software that manages a computer&#8217;s resources for its users and their applications. </FONT>
<DT class=description><A id="glo:operating system kernel" name="glo:operating system kernel"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>operating system kernel</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The kernel is the lowest level of software running on the system, with full access to all of the capabilities of the hardware. </FONT>
<DT class=description><A id="glo:optimal cache replacement" name="glo:optimal cache replacement"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>optimal cache replacement</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Replace whichever block is used farthest in the future. </FONT>
<DT class=description><A id=glo:overhead name=glo:overhead></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>overhead</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The added resource cost of implementing an abstraction versus using the underlying hardware resources directly. </FONT>
<DT class=description><A id="glo:ownership design pattern" name="glo:ownership design pattern"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ownership design pattern</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A technique for managing concurrent access to shared objects in which at most one thread owns an object at any time, and therefore the thread can access the shared data without a lock. </FONT>
<DT class=description><A id="glo:page coloring" name="glo:page coloring"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>page coloring</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The assignment of physical page frames to virtual addresses by partitioning frames based on which portions of the cache they will use. </FONT>
<DT class=description><A id="glo:page fault" name="glo:page fault"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>page fault</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware trap to the operating system kernel when a process references a virtual address with an invalid page table entry. </FONT>
<DT class=description><A id="glo:page frame" name="glo:page frame"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>page frame</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An aligned, fixed-size chunk of physical memory that can hold a virtual page. </FONT>
<DT class=description><A id="glo:paged memory" name="glo:paged memory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>paged memory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware address translation mechanism where memory is allocated in aligned, fixed-sized chunks, called pages. Any virtual page can be assigned to any physical page frame. </FONT>
<DT class=description><A id="glo:paged segmentation" name="glo:paged segmentation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>paged segmentation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware mechanism where physical memory is allocated in page frames, but virtual addresses are segmented. </FONT>
<DT class=description><A id="glo:pair of stubs" name="glo:pair of stubs"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>pair of stubs</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A pair of short procedures that mediate between two execution contexts. </FONT>
<DT class=description><A id=glo:paravirtualization name=glo:paravirtualization></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>paravirtualization</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A virtual machine abstraction that allows the guest operating system to make system calls into the host operating system to perform hardware-specific operations, such as changing a page table entry. </FONT>
<DT class=description><A id="glo:parent process" name="glo:parent process"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>parent process</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A process that creates another process. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:child process"}'>child process</A></EM>. </FONT>
<DT class=description><A id=glo:path name=glo:path></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>path</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The string that identifies a file or directory. </FONT>
<DT class=description><A id=glo:PCB name=glo:PCB></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>PCB</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:process control block"}'>process control block</A></EM>. </FONT>
<DT class=description><A id=glo:PCM name=glo:PCM></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>PCM</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:phase change memory"}'>phase change memory</A></EM>. </FONT>
<DT class=description><A id="glo:performance predictability" name="glo:performance predictability"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>performance predictability</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Whether a system&#8217;s response time or other performance metric is consistent over time. </FONT>
<DT class=description><A id="glo:persistent data" name="glo:persistent data"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>persistent data</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Data that is stored until it is explicitly deleted, even if the computer storing it crashes or loses power. </FONT>
<DT class=description><A id="glo:persistent storage" name="glo:persistent storage"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>persistent storage</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:non-volatile storage"}'>non-volatile storage</A></EM>. </FONT>
<DT class=description><A id="glo:phase change behavior" name="glo:phase change behavior"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>phase change behavior</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Abrupt changes in a program&#8217;s working set, causing bursty cache miss rates: periods of low cache misses interspersed with periods of high cache misses. </FONT>
<DT class=description><A id="glo:phase change memory" name="glo:phase change memory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>phase change memory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A type of non-volatile memory that uses the phase of a material to represent a data bit. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:PCM"}'>PCM</A></EM>. </FONT>
<DT class=description><A id="glo:physical address" name="glo:physical address"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>physical address</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An address in physical memory. </FONT>
<DT class=description><A id="glo:physical separation" name="glo:physical separation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>physical separation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A backup storage policy where the backup is stored at a different location than the primary storage. </FONT>
<DT class=description><A id="glo:physically addressed cache" name="glo:physically addressed cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>physically addressed cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A processor cache that is accessed using physical memory addresses. </FONT>
<DT class=description><A id=glo:pin name=glo:pin></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>pin</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">To bind a virtual resource to a physical resource, such as a thread to a processor or a virtual page to a physical page. </FONT>
<DT class=description><A id=glo:platter name=glo:platter></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>platter</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A single thin round plate that stores information in a magnetic disk, often on both surfaces. </FONT>
<DT class=description><A id="glo:policy-mechanism separation" name="glo:policy-mechanism separation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>policy-mechanism separation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system design principle where the implementation of an abstraction is independent of the resource allocation policy of how the abstraction is used. </FONT>
<DT class=description><A id=glo:polling name=glo:polling></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>polling</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An alternative to hardware interrupts, where the processor waits for an asynchronous event to occur, by looping, or busy-waiting, until the event occurs. </FONT>
<DT class=description><A id=glo:portability name=glo:portability></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>portability</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The ability of software to work across multiple hardware platforms. </FONT>
<DT class=description><A id="glo:precise interrupts" name="glo:precise interrupts"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>precise interrupts</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">All instructions that occur before the interrupt or exception, according to the program execution, are completed by the hardware before the interrupt handler is invoked. </FONT>
<DT class=description><A id=glo:preemption name=glo:preemption></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>preemption</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When a scheduler takes the processor away from one task and gives it to another. </FONT>
<DT class=description><A id="glo:preemptive multi-threading" name="glo:preemptive multi-threading"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>preemptive multi-threading</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The operating system scheduler may switch out a running thread, e.g., on a timer interrupt, without any explicit action by the thread to relinquish control at that point. </FONT>
<DT class=description><A id=glo:prefetch name=glo:prefetch></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>prefetch</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">To bring data into a cache before it is needed. </FONT>
<DT class=description><A id="glo:principle of least privilege" name="glo:principle of least privilege"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>principle of least privilege</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">System security and reliability are enhanced if each part of the system has exactly the privileges it needs to do its job and no more. </FONT>
<DT class=description><A id="glo:priority donation" name="glo:priority donation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>priority donation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A solution to priority inversion: when a thread waits for a lock held by a lower priority thread, the lock holder is temporarily increased to the waiter&#8217;s priority until the lock is released. </FONT>
<DT class=description><A id="glo:priority inversion" name="glo:priority inversion"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>priority inversion</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling anomaly that occurs when a high priority task waits indefinitely for a resource (such as a lock) held by a low priority task, because the low priority task is waiting in turn for a resource (such as the processor) held by a medium priority task. </FONT>
<DT class=description><A id=glo:privacy name=glo:privacy></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>privacy</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Data stored on a computer is only accessible to authorized users. </FONT>
<DT class=description><A id="glo:privileged instruction" name="glo:privileged instruction"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>privileged instruction</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Instruction available in kernel mode but not in user mode. </FONT>
<DT class=description><A id=glo:process name=glo:process></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>process</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The execution of an application program with restricted rights &#8212; the abstraction for protection provided by the operating system kernel. </FONT>
<DT class=description><A id="glo:process control block" name="glo:process control block"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>process control block</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A data structure that stores all the information the operating system needs about a particular process: e.g., where it is stored in memory, where its executable image is on disk, which user asked it to start executing, and what privileges the process has. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:PCB"}'>PCB</A></EM>. </FONT>
<DT class=description><A id="glo:process migration" name="glo:process migration"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>process migration</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The ability to take a running program on one system, stop its execution, and resume it on a different machine. </FONT>
<DT class=description><A id="glo:processor exception" name="glo:processor exception"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>processor exception</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware event caused by user program behavior that causes a transfer of control to a kernel handler. For example, attempting to divide by zero causes a processor exception in many architectures. </FONT>
<DT class=description><A id="glo:processor scheduling policy" name="glo:processor scheduling policy"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>processor scheduling policy</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When there are more runnable threads than processors, the policy that determines which threads to run first. </FONT>
<DT class=description><A id="glo:processor status register" name="glo:processor status register"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>processor status register</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware register containing flags that control the operation of the processor, including the privilege level. </FONT>
<DT class=description><A id="glo:producer-consumer communication" name="glo:producer-consumer communication"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>producer-consumer communication</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Interprocess communication where the output of one process is the input of another. </FONT>
<DT class=description><A id="glo:proprietary system" name="glo:proprietary system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>proprietary system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system that is under the control of a single company; it can be changed at any time by its provider to meet the needs of its customers. </FONT>
<DT class=description><A id=glo:protection name=glo:protection></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>protection</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The isolation of potentially misbehaving applications and users so that they do not corrupt other applications or the operating system itself. </FONT>
<DT class=description><A id=glo:publish name=glo:publish></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>publish</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">For a read-copy-update lock, a single, atomic memory write that updates a shared object protected by the lock. The write allows new reader threads to observe the new version of the object. </FONT>
<DT class=description><A id="glo:queueing delay" name="glo:queueing delay"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>queueing delay</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time a task waits in line without receiving service. </FONT>
<DT class=description><A id=glo:quiescent name=glo:quiescent></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>quiescent</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">For a read-copy-update lock, no reader thread that was active at the time of the last modification is still active. </FONT>
<DT class=description><A id="glo:race condition" name="glo:race condition"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>race condition</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When the behavior of a program relies on the interleaving of operations of different threads. </FONT>
<DT class=description><A id=glo:RAID name=glo:RAID></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RAID</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A Redundant Array of Inexpensive Disks (RAID) is a system that spreads data redundantly across multiple disks in order to tolerate individual disk failures. </FONT>
<DT class=description><A id="glo:RAID 1" name="glo:RAID 1"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RAID 1</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:mirroring"}'>mirroring</A></EM>. </FONT>
<DT class=description><A id="glo:RAID 5" name="glo:RAID 5"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RAID 5</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:rotating parity"}'>rotating parity</A></EM>. </FONT>
<DT class=description><A id="glo:RAID 6" name="glo:RAID 6"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RAID 6</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:dual redundancy array"}'>dual redundancy array</A></EM>. </FONT>
<DT class=description><A id="glo:RAID strip" name="glo:RAID strip"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RAID strip</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A set of several sequential blocks placed on one disk by a RAID block placement algorithm. </FONT>
<DT class=description><A id="glo:RAID stripe" name="glo:RAID stripe"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RAID stripe</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A set of RAID strips and their parity strip. </FONT>
<DT class=description><A id=glo:R-CSCAN name=glo:R-CSCAN></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>R-CSCAN</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A variation of the CSCAN disk scheduling policy in which the disk takes into account rotation time. </FONT>
<DT class=description><A id=glo:RCU name=glo:RCU></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RCU</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:read-copy-update"}'>read-copy-update</A></EM>. </FONT>
<DT class=description><A id="glo:read disturb error" name="glo:read disturb error"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>read disturb error</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Reading a flash memory cell a large number of times can cause the data in surrounding cells to become corrupted. </FONT>
<DT class=description><A id=glo:read-copy-update name=glo:read-copy-update></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>read-copy-update</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A synchronization abstraction that allows concurrent access to a data structure by multiple readers and a single writer at a time. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:RCU"}'>RCU</A></EM>. </FONT>
<DT class=description><A id="glo:readers/writers lock" name="glo:readers/writers lock"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>readers/writers lock</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A lock which allows multiple &#8220;reader&#8221; threads to access shared data concurrently provided they never modify the shared data, but still provides mutual exclusion whenever a &#8220;writer&#8221; thread is reading or modifying the shared data. </FONT>
<DT class=description><A id="glo:ready list" name="glo:ready list"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ready list</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The set of threads that are ready to be run but which are not currently running. </FONT>
<DT class=description><A id="glo:real-time constraint" name="glo:real-time constraint"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>real-time constraint</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The computation must be completed by a deadline if it is to have value. </FONT>
<DT class=description><A id="glo:recoverable virtual memory" name="glo:recoverable virtual memory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>recoverable virtual memory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The abstraction of persistent memory, so that the contents of a memory segment can be restored after a failure. </FONT>
<DT class=description><A id="glo:redo logging" name="glo:redo logging"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>redo logging</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A way of implementing a transaction by recording in a log the set of writes to be executed when the transaction commits. </FONT>
<DT class=description><A id="glo:relative path" name="glo:relative path"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>relative path</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file path name interpreted as beginning with the process&#8217;s current working directory. </FONT>
<DT class=description><A id=glo:reliability name=glo:reliability></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>reliability</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A property of a system that does exactly what it is designed to do. </FONT>
<DT class=description><A id="glo:request parallelism" name="glo:request parallelism"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>request parallelism</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Parallel execution on a server that arises from multiple concurrent requests. </FONT>
<DT class=description><A id="glo:resident attribute" name="glo:resident attribute"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>resident attribute</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In NTFS, an attribute record whose contents are stored directly in the master file table. </FONT>
<DT class=description><A id="glo:response time" name="glo:response time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>response time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time for a task to complete, from when it starts until it is done. </FONT>
<DT class=description><A id=glo:restart name=glo:restart></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>restart</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The resumption of a process from a checkpoint, e.g., after a failure or for debugging. </FONT>
<DT class=description><A id="glo:roll back" name="glo:roll back"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>roll back</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The outcome of a transaction where none of its updates occur. </FONT>
<DT class=description><A id="glo:root directory" name="glo:root directory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>root directory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The top-level directory in a file system. </FONT>
<DT class=description><A id="glo:root inode" name="glo:root inode"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>root inode</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In a copy-on-write file system, the inode table&#8217;s inode: the disk block containing the metadata needed to find the inode table. </FONT>
<DT class=description><A id="glo:rotating parity" name="glo:rotating parity"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>rotating parity</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system for redundantly storing data on disk where the system writes several blocks of data across several disks, protecting those blocks with one redundant block stored on yet another disk. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:RAID 5"}'>RAID 5</A></EM>. </FONT>
<DT class=description><A id="glo:rotational latency" name="glo:rotational latency"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>rotational latency</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Once the disk head has settled on the right track, it must wait for the target sector to rotate under it. </FONT>
<DT class=description><A id="glo:round robin" name="glo:round robin"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>round robin</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling policy that takes turns running each ready task for a limited period before switching to the next task. </FONT>
<DT class=description><A id=glo:R-SCAN name=glo:R-SCAN></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>R-SCAN</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A variation of the SCAN disk scheduling policy in which the disk takes into account rotation time. </FONT>
<DT class=description><A id="glo:safe state" name="glo:safe state"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>safe state</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In the context of deadlock, a state of an execution such that regardless of the sequence of future resource requests, there is at least one safe sequence of decisions as to when to satisfy requests such that all pending and future requests are met. </FONT>
<DT class=description><A id="glo:safety property" name="glo:safety property"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>safety property</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A constraint on program behavior such that it never computes the wrong result. Compare: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:liveness property"}'>liveness property</A></EM>. </FONT>
<DT class=description><A id="glo:sample bias" name="glo:sample bias"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>sample bias</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A measurement error that occurs when some members of a group are less likely to be included than others, and where those members differ in the property being measured. </FONT>
<DT class=description><A id=glo:sandbox name=glo:sandbox></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>sandbox</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A context for executing untrusted code, where protection for the rest of the system is provided in software. </FONT>
<DT class=description><A id=glo:SCAN name=glo:SCAN></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>SCAN</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A disk scheduling policy where the disk arm repeatedly sweeps from the inner to the outer tracks and back again, servicing each pending request whenever the disk head passes that track. </FONT>
<DT class=description><A id="glo:scheduler activations" name="glo:scheduler activations"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>scheduler activations</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A multiprocessor scheduling policy where each application is informed of how many processors it has been assigned and whenever the assignment changes. </FONT>
<DT class=description><A id=glo:scrubbing name=glo:scrubbing></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>scrubbing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A technique for reducing non-recoverable RAID errors by periodically scanning for corrupted disk blocks and reconstructing them from the parity block. </FONT>
<DT class=description><A id="glo:secondary bottleneck" name="glo:secondary bottleneck"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>secondary bottleneck</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A resource with relatively low contention, due to a large amount of queueing at the primary bottleneck. If the primary bottleneck is improved, the secondary bottleneck will have much higher queueing delay. </FONT>
<DT class=description><A id=glo:sector name=glo:sector></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>sector</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The minimum amount of a disk that can be independently read or written. </FONT>
<DT class=description><A id="glo:sector failure" name="glo:sector failure"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>sector failure</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A magnetic disk error where data on one or more individual sectors of a disk are lost, but the rest of the disk continues to operate correctly. </FONT>
<DT class=description><A id="glo:sector sparing" name="glo:sector sparing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>sector sparing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Transparently hiding a faulty disk sector by remapping it to a nearby spare sector. </FONT>
<DT class=description><A id=glo:security name=glo:security></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>security</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A computer&#8217;s operation cannot be compromised by a malicious attacker. </FONT>
<DT class=description><A id="glo:security enforcement" name="glo:security enforcement"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>security enforcement</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The mechanism the operating system uses to ensure that only permitted actions are allowed. </FONT>
<DT class=description><A id="glo:security policy" name="glo:security policy"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>security policy</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">What operations are permitted &#8212; who is allowed to access what data, and who can perform what operations. </FONT>
<DT class=description><A id=glo:seek name=glo:seek></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>seek</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The movement of the disk arm to re-position it over a specific track to prepare for a read or write. </FONT>
<DT class=description><A id=glo:segmentation name=glo:segmentation></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>segmentation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A virtual memory mechanism where addresses are translated by table lookup, where each entry in the table is to a variable-size memory region. </FONT>
<DT class=description><A id="glo:segmentation fault" name="glo:segmentation fault"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>segmentation fault</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An error caused when a process attempts to access memory outside of one of its valid memory regions. </FONT>
<DT class=description><A id="glo:segment-local address" name="glo:segment-local address"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>segment-local address</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An address that is relative to the current memory segment. </FONT>
<DT class=description><A id=glo:self-paging name=glo:self-paging></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>self-paging</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A resource allocation policy for allocating page frames among processes; each page replacement is taken from a page frame already assigned to the process causing the page fault. </FONT>
<DT class=description><A id=glo:semaphore name=glo:semaphore></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>semaphore</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A type of synchronization variable with only two atomic operations, P() and V(). P waits for the value of the semaphore to be positive, and then atomically decrements it. V atomically increments the value, and if any threads are waiting in P, triggers the completion of the P operation. </FONT>
<DT class=description><A id=glo:serializability name=glo:serializability></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>serializability</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The result of any program execution is equivalent to an execution in which requests are processed one at a time in some sequential order. </FONT>
<DT class=description><A id="glo:service time" name="glo:service time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>service time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time it takes to complete a task at a resource, assuming no waiting. </FONT>
<DT class=description><A id="glo:set associative cache" name="glo:set associative cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>set associative cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The cache is partitioned into sets of entries. Each memory location can only be stored in its assigned set, by it can be stored in any cache entry in that set. On a lookup, the system needs to check the address against all the entries in its set to determine if there is a cache hit. </FONT>
<DT class=description><A id=glo:settle name=glo:settle></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>settle</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The fine-grained re-positioning of a disk head after moving to a new track before the disk head is ready to read or write a sector of the new track. </FONT>
<DT class=description><A id="glo:shadow page table" name="glo:shadow page table"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>shadow page table</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A page table for a process inside a virtual machine, formed by constructing the composition of the page table maintained by the guest operating system and the page table maintained by the host operating system. </FONT>
<DT class=description><A id="glo:shared object" name="glo:shared object"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>shared object</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An object (a data structure and its associated code) that can be accessed safely by multiple concurrent threads. </FONT>
<DT class=description><A id=glo:shell name=glo:shell></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>shell</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A job control system implemented as a user-level process. When a user types a command to the shell, it creates a process to run the command. </FONT>
<DT class=description><A id="glo:shortest job first" name="glo:shortest job first"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>shortest job first</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A scheduling policy that performs the task with the least remaining time left to finish. </FONT>
<DT class=description><A id="glo:shortest positioning time first" name="glo:shortest positioning time first"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>shortest positioning time first</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A disk scheduling policy that services whichever pending request can be handled in the minimum amount of time. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:SPTF"}'>SPTF</A></EM>. </FONT>
<DT class=description><A id="glo:shortest seek time first" name="glo:shortest seek time first"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>shortest seek time first</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A disk scheduling policy that services whichever pending request is on the nearest track. Equivalent to shortest positioning time first if rotational positioning is not considered. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:SSTF"}'>SSTF</A></EM>. </FONT>
<DT class=description><A id="glo:SIMD (single instruction multiple data) programming" name="glo:SIMD (single instruction multiple data) programming"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>SIMD (single instruction multiple data) programming</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See data parallel programming </FONT>
<DT class=description><A id="glo:simultaneous multi-threading" name="glo:simultaneous multi-threading"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>simultaneous multi-threading</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware technique where each processor simulates two (or more) virtual processors, alternating between them on a cycle-by-cycle basis. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:hyperthreading"}'>hyperthreading</A></EM>. </FONT>
<DT class=description><A id="glo:single-threaded program" name="glo:single-threaded program"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>single-threaded program</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A program written in a traditional way, with one logical sequence of steps as each instruction follows the previous one. Compare: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multi-threaded program"}'>multi-threaded program</A></EM>. </FONT>
<DT class=description><A id="glo:slip sparing" name="glo:slip sparing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>slip sparing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When remapping a faulty disk sector, remapping the entire sequence of disk sectors between the faulty sector and the spare sector by one slot to preserve sequential access performance. </FONT>
<DT class=description><A id="glo:soft link" name="glo:soft link"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>soft link</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A directory entry that maps one file or directory name to another. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:symbolic link"}'>symbolic link</A></EM>. </FONT>
<DT class=description><A id="glo:software transactional memory (STM)" name="glo:software transactional memory (STM)"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>software transactional memory (STM)</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system for general-purpose transactions for in-memory data structures. </FONT>
<DT class=description><A id="glo:software-loaded TLB" name="glo:software-loaded TLB"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>software-loaded TLB</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware TLB whose entries are installed by software, rather than hardware, on a TLB miss. </FONT>
<DT class=description><A id="glo:solid state storage" name="glo:solid state storage"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>solid state storage</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A persistent storage device with no moving parts; it stores data using electrical circuits. </FONT>
<DT class=description><A id="glo:space sharing" name="glo:space sharing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>space sharing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A multiprocessor allocation policy that assigns different processors to different tasks. </FONT>
<DT class=description><A id="glo:spatial locality" name="glo:spatial locality"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>spatial locality</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Programs tend to reference instructions and data near those that have been recently accessed. </FONT>
<DT class=description><A id=glo:spindle name=glo:spindle></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>spindle</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The axle of rotation of the spinning disk platters making up a disk. </FONT>
<DT class=description><A id=glo:spinlock name=glo:spinlock></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>spinlock</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A lock where a thread waiting for a BUSY&nbsp;lock &#8220;spins&#8221; in a tight loop until some other thread makes it FREE. </FONT>
<DT class=description><A id=glo:SPTF name=glo:SPTF></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>SPTF</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:shortest positioning time first"}'>shortest positioning time first</A></EM>. </FONT>
<DT class=description><A id=glo:SSTF name=glo:SSTF></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>SSTF</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:shortest seek time first"}'>shortest seek time first</A></EM>. </FONT>
<DT class=description><A id="glo:stable property" name="glo:stable property"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>stable property</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A property of a program, such that once the property becomes true in some execution of the program, it will stay true for the remainder of the execution. </FONT>
<DT class=description><A id="glo:stable storage" name="glo:stable storage"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>stable storage</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:non-volatile storage"}'>non-volatile storage</A></EM>. </FONT>
<DT class=description><A id="glo:stable system" name="glo:stable system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>stable system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A queueing system where the arrival rate matches the departure rate. </FONT>
<DT class=description><A id="glo:stack frame" name="glo:stack frame"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>stack frame</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A data structure stored on the stack with storage for one invocation of a procedure: the local variables used by the procedure, the parameters the procedure was called with, and the return address to jump to when the procedure completes. </FONT>
<DT class=description><A id="glo:staged architecture" name="glo:staged architecture"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>staged architecture</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A staged architecture divides a system into multiple subsystems or stages, where each stage includes some state private to the stage and a set of one or more worker threads that operate on that state. </FONT>
<DT class=description><A id=glo:starvation name=glo:starvation></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>starvation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The lack of progress for one task, due to resources given to higher priority tasks. </FONT>
<DT class=description><A id="glo:state variable" name="glo:state variable"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>state variable</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Member variable of a shared object. </FONT>
<DT class=description><A id=glo:STM name=glo:STM></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>STM</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:software transactional memory (STM)"}'>software transactional memory (STM)</A></EM>. </FONT>
<DT class=description><A id="glo:structured synchronization" name="glo:structured synchronization"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>structured synchronization</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A design pattern for writing correct concurrent programs, where concurrent code uses a set of standard synchronization primitives to control access to shared state, and where all routines to access the same shared state are localized to the same logical module. </FONT>
<DT class=description><A id=glo:superpage name=glo:superpage></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>superpage</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A set of contiguous pages in physical memory that map a contiguous region of virtual memory, where the pages are aligned so that they share the same high-order (superpage) address. </FONT>
<DT class=description><A id=glo:surface name=glo:surface></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>surface</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">One side of a disk platter. </FONT>
<DT class=description><A id="glo:surface transfer time" name="glo:surface transfer time"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>surface transfer time</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The time to transfer one or more sequential sectors from (or to) a surface once the disk head begins reading (or writing) the first sector. </FONT>
<DT class=description><A id=glo:swapping name=glo:swapping></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>swapping</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Evicting an entire process from physical memory. </FONT>
<DT class=description><A id="glo:symbolic link" name="glo:symbolic link"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>symbolic link</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:soft link"}'>soft link</A></EM>. </FONT>
<DT class=description><A id="glo:synchronization barrier" name="glo:synchronization barrier"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>synchronization barrier</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A synchronization primitive where n threads operating in parallel check in to the barrier when their work is completed. No thread returns from the barrier until all n check in. </FONT>
<DT class=description><A id="glo:synchronization variable" name="glo:synchronization variable"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>synchronization variable</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A data structure used for coordinating concurrent access to shared state. </FONT>
<DT class=description><A id="glo:system availability" name="glo:system availability"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>system availability</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The probability that a system will be available at any given time. </FONT>
<DT class=description><A id="glo:system call" name="glo:system call"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>system call</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A procedure provided by the kernel that can be called from user level. </FONT>
<DT class=description><A id="glo:system reliability" name="glo:system reliability"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>system reliability</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The probability that a system will continue to be reliable for some specified period of time. </FONT>
<DT class=description><A id="glo:tagged command queueing" name="glo:tagged command queueing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>tagged command queueing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A disk interface that allows the operating system to issue multiple concurrent requests to the disk. Requests are processed and acknowledged out of order. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:native command queueing"}'>native command queueing</A></EM>. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:NCQ"}'>NCQ</A></EM>. </FONT>
<DT class=description><A id="glo:tagged TLB" name="glo:tagged TLB"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>tagged TLB</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A translation lookaside buffer whose entries contain a process ID; only entries for the currently running process are used during translation. This allows TLB entries for a process to remain in the TLB when the process is switched out. </FONT>
<DT class=description><A id=glo:task name=glo:task></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>task</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A user request. </FONT>
<DT class=description><A id=glo:TCB name=glo:TCB></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TCB</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:thread control block"}'>thread control block</A></EM>. </FONT>
<DT class=description><A id=glo:TCQ name=glo:TCQ></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TCQ</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:tagged command queueing"}'>tagged command queueing</A></EM>. </FONT>
<DT class=description><A id="glo:temporal locality" name="glo:temporal locality"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>temporal locality</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Programs tend to reference the same instructions and data that they had recently accessed. </FONT>
<DT class=description><A id="glo:test and test-and-set" name="glo:test and test-and-set"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>test and test-and-set</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An implementation of a spinlock where the waiting processor waits until the lock is FREE&nbsp;before attempting to acquire it. </FONT>
<DT class=description><A id=glo:thrashing name=glo:thrashing></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>thrashing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">When a cache is too small to hold its working set. In this case, most references are cache misses, yet those misses evict data that will be used in the near future. </FONT>
<DT class=description><A id=glo:thread name=glo:thread></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>thread</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A single execution sequence that represents a separately schedulable task. </FONT>
<DT class=description><A id="glo:thread context switch" name="glo:thread context switch"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>thread context switch</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Suspend execution of a currently running thread and resume execution of some other thread. </FONT>
<DT class=description><A id="glo:thread control block" name="glo:thread control block"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>thread control block</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The operating system data structure containing the current state of a thread. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:TCB"}'>TCB</A></EM>. </FONT>
<DT class=description><A id="glo:thread scheduler" name="glo:thread scheduler"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>thread scheduler</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Software that maps threads to processors by switching between running threads and threads that are ready but not running. </FONT>
<DT class=description><A id="glo:thread-safe bounded queue" name="glo:thread-safe bounded queue"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>thread-safe bounded queue</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A bounded queue that is safe to call from multiple concurrent threads. </FONT>
<DT class=description><A id=glo:throughput name=glo:throughput></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>throughput</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The rate at which a group of tasks are completed. </FONT>
<DT class=description><A id="glo:time of check vs. time of use attack" name="glo:time of check vs. time of use attack"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>time of check vs. time of use attack</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A security vulnerability arising when an application can modify the user memory holding a system call parameter (such as a file name), <EM>after</EM> the kernel checks the validity of the parameter, but <EM>before</EM> the parameter is used in the actual implementation of the routine. Often abbreviated TOCTOU. </FONT>
<DT class=description><A id="glo:time quantum" name="glo:time quantum"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>time quantum</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The length of time that a task is scheduled before being preempted. </FONT>
<DT class=description><A id="glo:timer interrupt" name="glo:timer interrupt"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>timer interrupt</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A hardware processor interrupt that signifies a period of elapsed real time. </FONT>
<DT class=description><A id="glo:time-sharing operating system" name="glo:time-sharing operating system"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>time-sharing operating system</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operating system designed to support interactive use of the computer. </FONT>
<DT class=description><A id=glo:TLB name=glo:TLB></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TLB</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:translation lookaside buffer"}'>translation lookaside buffer</A></EM>. </FONT>
<DT class=description><A id="glo:TLB flush" name="glo:TLB flush"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TLB flush</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An operation to remove invalid entries from a TLB, e.g., after a process context switch. </FONT>
<DT class=description><A id="glo:TLB hit" name="glo:TLB hit"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TLB hit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A TLB lookup that succeeds at finding a valid address translation. </FONT>
<DT class=description><A id="glo:TLB miss" name="glo:TLB miss"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TLB miss</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A TLB lookup that fails because the TLB does not contain a valid translation for that virtual address. </FONT>
<DT class=description><A id="glo:TLB shootdown" name="glo:TLB shootdown"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TLB shootdown</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A request to another processor to remove a newly invalid TLB entry. </FONT>
<DT class=description><A id=glo:TOCTOU name=glo:TOCTOU></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>TOCTOU</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:time of check vs. time of use attack"}'>time of check vs. time of use attack</A></EM>. </FONT>
<DT class=description><A id=glo:track name=glo:track></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>track</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A circle of sectors on a disk surface. </FONT>
<DT class=description><A id="glo:track buffer" name="glo:track buffer"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>track buffer</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Memory in the disk controller to buffer the contents of the current track even though those sectors have not yet been requested by the operating system. </FONT>
<DT class=description><A id="glo:track skewing" name="glo:track skewing"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>track skewing</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A staggered alignment of disk sectors to allow sequential reading of sectors on adjacent tracks. </FONT>
<DT class=description><A id=glo:transaction name=glo:transaction></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>transaction</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A group of operations that are applied persistently, atomically as a group or not at all, and independently of other transactions. </FONT>
<DT class=description><A id="glo:translation lookaside buffer" name="glo:translation lookaside buffer"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>translation lookaside buffer</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A small hardware table containing the results of recent address translations. See also: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:TLB"}'>TLB</A></EM>. </FONT>
<DT class=description><A id=glo:trap name=glo:trap></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>trap</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A synchronous transfer of control from a user-level process to a kernel-mode handler. Traps can be caused by processor exceptions, memory protection errors, or system calls. </FONT>
<DT class=description><A id="glo:triple indirect block" name="glo:triple indirect block"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>triple indirect block</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A storage block containing pointers to double indirect blocks. </FONT>
<DT class=description><A id="glo:two-phase locking" name="glo:two-phase locking"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>two-phase locking</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A strategy for acquiring locks needed by a multi-operation request, where no lock can be released before all required locks have been acquired. </FONT>
<DT class=description><A id=glo:uberblock name=glo:uberblock></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>uberblock</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In ZFS, the root of the ZFS storage system. </FONT>
<DT class=description><A id="glo:UNIX exec" name="glo:UNIX exec"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>UNIX exec</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system call on UNIX that causes the current process to bring a new executable image into memory and start it running. </FONT>
<DT class=description><A id="glo:UNIX fork" name="glo:UNIX fork"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>UNIX fork</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system call on UNIX that creates a new process as a complete copy of the parent process. </FONT>
<DT class=description><A id="glo:UNIX pipe" name="glo:UNIX pipe"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>UNIX pipe</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A two-way byte stream communication channel between UNIX processes. </FONT>
<DT class=description><A id="glo:UNIX signal" name="glo:UNIX signal"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>UNIX signal</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An asynchronous notification to a running process. </FONT>
<DT class=description><A id="glo:UNIX stdin" name="glo:UNIX stdin"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>UNIX stdin</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file descriptor set up automatically for a new process to use as its input. </FONT>
<DT class=description><A id="glo:UNIX stdout" name="glo:UNIX stdout"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>UNIX stdout</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A file descriptor set up automatically for a new process to use as its output. </FONT>
<DT class=description><A id="glo:UNIX wait" name="glo:UNIX wait"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>UNIX wait</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A system call that pauses until a child process finishes. </FONT>
<DT class=description><A id="glo:unsafe state" name="glo:unsafe state"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>unsafe state</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">In the context of deadlock, a state of an execution such that there is at least one sequence of future resource requests that leads to deadlock no matter what processing order is tried. </FONT>
<DT class=description><A id=glo:upcall name=glo:upcall></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>upcall</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An event, interrupt, or exception delivered by the kernel to a user-level process. </FONT>
<DT class=description><A id="glo:use bit" name="glo:use bit"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>use bit</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A status bit in a page table entry recording whether the page has been recently referenced. </FONT>
<DT class=description><A id="glo:user-level memory management" name="glo:user-level memory management"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>user-level memory management</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The kernel assigns each process a set of page frames, but how the process uses its assigned memory is left up to the application. </FONT>
<DT class=description><A id="glo:user-level page handler" name="glo:user-level page handler"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>user-level page handler</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An application-specific upcall routine invoked by the kernel on a page fault. </FONT>
<DT class=description><A id="glo:user-level thread" name="glo:user-level thread"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>user-level thread</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A type of application thread where the thread is created, runs, and finishes without calls into the operating system kernel. </FONT>
<DT class=description><A id="glo:user-mode operation" name="glo:user-mode operation"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>user-mode operation</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The processor operates in a restricted mode that limits the capabilities of the executing process. Compare: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:kernel-mode operation"}'>kernel-mode operation</A></EM>. </FONT>
<DT class=description><A id=glo:utilization name=glo:utilization></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>utilization</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The fraction of time a resource is busy. </FONT>
<DT class=description><A id="glo:virtual address" name="glo:virtual address"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>virtual address</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An address that must be translated to produce an address in physical memory. </FONT>
<DT class=description><A id="glo:virtual machine" name="glo:virtual machine"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>virtual machine</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An execution context provided by an operating system that mimics a physical machine, e.g., to run an operating system as an application on top of another operating system. </FONT>
<DT class=description><A id="glo:virtual machine honeypot" name="glo:virtual machine honeypot"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>virtual machine honeypot</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A virtual machine constructed for the purpose of executing suspect code in a safe environment. </FONT>
<DT class=description><A id="glo:virtual machine monitor" name="glo:virtual machine monitor"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>virtual machine monitor</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">See: <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:host operating system"}'>host operating system</A></EM>. </FONT>
<DT class=description><A id="glo:virtual memory" name="glo:virtual memory"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>virtual memory</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The illusion of a nearly infinite amount of physical memory, provided by demand paging of virtual addresses. </FONT>
<DT class=description><A id=glo:virtualization name=glo:virtualization></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>virtualization</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Provide an application with the illusion of resources that are not physically present. </FONT>
<DT class=description><A id="glo:virtually addressed cache" name="glo:virtually addressed cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>virtually addressed cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A processor cache which is accessed using virtual, rather than physical, memory addresses. </FONT>
<DT class=description><A id=glo:volume name=glo:volume></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>volume</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A collection of physical storage blocks that form a logical storage device (e.g., a logical disk). </FONT>
<DT class=description><A id="glo:wait while holding" name="glo:wait while holding"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>wait while holding</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A necessary condition for deadlock to occur: a thread holds one resource while waiting for another. </FONT>
<DT class=description><A id="glo:wait-free data structures" name="glo:wait-free data structures"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>wait-free data structures</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Concurrent data structure that guarantees progress for every thread: every method finishes in a finite number of steps, regardless of the state of other threads executing in the data structure. </FONT>
<DT class=description><A id="glo:waiting list" name="glo:waiting list"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>waiting list</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The set of threads that are waiting for a synchronization event or timer expiration to occur before becoming eligible to be run. </FONT>
<DT class=description><A id="glo:wear leveling" name="glo:wear leveling"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>wear leveling</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A flash memory management policy that moves logical pages around the device to ensure that each physical page is written/erased approximately the same number of times. </FONT>
<DT class=description><A id="glo:web proxy cache" name="glo:web proxy cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>web proxy cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A cache of frequently accessed web pages to speed web access and reduce network traffic. </FONT>
<DT class=description><A id="glo:work-conserving scheduling policy" name="glo:work-conserving scheduling policy"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>work-conserving scheduling policy</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A policy that never leaves the processor idle if there is work to do. </FONT>
<DT class=description><A id="glo:working set" name="glo:working set"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>working set</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The set of memory locations that a program has referenced in the recent past. </FONT>
<DT class=description><A id=glo:workload name=glo:workload></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>workload</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A set of tasks for some system to perform, along with when each task arrives and how long each task takes to complete. </FONT>
<DT class=description><A id="glo:wound wait" name="glo:wound wait"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>wound wait</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">An approach to deadlock recovery that ensures progress by aborting the most recent transaction in any deadlock. </FONT>
<DT class=description><A id="glo:write acceleration" name="glo:write acceleration"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>write acceleration</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">Data to be stored on disk is first written to the disk&#8217;s buffer memory. The write is then acknowledged and completed in the background. </FONT>
<DT class=description><A id="glo:write-back cache" name="glo:write-back cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>write-back cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A cache where updates can be stored in the cache and only sent to memory when the cache runs out of space. </FONT>
<DT class=description><A id="glo:write-through cache" name="glo:write-through cache"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>write-through cache</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A cache where updates are sent immediately to memory. </FONT>
<DT class=description><A id="glo:zero-copy I/O" name="glo:zero-copy I/O"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>zero-copy I/O</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A technique for transferring data across the kernel-user boundary without a memory-to-memory copy, e.g., by manipulating page table entries. </FONT>
<DT class=description><A id=glo:zero-on-reference name=glo:zero-on-reference></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>zero-on-reference</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">A method for clearing memory only if the memory is used, rather than in advance. If the first access to memory triggers a trap to the kernel, the kernel can zero the memory and then resume. </FONT>
<DT class=description><A id="glo:Zipf distribution" name="glo:Zipf distribution"></A><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Zipf distribution</B> </FONT>
<DD class=description><FONT style="BACKGROUND-COLOR: #7be1e1">The relative frequency of an event is inversely proportional to its position in a rank order of popularity.</FONT></DD></DL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-1390008 name=x1-1390008>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1">About the Authors</FONT></I></H2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thomas Anderson</B> holds the Warren Francis and Wilma Kolm Bradley Chair of Computer Science and Engineering at the University of Washington, where he has been teaching computer science since 1997. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Professor Anderson has been widely recognized for his work, receiving the Diane S. McEntyre Award for Excellence in Teaching, the USENIX Lifetime Achievement Award, the IEEE Koji Kobayashi Computers and Communications Award, the ACM SIGOPS Mark Weiser Award, the USENIX Software Tools User Group Award, the IEEE Communications Society William R. Bennett Prize, the NSF Presidential Faculty Fellowship, and the Alfred P. Sloan Research Fellowship. He is an ACM Fellow. He has served as program co-chair of the ACM SIGCOMM Conference and program chair of the ACM Symposium on Operating Systems Principles (SOSP). In 2003, he helped co-found the USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Professor Anderson&#8217;s research interests span all aspects of building practical, robust, and efficient computer systems, including operating systems, distributed systems, computer networks, multiprocessors, and computer security. Over his career, he has authored or co-authored over one hundred peer-reviewed papers; nineteen of his papers have won best paper awards. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Michael Dahlin</B> is a Principal Engineer at Google. Prior to that, from 1996 to 2014, he was a Professor of Computer Science at the University of Texas in Austin, where he taught operating systems and other subjects and where he was awarded the College of Natural Sciences Teaching Excellence Award. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Professor Dahlin&#8217;s research interests include Internet- and large-scale services, fault tolerance, security, operating systems, distributed systems, and storage systems. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Professor Dahlin&#8217;s work has been widely recognized. Over his career, he has authored over seventy peer reviewed papers; ten of which have won best paper awards. He is both an ACM Fellow and an IEEE Fellow, and he has received an Alfred P. Sloan Research Fellowship and an NSF CAREER award. He has served as the program chair of the ACM Symposium on Operating Systems Principles (SOSP), co-chair of the USENIX/ACM Symposium on Networked Systems Design and Implementation (NSDI), and co-chair of the International World Wide Web conference (WWW). </FONT></P><BR><BR><FONT style="BACKGROUND-COLOR: #7be1e1">