<STRONG><FONT style="BACKGROUND-COLOR: #7be1e1" color=blue>Operating Systems: Principles and Practice (Second Edition) Volume II : </FONT></STRONG>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=headers>Title:</SPAN></B><SPAN class=RefText> Operating Systems: Principles and Practice (Second Edition) Volume II : 7. Scheduling</SPAN></FONT></FONT></I></H2></A>
<DIV class=chapterQuote>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Time is money &#8212;<I>Ben Franklin</I> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The best performance improvement is the transition from the non-working state to the working state. That&#8217;s infinite speedup. &#8212;<I>John Ousterhout</I> </FONT></P>
<DL>
<DT><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DD><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DD></DL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
<BR></FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When there are multiple things to do, how do you choose which one to do first? In the last few chapters, we have described how to create threads, switch between them, and synchronize their access to shared data. At any point in time, some threads are running on the system&#8217;s processor. Others are waiting their turn for a processor. Still other threads are blocked waiting for I/O to complete, a condition variable to be signaled, or for a lock to be released. When there are more runnable threads than processors, the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:processor scheduling policy"}'>processor scheduling policy</A></EM> determines which threads to run first. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You might think the answer to this question is easy: just do the work in the order in which it arrives. After all, that seems to be the only fair thing to do. Because it is obviously fair, almost all government services work this way. When you go to your local Department of Motor Vehicles (DMV) to get a driver&#8217;s license, you take a number and wait your turn. Although fair, the DMV often feels slow. There&#8217;s a reason why: as we&#8217;ll see later in this chapter, doing things in order of arrival is sometimes the worst thing you can do in terms of improving user-perceived response time. Advertising that your operating system uses the same scheduling algorithm as the DMV is probably not going to increase your sales! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You might think that the answer to this question is unimportant. With the million-fold improvement in processor performance over the past thirty years, it might seem that we are a million times less likely to have anything waiting for its turn on a processor. We disagree! Server operating systems in particular are often overloaded. Parallel applications can create more work than processors, and if care is not taken in the design of the scheduling policy, performance can badly degrade. There are subtle relationships between scheduling policy and energy management on battery-powered devices such as smartphones and laptops. Further, scheduling issues apply to any scarce resource, whether the source of contention is the processor, memory, disk, or network. We will revisit the issues covered in this chapter throughout the rest of the book. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling policy is not a panacea. Without enough capacity, performance may be poor regardless of which thread we run first. In this chapter, we will also discuss how to predict overload conditions and how to adapt to them. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, you probably have quite a bit of intuition as to impact of different scheduling policies and capacity on issues like response time, fairness, and throughput. Anyone who waits in line probably wonders how we could get the line to go faster. That&#8217;s true whether we&#8217;re waiting in line at the supermarket, a bank, the DMV, or at a popular restaurant. Remarkably, in each of these settings, there is a different approach to how they deal with waiting. We will try to answer why. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There is no one right answer; rather, any scheduling policy poses a complex set of tradeoffs between various desirable properties. The goal of this chapter is not to enumerate all of the interesting possibilities, explore the full design space, or even to identify specific useful policies. Instead, we describe some of the trade-offs and try to illustrate how a designer can approach the problem of selecting a scheduling policy. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider what happens if you are running the web site for a company trying to become the next Facebook. Based on history, you&#8217;ll be able to guess how much server capacity you need to be able to keep up with demand and still have reasonable response time. What happens if your site appears on Slashdot, and suddenly you have twice as many users as you had an hour ago? If you are not careful, everyone will think your site is terribly slow, and permanently go elsewhere. Google, Amazon, and Yahoo have each estimated that they lose approximately 5-10% of their customers if their response time increases by as little as 100 milliseconds. If faced with overload: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Would quickly implementing a different scheduling policy help, or hurt? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How much worse will your performance be if the number of users doubles again? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Should you turn away some users so that others will get acceptable performance? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Does it matter which users you turn away? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If you run out to the local electronics store and buy a server, how much better will performance get? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Do the answers change if you are under a denial-of-service attack by a competitor?</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this chapter, we will try to give you the conceptual and analytic tools to help you answer these questions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Performance terminology</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In Chapter 1 we defined some performance-related terms we will use throughout this chapter and the rest of the book; we summarize those terms here. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Task.</B> A user request. A task is also often called a <EM>job</EM>. A task can be any size, from simply redrawing the screen to show the movement of the mouse cursor to computing the shape of a newly discovered protein. When discussing scheduling, we use the term task, rather than thread or process, because a single thread or process may be responsible for multiple user requests or tasks. For example, in a word processor, each character typed is an individual user request to add that character to the file and display the result on the screen. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Response time (or delay).</B> The user-perceived time to do some task. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Predictability.</B> Low variance in response times for repeated requests. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Throughput.</B> The rate at which tasks are completed. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Scheduling overhead.</B> The time to switch from one task to another. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Fairness.</B> Equality in the number and timeliness of resources given to each task. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Starvation.</B> The lack of progress for one task, due to resources given to a higher priority task.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Chapter roadmap:</B> </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Uniprocessor Scheduling.</B> How do uniprocessor scheduling policies affect fairness, response time, and throughput? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1080001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiprocessor Scheduling.</B> How do scheduling policies change when we have multiple processor cores per computer? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1150002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Energy-Aware Scheduling.</B> Many new computer systems can save energy by turning off portions of the computer, slowing the execution speed. How do we make this tradeoff while minimizing the impact on user perceived response time? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1210003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Real-Time Scheduling.</B> More generally, how do we make sure tasks finish in time? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1220004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Queueing Theory.</B> In a server environment, how are response time and throughput affected by the rate at which requests arrive for processing and by the scheduling policy? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1230005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Overload Management.</B> How do we keep response time reasonable when a system becomes overloaded? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1330006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Case Study: Servers in a Data Center.</B> How do we combine these technologies to manage servers a data center? (Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1340007"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P></LI></UL><A id=x1-107001r179 name=x1-107001r179></A><A id=x1-1080001 name=x1-1080001>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1 Uniprocessor Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">We start by considering one processor, generalizing to multiprocessor scheduling policies in the next section. We begin with three simple policies &#8212; first-in-first-out, shortest-job-first, and round robin &#8212; as a way of illustrating scheduling concepts. Each approach has its own the strengths and weaknesses, and most resource allocation systems (whether for processors, memory, network or disk) combine aspects of all three. At the end of the discussion, we will show how the different approaches can be synthesized into a more practical and complete processor scheduler. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Before proceeding, we need to define a few terms. A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:workload"}'>workload</A></EM> is a set of tasks for some system to perform, along with when each task arrives and how long each task takes to complete. In other words, the workload defines the input to a scheduling algorithm. Given a workload, a processor scheduler decides when each task is to be assigned the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We are interested in scheduling algorithms that work well across a wide variety of environments, because workloads will vary quite a bit from system to system and user to user. Some tasks are <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:compute-bound task"}'>compute-bound</A></EM> and only use the processor. Others, such as a compiler or a web browser, mix I/O and computation. Still others, such as a BitTorrent download, are <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:I/O-bound task"}'>I/O-bound</A></EM>, spending most of their time waiting for I/O and only brief periods computing. In the discussion, we start with very simple compute-bound workloads and then generalize to include mixtures of different types of tasks as we proceed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some of the policies we outline are the best possible policy on a particular metric and workload, and some are the worst possible policy. When discussing optimality and pessimality, we are only comparing to policies that are <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:work-conserving scheduling policy"}'>work-conserving</A></EM>. A scheduler is work-conserving if it never leaves the processor idle if there is work to do. Obviously, a trivially poor policy has the processor sit idle for long periods when there are tasks in the ready list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our discussion also assumes the scheduler has the ability to <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:preemption"}'>preempt</A></EM> the processor and give it to some other task. Preemption can happen either because of a timer interrupt, or because some task arrives on the ready list with a higher priority than the current task, at least according to some scheduling policy. We explained how to switch the processor between tasks in Chapter&nbsp;2 and Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. While much of the discussion is also relevant to non-preemptive schedulers, there are few such systems left, so we leave that issue aside for simplicity. </FONT><A id=x1-108001r173 name=x1-108001r173></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.1 </FONT><A id=x1-1090001 name=x1-1090001></A><FONT style="BACKGROUND-COLOR: #7be1e1">First-In-First-Out (FIFO)</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Perhaps the simplest scheduling algorithm possible is first-in-first-out (FIFO): do each task in the order in which it arrives. (FIFO is sometimes also called first-come-first-served, or FCFS.) When we start working on a task, we keep running it until it finishes. FIFO minimizes overhead, switching between tasks only when each one completes. Because it minimizes overhead, if we have a fixed number of tasks, and those tasks only need the processor, FIFO will have the best throughput: it will complete the most tasks the most quickly. And as we mentioned, FIFO appears to be the definition of fairness &#8212; every task patiently waits its turn. </FONT><A id=x1-1090011 name=x1-1090011></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00416.gif" data-calibre-src="OEBPS/Images/image00416.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.1: </B>Completion times with FIFO (top) and SJF (bottom) scheduling when several short tasks (2-5) arrive immediately after a long task (1).</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, FIFO has a weakness. If a task with very little work to do happens to land in line behind a task that takes a very long time, then the system will seem very inefficient. Figure &nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1090011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates a particularly bad workload for FIFO; it also shows SJF, which we will discuss in a bit. If the first task in the queue takes one second, and the next four arrive an instant later, but each only needs a millisecond of the processor, then they will all need to wait until the first one finishes. The average response time will be over a second, but the optimal average response time is much less than that. In fact, if we ignore switching overhead, there are some workloads where FIFO is literally the worst possible policy for average response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>FIFO and memcached</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although you may think that FIFO is too simple to be useful, there are some important cases where it is exactly the right choice for the workload. One such example is memcached. Many web services, such as Facebook, store their user data in a database. The database provides flexible and consistent lookups, such as, which friends need to be notified of a particular update to a user&#8217;s Facebook wall. In order to improve performance, Facebook and other systems put a cache called memcached in front of the database, so that if a user posts two items to her Facebook wall, the system only needs to lookup the friend list once. The system first checks whether the information is cached, and if so uses that copy. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because almost all requests are for small amounts of data, memcached replies to requests in FIFO order. This minimizes overhead, as there is no need to time slice between requests. For this workload where tasks are roughly equal in size, FIFO is simple, minimizes average response time, and even maximizes throughput. Win-win! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-109002r185 name=x1-109002r185></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.2 </FONT><A id=x1-1100002 name=x1-1100002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Shortest Job First (SJF)</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">If FIFO can be a poor choice for average response time, is there an optimal policy for minimizing average response time? The answer is yes: schedule the shortest job first (SJF). </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we could know how much time each task needed at the processor. (In general, we will not know, so this is not meant as a practical policy! Rather, we use it as a thought experiment; later on, we will see how to approximate SJF in practice.) If we always schedule the task that has the least remaining work to do, that will minimize average response time. (For this reason, some call SJF shortest-remaining-time-first or SRTF.) </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To see that SJF is optimal, consider a hypothetical alternative policy that is not SJF, but that we think might be optimal. Because the alternative is not SJF, at some point it will choose to run a task that is longer than something else in the queue. If we now switch the order of tasks, keeping everything the same, but doing the shorter task first, we will reduce the average response time. Thus, any alternative to SJF cannot be optimal. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1090011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates SJF on the same example we used for FIFO. If a long task is the first to arrive, it will be scheduled (if we are work-conserving). When a short task arrives a bit later, the scheduler will preempt the current task, and start the shorter one. The remaining short tasks will be processed in order of arrival, followed by finishing the long task. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">What counts as &#8220;shortest&#8221; is the remaining time left on the task, not its original length. If we are one nanosecond away from finishing an hour-long task, we will minimize average response time by staying with that task, rather than preempting it for a minute long task that just arrived on the ready list. Of course, if they both arrive at about the same time, doing the minute long task first will dramatically improve average response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Starvation and sample bias</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Systems that might suffer from starvation require extra care when being measured. Suppose you want to compare FIFO and SJF experimentally. You set up two computers, one running each scheduler, and send them the same sequence of tasks. After some period, you stop and report the average response time of completed tasks. If some tasks starve, however, the set of completed tasks will be different for the two policies. We will have excluded the longest tasks from the results for SJF, skewing the average response time even further. Put another way, if you want to manipulate statistics to &#8220;prove&#8221; a point, this is a good trick to use! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How might you redesign the experiment to provide a valid comparison between FIFO and SJF? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Does SJF have any other downsides (other than being impossible to implement because it requires knowledge of the future)? It turns out that SJF is pessimal for variance in response time. By doing the shortest tasks as quickly as possible, SJF necessarily does longer tasks as slowly as possible (among policies that are work-conserving). In other words, there is a fundamental tradeoff between reducing average response time and reducing the variance in average response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Worse, SJF can suffer from starvation and frequent context switches. If enough short tasks arrive, long tasks may never complete. Whenever a new task on the ready list is shorter than the remaining time left on the currently scheduled task, the scheduler will switch to the new task. If this keeps happening indefinitely, a long task may never finish. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose a supermarket manager reads a portion of this textbook and decides to implement shortest job first to reduce average waiting times. The manager tells herself: who cares about variance! A benefit is that there would no longer be any need for express lanes &#8212; if someone has only a few items, she can be immediately whisked to the front of the line, interrupting the parent shopping for eighteen kids. Of course, the wait times of the customers with full baskets skyrockets; if the supermarket is open twenty-four hours a day, customers with the largest purchases might have to wait until 3am to finally get through the line. This would probably lead their best customers to go to the supermarket down the street, not exactly what the manager had in mind! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Customers could also try to game the system: if you have a lot of items to purchase, simply go through the line with one item at a time &#8212; you will always be whisked to the front, at least until everyone else figures out the same dodge. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Shortest Job First and bandwidth-constrained web service</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although SJF may seem completely impractical, there are circumstances where it is exactly the right policy. One example is in a web server for static content. Many small-scale web servers are limited by their bandwidth to the Internet, because it is often more expensive to pay for more capacity. Web pages at most sites vary in size, with most pages being relatively short, while some pages are quite large. The average response time for accessing web pages is dominated by the more frequent requests to short pages, while the bandwidth costs are dominated by the less frequent requests to large pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This combination is almost ideal for using SJF for managing the allocation of network bandwidth by the server. With static pages, it is possible to predict from the name of the page how much bandwidth each request will consume. By transferring short pages first, the web server can ensure that its average response time is very low. Even if most requests are to small pages, the aggregate bandwidth for small pages is low, so requests to large pages are not significantly slowed down. The only difficulty comes when the web server is overloaded, because then the large page requests can be starved. As we will see later, overload situations need their own set of solutions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-110001r187 name=x1-110001r187></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.3 </FONT><A id=x1-1110003 name=x1-1110003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Round Robin</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A policy that addresses starvation is to schedule tasks in a round robin fashion. With Round Robin, tasks take turns running on the processor for a limited period of time. The scheduler assigns the processor to the first task in the ready list, setting a timer interrupt for some delay, called the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:time quantum"}'>time quantum</A></EM>. At the end of the quantum, if the task has not completed, the task is preempted and the processor is given to the next task in the ready list. The preempted task is put back on the ready list where it can wait its next turn. With Round Robin, there is no possibility that a task will starve &#8212; it will eventually reach the front of the queue and get its time quantum. </FONT><A id=x1-1110012 name=x1-1110012></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00417.gif" data-calibre-src="OEBPS/Images/image00417.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.2: </B>Completion times with Round Robin scheduling when short tasks arrive just after a long task, with a time quantum of 1 ms (top) and 100 ms (bottom).</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, we need to pick the time quantum carefully. One consideration is overhead: if we have too short a time quantum, the processor will spend all of its time switching and getting very little useful work done. If we pick too long a time quantum, tasks will have to wait a long time until they get a turn. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1110012"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the behavior of Round Robin, on the same workload as in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1090011"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, for two different values for the time quantum. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A good analogy for Round Robin is a particularly hyperkinetic student, studying for multiple finals simultaneously. You won&#8217;t get much done if you read a paragraph from one textbook, then switch to reading a paragraph from the next textbook, and then switch to yet a third textbook. However, if you never switch, you may never get around to studying for some of your courses. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>What is the overhead of a Round Robin time slice?</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might think that the cost of switching tasks after a time slice is modest: the cost of interrupting the processor, saving its registers, dispatching the timer interrupt handler, and restoring the registers of the new task. On a modern processor, all these steps can be completed in a few tens of microseconds. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, we must also include the impact of time slices on the efficiency of the processor cache. Each newly scheduled task will need to fetch its data from memory into cache, evicting some of the data that had been stored by the previous task. Exactly how long this takes will depend on the memory hierarchy, the reference pattern of the new task, and whether any of its state is still in the cache from its previous time slice. Modern processors often have multiple levels of cache to improve performance. Reloading just the first level on-chip cache from scratch can take several milliseconds; reloading the second and third level caches takes even longer. Thus, it is typical for operating systems to set their time slice interval to be somewhere between 10 and 100 milliseconds, depending on the goals of the system: better responsiveness or reduced overhead. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One way of viewing Round Robin is as a compromise between FIFO and SJF. At one extreme, if the time quantum is infinite (or at least, longer than the longest task), Round Robin behaves exactly the same as FIFO. Each task runs to completion and then yields the processor to the next in line. At the other extreme, suppose it was possible to switch between tasks with zero overhead, so we could choose a time quantum of a single instruction. With fine-grained time slicing, tasks would finish in the order of length, as with SJF, but slower: a task A will complete within a factor of n of when it would have under SJF, where n is the maximum number of other runnable tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Simultaneous multi-threading</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although zero overhead switching may seem far-fetched, most modern processors do a form of it called <EM>simultaneous multi-threading (SMT)</EM> or <EM>hyperthreading</EM>. With SMT, each processor simulates two (or more) virtual processors, alternating between them on a cycle-by-cycle basis. Since most threads need to wait for memory from time to time, another thread can use the processor during those gaps, or vice versa. In normal operation, neither thread is significantly slowed when running on an SMT. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You can test whether your computer implements SMT by testing how fast the processor operates when it has one or more tasks, each running a tight loop of arithmetic operations. (Note that on a multicore system, you will need to create enough tasks to fill up each of the cores, or physical processors, before the system will begin to use SMT.) With one task per physical processor, each task will run at the maximum rate of the processor. With a two-way SMT and two tasks per processor, each task will run at somewhat less than the maximum rate, but each task will run at approximately the same uniform speed. As you increase the number of tasks beyond the SMT level, however, the operating system will begin to use coarse-grained time slicing, so tasks will progress in spurts &#8212; alternating time on and off the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-1110023 name=x1-1110023></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00418.gif" data-calibre-src="OEBPS/Images/image00418.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.3: </B>Completion times with Round Robin (top) versus FIFO and SJF (bottom) when scheduling equal length tasks.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, Round Robin has some weaknesses. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1110023"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates what happens for FIFO, SJF, and Round Robin when several tasks start at roughly same time and are of the same length. Round Robin will rotate through the tasks, doing a bit of each, finishing them all at roughly the same time. This is nearly the worst possible scheduling policy for this workload! FIFO does much better, picking a task and sticking with it until it finishes. Not only does FIFO reduce average response time for this workload relative to Round Robin, no task is worse off under FIFO &#8212; every task finishes at least as early as it would have under Round Robin. Time slicing added overhead without any benefit. Finally, consider what SJF does on this workload. SJF schedules tasks in exactly the same order as FIFO. The first task that arrives will be assigned the processor, and as soon as it executes a single instruction, it will have less time remaining than all of the other tasks, and so it will run to completion. Since we know SJF is optimal for average response time, this means that both FIFO and Round Robin are optimal for some workloads and pessimal for others, just different ones in each case. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Round Robin and streaming video</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Round Robin is sometimes the best policy even when all tasks are roughly the same size. An example is managing the server bandwidth for streaming video. When streaming, response time is much less of a concern than achieving a predictable, stable rate of progress. For this, Round Robin is nearly ideal: all streams progress at the same rate. As long as Round Robin serves the data as fast or faster than the viewer consumes the video stream, the time to completely download the stream is unimportant. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Depending on the time quantum, Round Robin can also be quite poor when running a mixture of I/O-bound and compute-bound tasks. I/O-bound tasks often need very short periods on the processor in order to compute the next I/O operation to issue. Any delay to be scheduled onto the processor can lead to system-wide slowdowns. For example, in a text editor, it often takes only a few milliseconds to echo a keystroke to the screen, a delay much faster than human perception. However, if we are sharing the processor between a text editor and several other tasks using Round Robin, the editor must wait several time quanta to be scheduled for each keystroke &#8212; with a 100 ms time quantum, this can become annoyingly apparent to the user. </FONT><A id=x1-1110034 name=x1-1110034></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00419.gif" data-calibre-src="OEBPS/Images/image00419.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.4: </B>Scheduling behavior with Round Robin when running a mixture of I/O-bound and compute-bound tasks. The I/O-bound task yields the processor when it does I/O. Even though the I/O completes quickly, the I/O-bound task must wait to be reassigned the processor until the compute-bound tasks both complete their time quanta.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1110034"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates similar behavior with a disk-bound task. Suppose we have a task that computes for 1 ms and then uses the disk for 10 ms, in a loop. Running alone, the task can keep the disk almost completely busy. Suppose we also have two compute bound tasks; again, running by themselves, they can keep the processor busy. What happens when we run the disk-bound and compute-bound tasks at the same time? With Round Robin and a time quantum of 100 ms, the disk-bound task slows down by nearly a factor of twenty &#8212; each time it needs the processor, it must wait nearly 200 ms for its turn. SJF on this workload would perform well &#8212; prioritizing short tasks at the processor keeps the disk-bound task busy, while modestly slowing down the compute-bound tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If you have ever tried to surf the web while doing a large BitTorrent download over a slow link, you can see that network operations visibly slow during the download. This is even though your browser may need to transfer only a very small amount of data to provide good responsiveness. The reason is quite similar. Browser packets get their turn, but only after being queued behind a much larger number of packets for the bulk download. Prioritizing the browser&#8217;s packets would have only a minimal impact on the download speed and a large impact on the perceived responsiveness of the system. </FONT><A id=x1-111004r188 name=x1-111004r188></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.4 </FONT><A id=x1-1120004 name=x1-1120004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Max-Min Fairness</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">In many settings, a fair allocation of resources is as important to the design of a scheduler as responsiveness and low overhead. On a multi-user machine or on a server, we do not want to allow a single user to be able to monopolize the resources of the machine, degrading service for other users. While it might seem that fairness has little value in single-user machines, individual applications are often written by different companies, each with an interest in making their application performance look good even if that comes at a cost of degrading responsiveness for other applications. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another complication arises with whether we should allocate resources fairly among users, applications, processes, or threads. Some applications may run inside a single process, while others may create many processes, and each process may involve multiple threads. Round robin among threads can lead to starvation if applications with only a single thread are competing with applications with hundreds of threads. We can be concerned with fair allocation at any of these levels of granularity: threads within a process, processes for a particular user, users sharing a physical machine. For example, we could be concerned with making sure that every thread within a process makes progress. For simplicity, however, our discussion will assume we are interested in providing fairness among processes &#8212; the same principles apply if the unit receiving resources is the user, application, or thread. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fairness is easy if all processes are compute-bound: Round Robin will give each process an equal portion of the processor. In practice, however, different processes consume resources at different rates. An I/O-bound process may need only a small portion of the processor, while a compute-bound process is willing to consume all available processor time. What is a fair allocation when there is a diversity of needs? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One possible answer is to say that whatever Round Robin does is fair &#8212; after all, each process gets an equal chance at the processor. As we saw above, however, Round Robin can result in I/O-bound processes running at a much slower rate than they would if they had the processor to themselves, while compute-bound processes are barely affected at all. That hardly seems fair! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While there are many possible definitions of fairness, a particularly useful one is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:max-min fairness"}'>max-min fairness</A></EM>. Max-min fairness iteratively maximizes the minimum allocation given to a particular process (user, application or thread) until all resources are assigned. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If all processes are compute-bound, the behavior of max-min is simple: we maximize the minimum by giving each process exactly the same share of the processor &#8212; that is, by using Round Robin. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The behavior of max-min fairness is more interesting if some processes cannot use their entire share, for example, because they are short-running or I/O-bound. If so, we give those processes their entire request and redistribute the unused portion to the remaining processes. Some of the processes receiving the extra portion may not be able to use their entire revised share, and so we must iterate, redistributing any unused portion. When no remaining requests can be fully satisfied, we divide the remainder equally among all remaining processes. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the example in the previous section. The disk-bound process needed only 10% of the processor to keep busy, but Round Robin only gave it 0.5% of the processor, while each of the two compute-bound processes received nearly 50%. Max-min fairness would assign 10% of the processor to the I/O-bound process, and it would split the remainder equally between the two compute-bound processes, with 45% each. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A hypothetical but completely impractical implementation of max-min would be to give the processor at each instant to whichever process has received the least portion of the processor. In the example above, the disk-bound task would always be scheduled instantly, preempting the compute-bound processes. However, we have already seen why this would not work well. With two equally long tasks, as soon as we execute one instruction in one task, it would have received more resources than the other one, so to preserve &#8220;fairness&#8221; we would need to instantly switch to the next task. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can approximate a max-min fair allocation by relaxing this constraint &#8212; to allow a process to get ahead of its fair allocation by one time quantum. Every time the scheduler needs to make a choice, it chooses the task for the process with the least accumulated time on the processor. If a new process arrives on the queue with much less accumulated time, such as the disk-bound task, it will preempt the process, but otherwise the current process will complete its quantum. Tasks may get up to one time quantum more than their fair share, but over the long term the allocation will even out. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The algorithm we just described was originally defined for network, and not processor, scheduling. If we share a link between a browser request and a long download, we will get reasonable responsiveness for the browser if we have approximately fair allocation &#8212; the browser needs few network packets, and so under max-min its packets will always be scheduled ahead of the packets from the download. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even this approximation, though, can be computationally expensive, since it requires tasks to be maintained on a priority queue. For some server environments, there can be tens or even hundreds of thousands of scheduling decisions to be made every second. To reduce the computational overhead of the scheduler, most commercial operating systems use a somewhat different algorithm, to the same goal, which we describe next. </FONT><A id=x1-112001r192 name=x1-112001r192></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.5 </FONT><A id=x1-1130005 name=x1-1130005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Case Study: Multi-Level Feedback</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Most commercial operating systems, including Windows, MacOS, and Linux, use a scheduling algorithm called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multi-level feedback queue"}'>multi-level feedback queue (MFQ)</A></EM>. MFQ is designed to achieve several simultaneous goals: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Responsiveness.</B> Run short tasks quickly, as in SJF. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Low Overhead.</B> Minimize the number of preemptions, as in FIFO, and minimize the time spent making scheduling decisions. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Starvation-Freedom.</B> All tasks should make progress, as in Round Robin. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Background Tasks.</B> Defer system maintenance tasks, such as disk defragmentation, so they do not interfere with user work. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Fairness.</B> Assign (non-background) processes approximately their max-min fair share of the processor.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As with any real system that must balance several conflicting goals, MFQ does not perfectly achieve any of these goals. Rather, it is intended to be a reasonable compromise in most real-world cases. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">MFQ is an extension of Round Robin. Instead of only a single queue, MFQ has multiple Round Robin queues, each with a different priority level and time quantum. Tasks at a higher priority level preempt lower priority tasks, while tasks at the same level are scheduled in Round Robin fashion. Further, higher priority levels have <EM>shorter</EM> time quanta than lower levels. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Tasks are moved between priority levels to favor short tasks over long ones. A new task enters at the top priority level. Every time the task uses up its time quantum, it drops a level; every time the task yields the processor because it is waiting on I/O, it stays at the same level (or is bumped up a level); and if the task completes it leaves the system. </FONT><A id=x1-1130015 name=x1-1130015></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00420.gif" data-calibre-src="OEBPS/Images/image00420.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.5: </B>Multi-level Feedback Queue when running a mixture of I/O-bound and compute-bound tasks. New tasks enter at high priority with a short quantum; tasks that use their quantum are reduced in priority.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1130015"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the operation of an MFQ with four levels. A new compute-bound task will start as high priority, but it will quickly exhaust its time quantum and fall to the next lower priority, and then the next. Thus, an I/O-bound task needing only a modest amount of computing will almost always be scheduled quickly, keeping the disk busy. Compute-bound tasks run with a long time quantum to minimize switching overhead while still sharing the processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">So far, the algorithm we have described does not achieve starvation freedom or max-min fairness. If there are too many I/O-bound tasks, the compute-bound tasks may receive no time on the processor. To combat this, the MFQ scheduler monitors every process to ensure it is receiving its fair share of the resources. At each level, Linux actually maintains two queues &#8212; tasks whose processes have already reached their fair share are only scheduled if all other processes at that level have also received their fair share. Periodically, any process receiving less than its fair share will have its tasks increased in priority; equally, tasks that receive more than their fair share can be reduced in priority. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Adjusting priority also addresses strategic behavior. From a purely selfish point of view, a task can attempt to keep its priority high by doing a short I/O request immediately before its time quantum expires. Eventually the system will detect this and reduce its priority to its fair-share level. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Our previously hapless supermarket manager reads a bit farther into the textbook and realizes that supermarket express lanes are a form of multi-level queue. By limiting express lanes to customers with a few items, the manager can ensure short tasks complete quickly, reducing average response time. The manager can also monitor wait times, adding extra lanes to ensure that everyone is served reasonably quickly. </FONT><A id=x1-113002r193 name=x1-113002r193></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.1.6 </FONT><A id=x1-1140006 name=x1-1140006></A><FONT style="BACKGROUND-COLOR: #7be1e1">Summary</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">We summarize the lessons from this section: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO is simple and minimizes overhead. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are variable in size, then FIFO can have very poor average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are equal in size, FIFO is optimal in terms of average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Considering only the processor, SJF is optimal in terms of average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">SJF is pessimal in terms of variance in response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are variable in size, Round Robin approximates SJF. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If tasks are equal in size, Round Robin will have very poor average response time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Tasks that intermix processor and I/O benefit from SJF and can do poorly under Round Robin. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Max-min fairness can improve response time for I/O-bound tasks. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Round Robin and Max-min fairness both avoid starvation. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By manipulating the assignment of tasks to priority queues, an MFQ scheduler can achieve a balance between responsiveness, low overhead, and fairness.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the rest of this chapter, we extend these ideas to multiprocessors, energy-constrained environments, real-time settings, and overloaded conditions. </FONT><A id=x1-114001r184 name=x1-114001r184></A></P><A id=x1-1150002 name=x1-1150002>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.2 Multiprocessor Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Today, most general-purpose computers are multiprocessors. Physical constraints in circuit design make it easier to add computational power by adding processors, or cores, onto a single chip, rather than making individual processors faster. Many high-end desktops and servers have multiple processing chips, each with multiple cores, and each core with hyperthreading. Even smartphones have 2-4 processors. This trend is likely to accelerate, with systems of the future having dozens or perhaps hundreds of processors per computer. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This poses two questions for operating system scheduling: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How do we make effective use of multiple cores for running sequential tasks? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How do we adapt scheduling algorithms for parallel applications?</FONT></P></LI></UL><A id=x1-115001r195 name=x1-115001r195></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.2.1 </FONT><A id=x1-1160001 name=x1-1160001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Sequential Applications on Multiprocessors</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a server handling a very large number of web requests. A common software architecture for servers is to allocate a separate thread for each user connection. Each thread consults a shared data structure to see which portions of the requested data are cached, and fetches any missing elements from disk. The thread then spools the result out across the network. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How should the operating system schedule these server threads? Each thread is I/O-bound, repeatedly reading or writing data to disk and the network, and therefore makes many small trips through the processor. Some requests may require more computation; to keep average response time low, we will want to favor short tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A simple approach would be to use a centralized multi-level feedback queue, with a lock to ensure only one processor at a time is reading or modifying the data structure. Each idle processor takes the next task off the MFQ and runs it. As the disk or network finishes requests, threads waiting on I/O are put back on the MFQ and executed by the network processor that becomes idle. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are several potential performance problems with this approach: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Contention for the MFQ lock.</B> Depending on how much computation each thread does before blocking on I/O, the centralized lock may become a bottleneck, particularly as the number of processors increases. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Cache Coherence Overhead.</B> Although only a modest number of instructions are needed for each visit to the MFQ, each processor will need to fetch the current state of the MFQ from the cache of the previous processor to hold the lock. On a single processor, the scheduling data structure is likely to be already loaded into the cache. On a multiprocessor, the data structure will be accessed and modified by different processors in turn, so the most recent version of the data is likely to be cached only by the processor that made the most recent update. Fetching data from a remote cache can take two to three orders of magnitude longer than accessing locally cached data. Since the cache miss delay occurs while holding the MFQ lock, the MFQ lock is held for longer periods and so can become even more of a bottleneck. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Limited Cache Reuse.</B> If threads run on the first available processor, they are likely to be assigned to a different processor each time they are scheduled. This means that any data needed by the thread is unlikely to be cached on that processor. Of course, some of the thread&#8217;s data will have been displaced from the cache during the time it was blocked, but on-chip caches are so large today that much of the thread&#8217;s data will remain cached. Worse, the most recent version of the thread&#8217;s data is likely to be in a remote cache, requiring even more of a slowdown as the remote data is fetched into the local cache.</FONT></P></LI></UL><A id=x1-1160016 name=x1-1160016></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00421.gif" data-calibre-src="OEBPS/Images/image00421.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.6: </B>Per-processor scheduling data structures. Each processor has its own (multi-level) queue of ready threads.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For these reasons, commercial operating systems such as Linux use a <EM>per-processor</EM> data structure: a separate copy of the multi-level feedback queue for each processor. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1160016"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this approach. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Each processor uses <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:affinity scheduling"}'>affinity scheduling</A></EM>: once a thread is scheduled on a processor, it is returned to the same processor when it is re-scheduled, maximizing cache reuse. Each processor looks at its own copy of the queue for new work to do; this can mean that some processors can idle while others have work waiting to be done. Rebalancing occurs only if the queue lengths are persistent enough to compensate for the time to reload the cache for the migrated threads. Because rebalancing is possible, the per-processor data structures must still be protected by locks, but in the common case the next processor to use the data will be the last one to have written it, minimizing cache coherence overhead and lock contention. </FONT><A id=x1-116002r197 name=x1-116002r197></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.2.2 </FONT><A id=x1-1170002 name=x1-1170002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Parallel Applications</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A different set of challenges occurs when scheduling parallel applications onto a multiprocessor. There is often a natural decomposition of a parallel application onto a set of processors. For example, an image processing application may divide the image up into equal size chunks, assigning one to each processor. While the application could divide the image into many more chunks than processors, this comes at a cost in efficiency: less cache reuse and more communication to coordinate work at the boundary between each chunk. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If there are multiple applications running at the same time, the application may receive fewer or more processors than it expected or started with. New applications can start up, acquiring processing resources. Other applications may complete, releasing resources. Even without multiple applications, the operating system itself will have system tasks to run from time to time, disrupting the mapping of parallel work onto a fixed number of processors. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1180002 name=x1-1180002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Oblivious Scheduling</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">One might imagine that the scheduling algorithms we have already discussed can take care of these cases. Each thread is time sliced onto the available processors; if two or more applications create more threads in aggregate than processors, multi-level feedback will ensure that each thread makes progress and receives a fair share of the processor. This is often called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:oblivious scheduling"}'>oblivious scheduling</A></EM>, as the operating system scheduler operates without knowledge of the intent of the parallel application &#8212; each thread is scheduled as a completely independent entity. Figure &nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1180017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates oblivious scheduling. </FONT><A id=x1-1180017 name=x1-1180017></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00422.gif" data-calibre-src="OEBPS/Images/image00422.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.7: </B>With oblivious scheduling, threads are time sliced by the multiprocessor operating system, with no attempt to ensure threads from the same process run at the same time.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, several problems can occur with oblivious scheduling on multiprocessors: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bulk synchronous delay.</B> A common design pattern in parallel programs is to split work into roughly equal sized chunks; once all the chunks finish, the processors synchronize at a barrier before communicating their results to the next stage of the computation. This <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:bulk synchronous"}'>bulk synchronous</A></EM> parallelism is easy to manage &#8212; each processor works independently, sharing its results only with the next stage in the computation. Google MapReduce is a widely used bulk synchronous application. </FONT><A id=x1-1180028 name=x1-1180028></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00423.gif" data-calibre-src="OEBPS/Images/image00423.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.8: </B>Bulk synchronous design pattern for a parallel program; each processor computes on local data and waits for every other processor to complete before proceeding to the next step. Preempting one processor can stall all processors until the preempted process is resumed.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1180028"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the problem with bulk synchronous computation under oblivious scheduling. At each step, the computation is limited by the slowest processor to complete that step. If a processor is preempted, its work will be delayed, stalling the remaining processors until the last one is scheduled. Even if one of the waiting processors picks up the preempted task, a single preemption can delay the entire computation by a factor of two, and possibly even more with cache effects. Since the application does not know that a processor was preempted, it cannot adapt its decomposition for the available number of processors, so each step is similarly delayed until the processor is returned. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Producer-consumer delay.</B> Some parallel applications use a producer-consumer design pattern, where the results of one thread are fed to the next thread, and the output of that thread is fed onward, as in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1180039"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Preempting a thread in the middle of a producer-consumer chain can stall all of the processors in the chain. </FONT><A id=x1-1180039 name=x1-1180039></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00424.gif" data-calibre-src="OEBPS/Images/image00424.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.9: </B>Producer-consumer design pattern for a parallel program. Preempting one stage can stall the remainder.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-11800410 name=x1-11800410></A>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00425.gif" data-calibre-src="OEBPS/Images/image00425.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.10: </B>Critical path of a parallel program; delays on the critical path increase execution time.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Critical path delay.</B> More generally, parallel programs have a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:critical path"}'>critical path</A></EM> &#8212; the minimum sequence of steps for the application to compute its result. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11800410"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the critical path for a fork-join parallel program. Work off the critical path can occur in parallel, but its precise scheduling is less important. Preempting a thread on the critical path, however, will slow down the end result. Although the application programmer may know which parts of the computation are on the critical path, with oblivious scheduling, the operating system will not; it will be equally likely to preempt a thread on the critical path as off. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Preemption of lock holder.</B> Many parallel programs use locks and condition variables for synchronizing their parallel execution. Often, to reduce the cost of acquiring locks, parallel programs will use a &#8220;spin-then-wait&#8221; strategy &#8212; if a lock is busy, the waiting thread spin-waits briefly for it to be released, and if the lock is still busy, it blocks and looks for other work to do. This can reduce overhead in the common case that the lock is held for only short periods of time. With oblivious scheduling, however, the lock holder can be preempted &#8212; other tasks will spin-then-wait until the lock holder is re-scheduled, increasing overhead. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>I/O.</B> Many parallel applications do I/O, and this can cause problems if the operating system scheduler is oblivious to the application decomposition into parallel work. If a read or write request blocks in the kernel, the thread blocks as well. To reuse the processor while the thread is waiting, the application program must have created more threads than processors, so that the scheduler can have an extra one to run in place of the blocked thread. However, if the thread does not block (e.g., on a file read when the file is cached in memory), that means that the scheduler has more threads than processors, and so needs to do time slicing to multiplex threads onto processors &#8212; causing all of the problems we have listed above. </FONT></P></LI></UL>
<H5 class=subsubsectionHead><A id=x1-1190002 name=x1-1190002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Gang Scheduling</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">One possible approach to some of these issues is to schedule all of the tasks of a program together. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:gang scheduling"}'>gang scheduling</A></EM>. The application picks some decomposition of work into some number of threads, and those threads run either together or not at all. If the operating system needs to schedule a different application, if there are insufficient idle resources, it preempts all of the processors of an application to make room. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900111"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.11</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates an example of gang scheduling. </FONT><A id=x1-11900111 name=x1-11900111></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00426.gif" data-calibre-src="OEBPS/Images/image00426.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.11: </B>With gang scheduling, threads from the same process are scheduled at exactly the same time, and they are time sliced together to provide a chance for other processes to run.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because of the value of gang scheduling, commercial operating systems, such as Linux, Windows, and MacOS, have mechanisms for dedicating a set of processors to a single application. This is often appropriate on a server dedicated to a single primary use, such as a database needing precise control over thread assignment. The application can <EM>pin</EM> each thread to a specific processor and (with the appropriate permissions) mark it to run with high priority. The system reserves a small subset of the processors to run other applications, multiplexed in the normal way but without interfering with the primary application. </FONT><A id=x1-11900212 name=x1-11900212></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00427.gif" data-calibre-src="OEBPS/Images/image00427.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.12: </B>Performance as a function of the number of processors, for some typical parallel applications. Some applications scale linearly with the number of processors; others achieve diminishing returns.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For multiplexing multiple parallel applications, however, gang scheduling can be inefficient. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900212"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates why. It shows the performance of three example parallel programs as a function of the number of processors assigned to the application. While some applications have perfect speedup and can make efficient use of many processors, other applications reach a point of diminishing returns, and still others have a maximum parallelism. For example, if adding processors does not decrease the time spent on the program&#8217;s critical path, there is no benefit to adding those resources. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An implication of Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900212"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> is that it is usually more efficient to run two parallel programs each with half the number of processors, than to time slice the two programs, each gang scheduled onto all of the processors. Allocating different processors to different tasks is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:space sharing"}'>space sharing</A></EM>, to differentiate it from time sharing, or time slicing &#8212; allocating a single processor among multiple tasks by alternating in time when each is scheduled onto the processor. Space sharing on a multiprocessor is also more efficient in that it minimizes processor context switches: as long as the operating system has not changed the allocation, the processors do not even need to be time sliced. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-11900313"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates an example of space sharing. </FONT><A id=x1-11900313 name=x1-11900313></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00428.gif" data-calibre-src="OEBPS/Images/image00428.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.13: </B>With space sharing, each process is assigned a subset of the processors.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Space sharing is straightforward if all tasks start and stop at the same time; in that case, we can just allocate evenly. However, the number of available processors is often a dynamic property in a multiprogrammed setting, because tasks start and stop at irregular intervals. How does the application know how many processors to use if the number changes over time? </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1200002 name=x1-1200002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduler Activations</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">A solution, recently added to Windows, is to make the assignment and re-assignment of processors to applications visible to applications. Applications are given an execution context, or <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:scheduler activations"}'>scheduler activation</A></EM>, on each processor assigned to the application; the application is informed explicitly, via an upcall, whenever a processor is added to its allocation or taken away. Blocking on an I/O request also causes an upcall to allow the application to repurpose the processor while the thread is waiting for I/O. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As we noted in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, user-level thread management is possible with scheduler activations. The operating system kernel assigns processors to applications, either evenly or according to some priority weighting. Each application then schedules its user-level threads onto the processors assigned to it, changing its allocation as the number of processors varies due to external events such as other processes starting or stopping. If no other application is running, an application can use all of the processors of the machine; with more contention, the application must remap its work onto a smaller number of processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduler activations defines a <EM>mechanism</EM> for informing an application of its processor allocation, but it leaves open the question of the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multiprocessor scheduling policy"}'>multiprocessor scheduling policy</A></EM>: how many processors should we assign each process? This is an open research question. As we explained in our discussion of uniprocessor scheduling policies, there is a fundamental tradeoff between policies (such as Shortest Job First) that improve average response time and those (such as max-min fairness) that attempt to achieve fair allocation of resources among different applications. In the multiprocessor setting, average response time may be improved by giving extra resources to parallel interactive tasks provided this did not cause long-running compute intensive parallel tasks to starve for resources. </FONT><A id=x1-120001r196 name=x1-120001r196></A></P><A id=x1-1210003 name=x1-1210003>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.3 Energy-Aware Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Another important consideration for processor scheduling is its impact on battery life and energy use. Laptops and smartphones compete on the basis of battery life, and even for servers, energy usage is a large fraction of the overall system cost. Choices that the operating system makes can have a large effect on these issues. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might think that processor scheduling has little role to play with respect to system energy usage. After all, each application has a certain amount of computing that needs to be done, computing that requires energy whether we are running on a direct power line or off of a battery. Of course, the operating system should delay background or system maintenance tasks (such as software upgrades) for when the system is connected to power, but this is likely to be a relatively minor effect on the overall power budget. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In part because of the importance of battery life to computer users, modern architectures have developed a number of ways of trading reduced computation speed for lower energy use. In other words, the mental model of each computation taking a fixed amount of energy is no longer accurate. There is quite a bit of flux in the types of hardware support available on different systems, and systems five years from now are likely to make very different tradeoffs than those in place today. Thus, our goal in this section is not to provide a set of widely used algorithms for managing power, but rather to outline the design issues energy management poses for the operating system. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several power optimizations are possible, provided hardware support: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Processor design.</B> There can be several orders of magnitude difference between one processor design and another with respect to power consumption. Often, making a processor faster requires extra circuitry, such as out of order execution, that itself consumes power; low power processors are slower and simpler. Likewise, processors designed for lower clock speeds can tolerate lower voltage swings at the circuit level, reducing power consumption dramatically. Some systems have begun to put this tradeoff under the control of the operating system, by including both a high power, high performance multiprocessor and a low power, lower performance uniprocessor on the same chip. High power is appropriate when response time is at a premium and low power when power consumption is more important. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Processor usage.</B> For systems with multiple processor chips, or multiple cores on a single chip, lightly used processors can be disabled to save power. Processors will typically draw much less power when they are completely idle, but as we mentioned above, many parallel programs achieve some benefit from using extra processors, yet also reach a point of diminishing returns. Thus, there is a tradeoff between somewhat faster execution (e.g., by using all available resources) and lower energy use (e.g., by turning off some processors even when using them would slightly decrease response time). </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>I/O device power.</B> Devices not in use can be powered off. Although this is most obvious in terms of the display, devices such as the WiFi or cellphone network interface also consume large amounts of power. Power-constrained embedded systems such as sensors will turn on their network interface hardware periodically to send or receive data, and then go back to quiescence. For this to work, the senders and receivers need to synchronize their periods of transmission, or the hardware needs to have a low power listening mode. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Heat dissipation</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A closely related topic to energy use is heat dissipation. In laptop computers, you can save weight by not including a fan to cool the processor. However, a modern multicore chip will consume up to 150 Watts, or more than a very bright incandescent light bulb. Just as with a light bulb, the heat generated has to go somewhere. Making things significantly more complicated, the processor will also break permanently if it runs at too high a temperature. Thus, the operating system increasingly must monitor and manage the temperature of the processor to ensure it stays within its operating region. Much like a cheetah, portable computers are now capable of running at very fast speeds for short periods of time, before they need to take a break to cool down. Or they can amble at much slower speeds for a longer period of time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The laptop one of us used to write this book illustrates this. Formatting this textbook takes about a minute when the computer is cold, but the same formatting request will stall in the middle of the build for several minutes if run immediately after a previous build request. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">At times, different power optimizations interact in subtle ways. For example, running application code quickly can sometimes improve power efficiency, by enabling the network interface hardware to be turned off more quickly once the application finishes. Because context switching consumes both time and energy to reload processor caches, affinity scheduling improves both performance and energy efficiency. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In most cases, however, there is a tradeoff: how should the operating system balance between competing demands for timeliness and energy efficiency? If the user has requested maximum responsiveness or maximum battery life, the choice is easy, but often the user wants a reasonable tradeoff between the two. </FONT><A id=x1-12100114 name=x1-12100114></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00429.gif" data-calibre-src="OEBPS/Images/image00429.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.14: </B>Example relationship between response time and user-perceived value. For most applications, faster response time is valuable within a range. Below some threshold, users will not be able to perceive the difference. Above some threshold, users will perform other activities while waiting for the result.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One approach would be to consider the value that the user places on fast response time for a particular application: quickly updating the display after a user interface command is probably more important than transferring files quickly in the background. We can capture the relationship between response time and value in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12100114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Although the precise shape and magnitude will vary from user to user and application to application, the curve will head down and to the right &#8212; the longer something takes, the less useful it is. Often, the curve is S-shaped. Human perception is unable to tell the difference between a few tens of milliseconds, so adding a short delay will not matter that much for most tasks; likewise, if a protein folding computation has already taken a few minutes, it won&#8217;t matter much if it takes a few more seconds. Not everything will be S-shaped: in high frequency stock trading, value starts high and plummets to zero within a few milliseconds. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Response time predictability affects this relationship as well. An online video that cuts out for a few seconds every minute is much less watchable than one that is lower quality on average but more predictable. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If we combine Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12100114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> with the fact that increased energy use often provides diminishing returns in terms of improved performance, this suggests a three prong strategy to spend the system&#8217;s energy budget where it will make the most difference: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Below the threshold of human perception.</B> Optimize for energy use, to the extent that tasks can be executed with greater energy efficiency without the user noticing. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Above the threshold of human perception.</B> Optimize for response time if the user will notice any slowdown. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Long-running or background tasks.</B> Balance energy use and responsiveness depending on the available battery resources. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Battery life and the kernel-user boundary</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An emerging issue on smartphones is that application behavior can have a significant impact on battery life, e.g., by more intensive use of the network or other power-hungry features of the architecture. If a user runs a mix of applications, how can she know which was most responsible for their smartphone running out of power? Among the resources we will discuss in this book, energy is almost unique in being a <EM>non-virtualizable</EM> resource. When an application drains the battery, the energy lost is no longer available to any other applications. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How can we prevent a misbehaving or greedy application from using more than its share of the battery? One model is to let the user decide: for the kernel to measure and record how much energy was used by each application, so the user can determine if each application is worth it. Apple has taken a different approach with the iPhone. Because Apple controls which applications can run on the system, it can and has barred applications that (in its view) unnecessarily drain the battery. It will be interesting to see which of these models wins out over time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-121002r210 name=x1-121002r210></A><A id=x1-1220004 name=x1-1220004>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.4 Real-Time Scheduling</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">On some systems, the operating system scheduler must account for process deadlines. For example, the sensor and control software to manage an airplane&#8217;s flight path must be executed in a timely fashion, if it is to be useful at all. Similarly, the software to control anti-lock brakes or anti-skid traction control on an automobile must occur at a precise time if it is to be effective. In a less life critical domain, when playing a movie on a computer, the next frame must be rendered in time or the user will perceive the video quality as poor. </FONT><A id=x1-12200115 name=x1-12200115></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00430.gif" data-calibre-src="OEBPS/Images/image00430.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.15: </B>With real-time constraints, the value of completing some task drops to zero if the deadline is not met.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These systems have <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:real-time constraint"}'>real-time constraints</A></EM>: computation that must be completed by a deadline if it is to have value. Real-time constraints are a special case of Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12100114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12200115"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.15</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, where the value of completing a task is uniform up to the deadline, and then drops to zero. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How do we design a scheduler to ensure deadlines are met? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We might start by assigning real-time tasks a higher priority than any less time critical tasks. We could then run the system under a variety of different of different workloads, and see if the system continues to comfortably meet its deadlines in all cases. If not, the system may need a faster processor or other hardware resources to speed up the real-time tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, testing alone is insufficient for guaranteeing real-time constraints. Recall that the specific ordering of execution events can sometimes lead to different execution sequences &#8212; e.g., sometimes a thread will need to wait for a lock held another thread, and other times the lock will be FREE. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One option is that, instead of threads, we should use a completely deterministic and repeatable schedule that ensures that the deadlines are met. This can work if the real-time tasks are periodic and fixed in advance. However, in dynamic systems, it is difficult to account for all possible variations affecting how long different parts of the computation will take. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are three widely used techniques for increasing the likelihood that threads meet their deadlines. These approaches are also useful whenever timeliness matters without a strict deadline, e.g., to ensure responsiveness of a user interface. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Over-provisioning.</B> A simple step is to ensure that the real-time tasks, in aggregate, use only a fraction of the system&#8217;s processing power. This way, the real-time tasks will be scheduled quickly, without having to wait for higher-priority, compute-intensive tasks. The equivalent step in college is to avoid signing up for too many hard courses in the same semester! </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Earliest deadline first.</B> Careful choice of the scheduling policy can also help meet deadlines. If you have a pile of homework to do, neither shortest job first nor round robin will ensure that the assignment due tomorrow gets done in time. Instead, real-time schedulers, mimicking real life, use a policy called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:earliest deadline first"}'>earliest deadline first</A></EM> (EDF). EDF sorts tasks by their deadline and performs them in that order. If it is possible to schedule the required work to meet their deadlines, and the tasks only need the processor (and not I/O, locks or other resources), EDF will ensure that all tasks are done in time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For complex tasks, however, EDF can produce anomalous behavior. Consider two tasks. Task A is I/O-bound with a deadline at 12 ms, needing 1 ms of computation followed by 10 ms of I/O. Task B is compute-bound with a deadline at 10 ms, but needing 5 ms of computation. Although there is a schedule that will meet both deadlines (run task A first), EDF will run the compute-bound task first, causing the I/O-bound task to miss its deadline. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This limitation can be addressed by breaking tasks into shorter units, each with its own deadline. In the example, the true deadline for the compute portion of the I/O-bound task is at 2 ms, because if it is not completed by then, the overall task deadline will be missed. If your homework next week needs a book from the library, you need to put that on hold first, even if that slightly delays the homework you have due tomorrow. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Priority donation.</B> Another problem can occur through the interaction of shared data structures, priorities, and deadlines. Suppose we have three tasks, each with a different priority level. The real-time task runs at the highest priority, and it has sufficient processing resources to meet its deadline, with some time to spare. However, the three tasks also access a shared data structure, protected by a lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose the low priority acquires the lock to modify the data structure, but it is then preempted by the medium priority task. The relative priorities imply that we should run the medium priority task first, even though the low priority task is in the middle of a critical section. Next, suppose the real-time task preempts the medium task and proceeds to access the shared data structure. It will find the lock busy and wait. Normally, the wait would be short, and the real-time task would be able to meet its deadline despite the delay. However, in this case, when the high priority task waits for the lock, the scheduler will pick the medium priority task to run next, causing an indefinite delay. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:priority inversion"}'>priority inversion</A></EM>; it can occur whenever a high priority task must wait for a lower priority task to complete its work. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A commonly used solution, implemented in most commercial operating systems, is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:priority donation"}'>priority donation</A></EM>: when a high priority task waits on a shared lock, it temporarily donates its priority to the task holding the lock. This allows the low priority task to be scheduled to complete the critical section, at which point its priority reverts to its original state, and the processor is re-assigned to the high priority, waiting, task. </FONT></P></LI></UL><A id=x1-122002r212 name=x1-122002r212></A><A id=x1-1230005 name=x1-1230005>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5 Queueing Theory</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you build a new web service, and the week before you are to take it live, you test it to see whether it will have reasonable response time. If your tests show that the performance is terrible, what then? Is it because the implementation is too slow? Perhaps you have the wrong scheduler? Quick, let&#8217;s re-implement that linked list with a hash table! And add more levels to the multi-level feedback queue! Our advice: don&#8217;t panic. In this section, we consider a third possibility, an effect that often trumps all of the others: response time depends non-linearly on the rate that tasks arrive at a system. Understanding this relationship is the topic of <EM>queueing theory</EM>. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, if you have ever waited in line (and who hasn&#8217;t?), you have an intuitive understanding of queueing theory. Its concepts apply whenever there is a queue waiting for a turn, whether it is tasks waiting for a processor, web requests waiting for a turn at a web server, restaurant patrons waiting for a table, cars waiting at a busy intersection, or people waiting in line at the supermarket. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While queueing theory is capable of providing precise predictions for complex systems, our interest is providing you the tools to be able to do back of the envelope calculations for where the time goes in a real system. For performance debugging, coarse estimates are often enough. For this reason, we make two simplifying assumptions for this discussion. First, we assume the system is work-conserving, so that all tasks that arrive are eventually serviced; this will normally be the case except in extreme overload conditions, a topic we will discuss in the next section of this chapter. Second, although the scheduling policy can affect a system&#8217;s queueing behavior, we will keep things simple and assume FIFO scheduling. </FONT><A id=x1-123001r199 name=x1-123001r199></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.1 </FONT><A id=x1-1240001 name=x1-1240001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Definitions</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Because queueing theory is concerned with the root causes of system performance, and not just its observable effects, we need to introduce a bit more terminology. A simple abstract queueing system is illustrated by Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12400116"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. In any queueing system, tasks arrive, wait their turn, get service, and leave. If tasks arrive faster than they can be serviced, the queue grows. The queue shrinks when the service rate exceeds the arrival rate. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To begin, we will consider single-queue, single-server, work-conserving systems. Later, we will introduce more complexity such as multiple queues, multiple servers, and finite queues that can discard some requests. </FONT><A id=x1-12400116 name=x1-12400116></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00431.gif" data-calibre-src="OEBPS/Images/image00431.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.16: </B>An abstract queueing system. Tasks arrive, wait their turn in the queue, get service, and leave.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Server.</B> A server is anything that performs tasks. A web server is obviously a server, performing web requests, but so is the processor on a client machine, since it executes application tasks. The cashier at a supermarket and a waiter in a restaurant are also servers. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Queueing delay (W) and number of tasks queued (Q).</B> The queueing delay, or wait time, is the total time a task must wait to be scheduled. In a time slicing system, a task might need to wait multiple times for the same server to complete its task; in this case the queueing delay includes all of the time a task spends waiting until it is completed. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Service time (S).</B> The service time S, or execution time, is the time to complete a task assuming no waiting. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Response time (R).</B> The response time is the queueing delay (how long you wait in line) plus the service time (how long it takes once you get to the front of the line). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; W + S </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the web server example we started with, the poor performance can be due to either factor &#8212; the system could be too slow even when no one is waiting, or the system could be too slow because each request spends most of its time waiting for service. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can improve the response time by improving either factor. We can reduce the queueing delay by buying more servers (for example, by having more processors than ready threads or more cashiers than customers), and we can reduce service time by buying a faster server or by engineering a faster implementation. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Arrival rate (&#955;) and arrival process.</B> The arrival rate &#955; is the average rate at which new tasks arrive. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">More generally, the arrival process describes when tasks arrive including both the average arrival rate and the pattern of those arrivals such as whether arrivals are bursty or spread evenly over time. As we will see, burstiness can have a large impact on queueing behavior. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Service rate (&#956;).</B> The service rate &#956; is the number of tasks the server can complete per unit of time when there is work to do. Notice that the service rate &#956; is the inverse of the service time S. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Utilization (U).</B> The utilization is the fraction of time the server is busy (0 &#8804; U &#8804; 1). In a work-conserving system, utilization is determined by the ratio of the average arrival rate to the service rate: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">U </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#955; / &#956; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if &#955; &lt; &#956; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if &#955; &#8805; &#956;) </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice that if &#955; &gt; &#956;, tasks arrive more quickly than they can be serviced. Such an overload condition is unstable; in a work-conserving system, the queue length and queueing delay grow without bound. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Throughput (X).</B> Throughput is the number of tasks processed by the system per unit of time. When the system is busy, the server processes tasks at the rate of &#956;, so we have: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">X </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; U &#956; </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Combining this equation with the previous one, we can see that when the average arrival rate &#955; is less than the service rate &#956;, the system throughput matches the arrival rate. We can also see that the throughput can never exceed &#956; no matter how quickly tasks arrive. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">X </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#955; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if U &lt; 1 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#956; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; if U = 1 </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Number of tasks in the system (N).</B> The average number of tasks in the system is just the number queued plus the number receiving service: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; Q + U </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR></LI></UL><A id=x1-124002r215 name=x1-124002r215></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.2 </FONT><A id=x1-1250002 name=x1-1250002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Little&#8217;s Law</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:Little"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Little&#8217;s Law</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> is a theorem proved by John Little in 1961 that applies to any <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:stable system"}'>stable system</A></EM> where the arrival rate matches the departure rate. It defines a very general relationship between the average throughput, response time, and the number of tasks in the system: </FONT>
<P><BR></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although this relationship is simple and intuitive, it is powerful because the &#8220;system&#8221; can be anything with arriving and departing tasks, provided the system is stable &#8212; regardless of the arrival process, number of servers, or queueing order. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose we have a queueing system like the one shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12400116"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and we observe over the course of an hour that an average of 100 requests arrive and depart each second and that the average request is completed 50 ms after it arrives. On average, how many requests are being handled by the system? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Since the arrival rate matches the departure rate, the system is stable and we can use Little&#8217;s Law. We have a throughput X = 100 requests/second and a response time R = 50 ms = 0.05 seconds: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 100 requests/second &#215; 0.05 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>5 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this system there are, on average, 5 requests waiting in the queue or being served. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can also zoom in to see what is happening at the server, ignoring the queue. The server itself is a system, and Little&#8217;s Law applies there, too. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose we have a server that processes one request at a time and we observe that an average of 100 requests arrive and depart each second and that the average request completes 5 ms after it arrives. What is the average utilization of the server? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The utilization of the server is the fraction of time the server is busy processing a request. Because the server handles one request at a time, its utilization equals the average number of requests in the server-only system. Using Little&#8217;s Law: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">U </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 100 requests/second &#215; 0.005 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>0.5 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR><FONT style="BACKGROUND-COLOR: #7be1e1">The average utilization is 0.5 or 50%. &#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can also look at the subsystem comprising just the queue. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>For the system described in the previous two examples, how long does an average request spend in the queue, and on average how many requests are in the queue? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>We know that an average task takes 50 ms to get through the queue and server and that it spends 5 ms at the server, so it must spend 45 ms in the queue. Similarly, we know that on average the system holds 5 tasks with 0.5 of them in the server, so the average queue length is 4.5 tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can get the same result with Little&#8217;s Law. One hundred tasks pass through the queue per second and spend an average of 45 ms in the queue, so the average number of tasks in the queue is: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 100 requests/second &#215; 0.045 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>4.5 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although Little&#8217;s Law is useful, remember that it only provides information about the system&#8217;s averages over time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>One thing might puzzle you. In the previous example, if the average number of tasks in the queue is 4.5 and processing a request takes 5 ms, how can the average queueing delay for a request be 45 ms rather than 4.5 &#215; 5 ms = 22.5 ms? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The <EM>average</EM> number of requests in the queue is 4.5. Sometimes there are more; sometimes there are fewer. Queues will grow during bursts of arrivals, and they will shrink when tasks are arriving slowly. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In fact, from the 0.5 server utilization rate calculated above, we know that the queue is empty half the time. To make up for the empty periods, there <EM>must</EM> be periods with longer-than-average queue lengths. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, the queues tend to be full during busy periods and they tend to be empty during idle periods, so relatively few requests enjoy short or empty queues and relatively many suffer long queues. So, the average request sees a longer queue than the average queue length over time might suggest, and the (per-request) average queueing delay exceeds the (time) average queue length times the (per-request) average service time. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Not only can we apply Little&#8217;s Law to a simple queueing system or its subcomponents, we can apply it to more complex systems, even those whose internal structure we do not fully understand. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose there is a complex web service like Google, Facebook, or Amazon, and we know that the average request takes 100 milliseconds and that the service handles an average of 10,000 queries per second. How many requests are pending in the system on average? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Applying Little&#8217;s Law: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">N </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; X R </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 10000 requests/second &#215; 0.1 seconds </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>1000 requests</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Note that this is true regardless of the internal structure of the web service. It may have many load balancers, processors, network switches, and databases, each with separate queues, and each with different queueing policies, but in aggregate in steady state the number of requests being handled must be equal to the product of the response time and the throughput. &#9633; </FONT><A id=x1-125001r217 name=x1-125001r217></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.3 </FONT><A id=x1-1260003 name=x1-1260003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Response Time Versus Utilization</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Because having more servers (whether processors on chip or cashiers in a supermarket) or faster servers is costly, you might think that the goal of the system designer is to maximize utilization. However, in most cases, there is no free lunch: as we will see, higher utilization normally implies higher queueing delay and higher response times. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Operating a system at high utilization also increases the risk of overload. Suppose you plan to minimize costs by operating a web site at 95% utilization, but your service turns out to be a little more popular than you expected. You can quickly find yourself operating in the unstable regime where requests are arriving faster than you can service them (&#955; &gt; &#956;) and where your queues and waiting times are growing without bound. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As a designer, you need to find an appropriate tradeoff between higher utilization and better response time. Fifty years ago, computer designers made the tradeoff in favor of higher utilization: when computers are wildly expensive, it is annoying but understandable to make people wait for the computer. Now that computers are much cheaper, our lives are better! We now usually make the computer do the waiting. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can predict a queueing system&#8217;s average response time from its arrival process and service time, but the relationship is more complex than the relationships discussed so far. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To provide intuition, we start with some extreme scenarios that bound the behavior of a queueing system; we will introduce more realistic scenarios as we proceed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Broadly speaking, higher arrival rates and burstier arrival patterns tend to yield longer queue lengths and response times than lower arrival rates and smoother arrival patterns. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Best case: Evenly spaced arrivals.</B> Suppose we have a set of fixed-sized tasks that arrive equally spaced from one another. For As long as the rate at which tasks arrive is less than the rate at which the server completes the tasks, there will be no queueing at all. Perfection! Each server finishes the previous customer in time for the next arrival. </FONT><A id=x1-12600117 name=x1-12600117></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00432.gif" data-calibre-src="OEBPS/Images/image00432.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.17: </B>Best case response time and throughput as a function of the task arrival rate relative to the service rate. These graphs assume arrivals are evenly spaced and service times are fixed-size.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the relationship between arrival rate and response time for this best case scenario of evenly spaced arrivals. There are three regimes: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>&#955; &lt; &#956;.</B> If the arrival rate is below the service rate, there is no queueing and the response time equals the service time.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, suppose we have a server that can handle 1000 requests per second, and one request arrives every 1000, 100, or 10 milliseconds. The server finishes processing request i- 1 before request i arrives, and request i completes 1 ms after it arrives, clearing the way for request i + 1. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The situation remains the same if arrivals are more closely spaced at 1.1, 1.01, 1.001, and so on down to 1.0 ms, where each request arrives at the moment the previous request completes. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>&#955; = &#956;.</B> If the arrival rate matches the service rate, the system is in a precarious equilibrium. If the queues are initially empty, they will stay empty, but if the queues are initially full, they will remain full.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose arrivals are coming every 1.0 ms, and at some point during the day a single extra request arrives; that request must wait until the previous one completes, but the server will then be busy when the next request arrives. That single extra request produces queueing delay for every subsequent request. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>&#955; &gt; &#956;.</B> If the arrival rate exceeds the service rate, queues will grow without bound. In this case, the system is not in equilibrium, and the steady state response time is undefined.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose the task arrival rate is one per 0.999 ms so that tasks arrive slightly faster than they can be processed? If a system&#8217;s arrival rate exceeds its service rate, then under our simple model its queues will grow without bound, and its queueing delay is undefined. In practice, memory is finite; once the queue&#8217;s capacity is reached, the system must discard some of the arriving requests. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> also shows the relationship between arrival rate and throughput. When the arrival rate is less than the service rate, increasing the arrival rate increases throughput. Once the arrival rate matches or exceeds the service rate, faster arrivals just grow the queues more quickly, they do not increase useful throughput. </FONT><A id=x1-12600218 name=x1-12600218></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00433.gif" data-calibre-src="OEBPS/Images/image00433.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.18: </B>Response time for a server that can handle 10 requests per second as we vary arrival rate of fixed-size tasks in two scenarios: evenly spaced arrivals and bursty arrivals where all of a second&#8217;s requests arrive in a group at the start of the second.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Worst case: Bursty arrivals.</B> Now consider the opposite case. Suppose a group of tasks arrive at exactly the same time. The average wait time increases linearly as more tasks arrive together &#8212; one task in a group can be serviced right away, but others must wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600218"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> considers a hypothetical server with a maximum throughput of 10 tasks per second as we vary the number of tasks that arrive per second. The graph shows two cases: one where requests are evenly spaced as in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and the other where requests arrive in a burst at the start of each second. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even when the request rate is below the server&#8217;s service rate, bursty arrivals suffer queueing delays. For example, when five requests arrive as a group at the start of each second, the first request is served immediately and finishes 0.1 seconds later. The server can then start processing the second request, finishing it 0.2 seconds after the start of the interval. The third, fourth, and fifth requests finish at 0.3, 0.4, and 0.5 seconds after the start of the second, giving an average response time of (0.1 + 0.2 + 0.3 + 0.4 + 0.5)/5 = 0.3 seconds. By the same logic, if ten requests arrive as a group, the average response time is (0.1 + 0.2 + 0.3 + 0.4 + 0.5 + 0.6 + 0.7 + 0.8 + 0.9 + 1.0)/10 = 0.55 seconds. If the same requests had arrived evenly spaced, their average response time would have been over five times better! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Exponential arrivals.</B> Most systems are somewhere in between this best case and worst case. Rather than being perfectly synchronized or perfectly desynchronized, task arrivals in many systems are random. For example, different customers in a supermarket do not coordinate with each other as to when they arrive. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Likewise, service times are not perfectly equal &#8212; there is randomness there as well. At a doctor&#8217;s office, everyone has an appointment, so it may seem like that should be the best case scenario, and no one should ever have to wait. Even so, there is often queueing! Why? If the amount of time the doctor takes with each patient is sometimes shorter and sometimes longer than the appointment length, then random chance will cause queueing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A particularly useful model for understanding queueing behavior is to use an exponential distribution to describe the time between tasks arriving and the time it takes to service each task. Once you get past a bit of math, the exponential provides a stunningly simple <EM>approximate</EM> description of most real-life queueing systems. We do not claim that all real systems always obey the exponential model in detail; in fact, most do not. However, the model is often accurate enough to provide insight on system behaviors, and as we will discuss, it is easy to understand the circumstances under which it is inaccurate. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Model vs. reality</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When trying to understand a complex system, it is often useful to construct a model of its behavior. A model is a simplification that tries to capture the most important aspects of a more complex system&#8217;s behavior. Models are neither true nor false, but they can be useful or not for a particular purpose. It is often the case that a more complex model will yield a closer approximation; whether the added complexity is useful or gets in the way depends on how the model is being used. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We often find it useful to use simple workload models when debugging early system implementations. Using the types of analysis described in this chapter and an understanding of the system being built, it is usually possible to predict how the system should behave under simple workloads. If measured behavior deviates from these predictions, there is a bug in our understanding or implementation of the system. Simple workloads can help us improve our understanding if it is the former and track down the bug if it is the latter. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We could, instead, evaluate early implementations by feeding them more realistic workloads. For example, if we are building a new web server, we could feed it a workload trace captured at some other server. However, this approach is often more complex. For example, to test our system under a range of conditions, we need to gather a range of traces &#8212; some with low load, some with high; some with bursty loads, some with smooth; etc. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Worst, even though this approach is more complex, it may yield less insight because it is harder to predict the expected system behavior. If we run a simulation with a trace and get worse performance than we expected, is it because we do not understand our system or because we do not understand the trace? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This is not to suggest that simple models are always superior to more complex, more realistic ones. Once we are satisfied with our new system&#8217;s behavior for workloads we understand, we should test it for workloads we do not understand or control. There may be (and probably are) important behaviors not captured in our simple models. We might find, for example, that bursts of interest in particular topics create &#8220;hot spots&#8221; of load that we did not anticipate. Evaluation under more realistic models might make us realize that we need to implement more aggressive caching of recently popular pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Selecting the right model for system evaluation is a delicate balance between complexity and accuracy. If after abstracting away detail, we can still provide approximately correct predictions of system behavior under a variety of scenarios, then it is likely the model captures the most important aspects of the system. If the model is inaccurate in some important respect, then it means our explanation for how the system behaves is too coarse, and to improve the prediction we need to revise the model. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-12600319 name=x1-12600319></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00434.gif" data-calibre-src="OEBPS/Images/image00434.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.19: </B>Exponential probability distribution.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">First, the math. An <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:exponential distribution"}'>exponential distribution</A></EM> of a continuous random variable with a mean of 1 / &#955; has the probability density function, shown in Figure </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600319"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.19</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">f(x) </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#955;e<SUP>-&#955;x</SUP> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, you need not understand that equation in any detail, except for the following. A useful property of an exponential distribution is that it is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:memoryless property"}'>memoryless</A></EM>. A memoryless distribution for the time between two events means that the likelihood of an event occurring remains the same, <EM>no matter how long we have already waited</EM> for the event, or what other events may have already happened. For example, on a web server, web requests from different users (usually) arrive independently. Sometimes, two requests will arrive close together in time; sometimes there will be more of a delay. For example, suppose a web server receives a request from a new user on average every 10 ms. If you want to predict how long until the next request arrives, it probably does not matter when the <EM>last</EM> request arrived: 0, 1, 5, or 50 ms ago. The expected time to the next request is still probably about 10 ms. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Not every distribution is memoryless. A Gaussian, or normal, distribution for the time between events is closer to the best case scenario described above &#8212; arrivals occur randomly, but they tend to occur at regular intervals, give or take a bit. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some probability distributions work the other way. With a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:heavy-tailed distribution"}'>heavy-tailed distribution</A></EM>, the longer you have waited for some event, the longer you are likely to still need to wait. This is closer to the worst case behavior above, as it means that most events are clustered together. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, a ticket seller&#8217;s web site might see bursty workloads. For long periods of time the site might see little traffic, but when tickets for a popular concert of sporting event go on sale, the traffic may be overwhelming. Here, external factors introduce synchronization across different users&#8217; activities so that requests from different users do not arrive independently. Such a workload is unlikely to be memoryless; if you look at a ticket seller&#8217;s web site at a random moment and see that it has been a long time since the last request arrived, you probably arrived during a lull, and you can predict that it will likely be a long time until the next request arrives. On the other hand, if the last request just arrived, you probably arrived during a burst, and the next request will arrive soon. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">With a memoryless distribution, the behavior of queueing systems becomes simple to understand. One can think of the queue as a finite state machine: with some probability, a new task arrives, increasing the queue by one. If the queue length is non-zero, with some other probability, a task completes, decreasing the queue by one. With a memoryless distribution of arrivals and departures, the probability of each transition is constant and independent of the other transitions, as illustrated in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600420"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.20</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-12600420 name=x1-12600420></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00435.gif" data-calibre-src="OEBPS/Images/image00435.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.20: </B>State machine representing a queue with exponentially distributed arrivals and departures. &#955; is the rate of arrivals; &#956; is the rate at which the server completes each task. With an exponential distribution, the probability of a state transition is independent of how long the system has been in any given state.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming that &#955; &lt; &#956;, the system is stable Assuming stability and exponential distributions for the arrival and departure processes, we can solve the model to determine the average response time R as a function of the utilization U and service time S: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - U) </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Recall that the utilization, the fraction of time that the server is busy, is simply the ratio between &#955; and &#956;. </FONT><A id=x1-12600521 name=x1-12600521></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00436.gif" data-calibre-src="OEBPS/Images/image00436.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.21: </B>Relationship between response time and utilization, assuming exponentially distributed arrivals and departures. Average response time goes to infinity as the system approaches full utilization.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This equation is graphed in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600521"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. When utilization is low, there is little queueing delay and response time is close to the service time. Furthermore, when utilization is low, small increases in the arrival rate result in small increases in queueing delay and response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As utilization increases, queueing and response time also increase, and the relationship is non-linear. At high utilizations, the queueing delay is high, and small increases in the arrival rate can drastically increase queueing delay and response time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose a queueing system with exponentially distributed arrivals and task sizes is 20% utilized and the load increases by 5%, by how much does the response time increase? How does that increase compare to the case when utilization goes from 90% to 95%? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>At 20% utilization, the response time is: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - U) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - 0.2) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1.25 S </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">At 25% utilization, the response time is: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - U) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; S / (1 - 0.25) </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1.33 S </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The 5% increase in load increases response time by about 8%.</B> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using the same equation, at 90% utilization we have R = 10S and at 95% we have R = 20S, <B>the 5% increase in load increases response time by a factor of two.</B> &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The response time of a system becomes unbounded as the system approaches full utilization. Although it might seem that full utilization is an achievable goal, if there is any randomness in arrivals <EM>or</EM> any randomness in service times, full utilization cannot be achieved in steady state without making some tasks wait unbounded amounts of time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In most systems, well before a system reaches full utilization, average response time will become unbearably long. In the next section, we discuss some of the steps system designers can take in response to overload. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Variance in the response time increases even faster as the system approaches full utilization, proportional to 1 / (1 - U)<SUP>2</SUP>. Even with 99% utilization, 1% of the time there is no queue at all; random chance means that while sometimes a large number of customers arrive at nearly the same time, at other times the server will be able to work through all of the backlog. If you are lucky enough to arrive at just that moment, you can receive service without waiting. If you are unlucky enough to arrive immediately after a burst of other customers, your wait will be quite long. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Exponential arrivals are burstier than the evenly spaced ones we considered in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600117"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and less bursty than the ones we considered in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-12600218"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. The response time line for the exponential arrivals is higher than the one for evenly spaced arrivals, which was flat across the entire stable range form U = 0 to U = 1, and the line is lower than the one for more bursty arrivals, which rose rapidly even when utilization was low. In general burstier arrivals will produce worse response time for a given level of load. </FONT><A id=x1-126006r218 name=x1-126006r218></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.4 </FONT><A id=x1-1270004 name=x1-1270004></A><FONT style="BACKGROUND-COLOR: #7be1e1">&#8220;What if?&#8221; Questions</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Queueing theory is particularly useful for answering &#8220;what if?&#8221; questions: what happens if we change some design parameter of the system. In this section, we consider a selection of these questions, as a way of providing you a bit more intuition. </FONT>
<H5 class=subsubsectionHead><A id=x1-1280004 name=x1-1280004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduling Policy</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">What happens to the response time curve for other scheduling policies? It depends on the burstiness and predictability of the workload. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the distribution of arrivals or service times is less bursty than an exponential (e.g., evenly spaced or Gaussian), FIFO will deliver nearly optimal response times, while Round Robin will perform worse than FIFO. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If task service times are exponentially distributed but individual task times are unpredictable, the average response time is the exactly the same for Round Robin as for FIFO. With a memoryless distribution, every queued task has the same expected remaining service time, so switching among tasks has no impact other than to increase overhead. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On the other hand, if task lengths can be predicted and there is variability of service times, Shortest Job First can improve average response time, particularly if arrivals are bursty. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Many real-world systems exhibit more bursty arrivals or service times than an exponential distribution. A bursty distribution is sometimes called <EM>heavy-tailed</EM> because it has more very long tasks; since the mean rate is the same, this also implies that the distribution has even more very short tasks. For example, web page size is heavy-tailed; so is the processing time per web page. Process execution times on desktop computers are also heavy-tailed. For these types of systems, burstiness results in worse average response time than would be predicted by an exponential distribution. That said, for these types of systems, there is an even greater benefit to approximating SJF to avoid stalling small requests behind long ones, and Round Robin will outperform FIFO. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Using SJF (or an approximation) to improve average response time comes at a cost of an increase in response time for long tasks. At low utilization, this increase is small, but at high utilization SJF can result in a massive increase in average response time for long tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To see this, note that any server alternates between periods of being idle (when the queue is empty) and periods of being busy (when the queue is non-empty). If we ignore switching overhead, the scheduling discipline has no impact on these periods &#8212; they are only affected by when tasks arrive. Scheduling can only affect which tasks the server handles first. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">With SJF, a long task will only complete immediately before an idle period; it is always the last thing in the queue to complete. As utilization increases, these idle periods become increasingly rare. For example, if the server is 99% busy, the server will be idle only 1% of the time. Further, idle periods are <EM>not</EM> evenly distributed &#8212; a server is much more likely to be idle if it was idle a second ago. This means that the long jobs are likely to wait for a long time under SJF under high load. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1290004 name=x1-1290004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Workloads That Vary With the Queueing Delay</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">So far, we have assumed that arrival rates and service times are independent of queueing delay. This is not always the case. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, suppose a system has 10 users. Each repeatedly issues one request, waits for the result, thinks about the results, and issues the next request. In such a system, the arrival rate will generally be lower during periods when many tasks are queued than during periods when few are. In the limit, during periods when 10 tasks are queued, no new tasks can arrive and the arrival rate is zero. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Or, consider an online store that becomes overloaded and sluggish during a holiday shopping season. Rather than continuing to browse, some customers may get fed up and leave, reducing the number of active browsing sessions and thereby reducing the arrival rate of requests for individual web pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another example is a system with a finite queue. If there is a burst of load that fills the queue, subsequent requests will be turned away until there is space. This heavy-load behavior can be modeled as either a reduced arrival rate or a reduced average service time (some tasks are &#8220;processed&#8221; by being discarded). </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1300004 name=x1-1300004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Multiple Servers</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Many real systems have not just one but multiple servers. Does it matter whether there is a single queue for everyone or a separate queue per server? Real systems take both approaches: supermarkets tend to have a separate queue per cashier; banks tend to have a single shared queue for bank tellers. Some systems do both: airports often have a single queue at security but have separate queues for the parking garage. Which is better for response time? </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Clearly, there are often efficiency gains from having separate queues. Multiprocessor schedulers use separate queues for affinity scheduling and to reduce switching costs; in a supermarket, it may not be practical to have a single queue. On the other hand, users often consider a single (FIFO) queue to be fairer than separate queues. It often seems that we always end up in the slowest line at the supermarket, even if that cannot possibly be true for everyone. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If we focus on average response time, however, a single queue is always better than separate queues, provided that users are not allowed to jump lanes. The reason is simple: because of variations in how long each task takes to service, one server can be idle while another server has multiple queued tasks. Likewise, a single fast server is always better for response time than a large number of slower servers of equal aggregate capacity to the fast server. There is no difference when all servers are busy, but the single fast server will process requests faster when there are fewer active tasks than servers. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1310004 name=x1-1310004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Secondary Bottlenecks</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">If a processor is 90% busy serving web requests, and we add another processor to reduce its load, how much will that improve average response time? Unfortunately, there is not enough information to say. You might like to believe that it will reduce response time by a considerable amount, from R = S / (1 - 0.9) = 10S to R = S / (1 - 0.45) = 1.8S. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, suppose each web request needs not only processing time, but also disk I/O and network bandwidth. If the disk was 80% busy beforehand, it will appear that the processor utilization was the primary problem. Once you add an extra processor, however, the disk becomes the new limiting factor to good performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In some cases, queueing theory can make a specific prediction as to the impact of improving one part of a system in isolation. For example, if arrival times are exponentially distributed and independent of the system response time, and if the service times at the processor, disk, and network are also exponentially distributed and independent of one another, then the overall response time for the system is just the sum of the response times of the components: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">R </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#8721; <SUB>i</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; S<SUB>i</SUB> / (1 - U<SUB>i</SUB>) </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this case, improving one part of the system will affect just its contribution to the aggregate system response time. Even though these conditions may not always hold, this is often useful as an approximation to what will occur in real life. </FONT><A id=x1-131001r224 name=x1-131001r224></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.5.5 </FONT><A id=x1-1320005 name=x1-1320005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Lessons</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">To summarize, almost all real-world systems exhibit some randomness in their arrival process or their service times, or both. For these systems: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Response time increases with increased load. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">System performance is predictable across a range of load factors if we can estimate the average service time per request. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Burstiness increases average response time. It is mathematically convenient to assume an exponential distribution, but many real-world systems exhibit more burstiness and therefore worse user performance. </FONT></P></LI></UL><A id=x1-132001r214 name=x1-132001r214></A><A id=x1-1330006 name=x1-1330006>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.6 Overload Management</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Many systems operate without any direct control over their workload. In the previous section, we explained that good response time and low variance in the response time are both predicated on operating well below peak utilization. If your web service generates interest on Slashdot, however, you can suddenly receive a ton of traffic from new users. Success! Except that the new users discover your service has horrible performance. Disaster! </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">More sophisticated scheduling can help at low to moderate load, but if the load is more than system can handle, response time will spike, even for short tasks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The key idea in overload management is to design your system to do less work when overloaded. This will seem strange! After all, you want your system to work a particular way; how can you cripple the user&#8217;s experience just when your system becomes popular? Under overload conditions, however, your system is incapable of serving all of the requests in the normal way. The only question is: do you choose what to disable, or do you let events choose for you? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An obvious step is to simply reject some requests in order to preserve reasonable response time for the remaining ones. While this can seem harsh, it is also pragmatic. Under overload, the only way to give anyone good service is to reduce or eliminate service for others. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The approach of turning away requests under overload conditions is common in streaming video applications. An overloaded movie service will reject requests to start new streams so that it can continue to provide good streaming service to users that have already started. Likewise, during the NCAA basketball tournament or during the Olympics, the broadcaster will turn requests away, rather than giving everyone poor service. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An apt analogy, perhaps, is that of a popular restaurant. Why not set out acres of tables so that everyone who shows up at the restaurant can be seated? If the waiters Round Robin among the various tables, you can be seated, but wait an hour to get a menu, then wait another hour to make an order, and so forth. That is one way of dealing with a persistent overload situation &#8212; by making the user experience so unpleasant that none of your customers will return! As absurd as this scenario is, however, it is close to how we allocate scarce space on congested highways &#8212; by making everyone wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A less obvious step is to somehow reduce the service time per request under overload conditions. A good example of this happened on September 11, 2001 when CNN&#8217;s web page was overwhelmed with people trying to get updates about the terrorist attacks. To make the site usable, CNN shifted to a static page that was less personalized and sophisticated but that was faster to serve. As another example, when experiencing unexpected load, EBay will update its auction listings less frequently, saving work that can be used for processing other requests. Finally, an overloaded movie service can reduce the bit rate for everyone in order to serve more simultaneous requests at slightly lower quality. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Amazon has designed its web site to always return a result quickly, even when the requested data is unavailable due to overload conditions. Every backend service has both a normal interface and a fallback to use if its results are not ready in time. For example, this means a user can be told that their purchase will be shipped shortly, even when the book is actually out of stock. This is a strategic decision that it is better to give a wrong answer quickly, and apologize later, rather than to wait to give the right answer more slowly. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, many systems have the opposite problem: they do more work per request as load increases. A simple example of this would be using a linked list to manage a queue of requests: as more requests are queued, more processing time is used maintaining the queue and not getting useful work done. If amount of work per task increases as the load increases, then response times will soar even faster with increased utilization, and throughput can decrease as we add load. This makes overload management even more important. </FONT><A id=x1-13300122 name=x1-13300122></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00437.gif" data-calibre-src="OEBPS/Images/image00437.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.22: </B>Measured throughput (cars per hour) versus occupancy (percentage of the road covered with vehicles). Each data point represents a separate observation. At low load, throughput increases linearly; once load passes a critical point, adding vehicles decreases average throughput. As each vehicle moves more slowly, it takes more time on the highway to complete its journey, increasing load. Data reprinted from Nagel and Schreckenberg&nbsp;[</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "Xnagel"}'><FONT style="BACKGROUND-COLOR: #7be1e1">122</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">].</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A real-life example of this phenomenon is with highway traffic. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-13300122"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.22</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> provides measured data of throughput versus load for one stretch of highway. As you add cars to an empty highway, it increases the rate that cars traverse a given point on the highway. However, at very high loads, the density of cars causes a transition to stop and go traffic, where the rate of progress is much slower than when there were fewer cars. A common solution for highways is to use onramp limiters &#8212; to limit the rate that new cars can enter the highway if the system is close to overload. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Time-slicing in the presence of caches has similar behavior. When load is low, there are few time slices, and every task uses its cache efficiently. As more tasks are added to the system, there are more time slices and fewer cache hits, slowing down the processor just when we need it to be running at peak efficiency. In networks, packets are dropped when the network is overloaded. Without careful protocol design, this can cause the sender to retransmit packets, further overloading the network. TCP congestion control, now a common part of almost every Internet connection, was developed precisely to deal with this effect. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">You may have even experienced this issue. Some students, as homework piles up, become less, rather than more, efficient. After all, it is hard to concentrate on one project if you know that you really ought to be also working on a different one. But if you decide to take the lessons of this textbook to heart and decide to blow off some of your homework to get the rest of your assignments done, let us suggest that you choose some class other than operating systems! </FONT><A id=x1-133002r230 name=x1-133002r230></A></P><A id=x1-1340007 name=x1-1340007>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.7 Case Study: Servers in a Data Center</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">We can illustrate the application of the ideas discussed in this chapter, by considering how we should manage a collection of servers in a data center to provide responsive web service. Many web services, such as Google, Facebook, and Amazon, are organized as a set of front-end machines that redirect incoming requests to a larger set of back-end machines. We illustrate this in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-13500123"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.23</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. This architecture isolates clients from the architecture of the back-end systems, so that more capacity can be added to the back-end simply by changing the configuration of the front-end systems. Back-end servers can also be taken off-line, have their software upgraded, and so forth, completely transparently to clients. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To provide good response time to the clients of the web service: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When clients first connect to the service, the front-end node assigns each customer to a back-end server to balance load. Customers can be spread evenly across the back-end servers or they can be assigned to a node with low current load, much as customers at a supermarket select the shortest line for a cashier. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Additional requests from the same client can be assigned to the same back-end server, as a form of affinity scheduling. Once a server has fetched client data, it will be faster for it to handle additional requests. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We need to prevent individual users from hogging resources, because that can disrupt performance for other users. A back-end server can favor short tasks over long ones; they can also keep track of the total resources used by each client, reducing the scheduling priority of any client consuming more than their fair share of resources. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is often crucial to the usability of a web service to keep response time small. This requires monitoring both the rate of arrivals and the average amount of computing, disk, and network resources consumed by each request. Back-end servers should be added before average server utilization gets too high. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since it often takes considerable time to bring new servers online, we need to predict future load and have a backup plan for overload conditions.</FONT></P></LI></UL><A id=x1-134001r232 name=x1-134001r232></A><A id=x1-1350008 name=x1-1350008>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">7.8 Summary and Future Directions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Resource scheduling is an ancient topic in computer science. Almost from the moment that computers were first multiprogrammed, operating system designers have had to decide which tasks to do first and which to leave for later. This decision &#8212; the system&#8217;s scheduling policy &#8212; can have a significant impact on system responsiveness and usability. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, the cumulative effect of Moore&#8217;s Law has shifted the balance towards a focus on improving response time for users, rather than on efficient utilization of resources for the computer. At the same time, the massive scale of the Internet means that many services need to be designed to provide good response time across a wide range of load conditions. Our goal in this chapter is to give you the conceptual basis for making those design choices. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several ongoing trends pose new and interesting challenges to effective resource scheduling. </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multicore systems.</B> Although almost all new servers, desktops, laptops and smartphones are multicore systems, relatively few widely used applications have been redesigned to take full advantage of multiple processors. This is likely to change over the next few years as multicore systems become ubiquitous and as they scale to larger numbers of processors per chip. Although we have the concepts in place to manage resource sharing among multiple parallel applications, commercial systems are only just now starting to deploy these ideas. It will be interesting to see how the theory works out in practice. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Cache affinity.</B> Over the past twenty years, processor architects have radically increased both the size and number of levels of on-chip caches. There is little reason to believe that this trend will reverse. Although processor clock rates are improving slowly, transistor density is still increasing at a rapid rate. This will make it both possible and desirable to have even larger, multi-level on-chip caches to achieve good performance. Thus, it is likely that scheduling for cache affinity will be an even larger factor in the future than it is today. Balancing when to respect affinity and when to migrate is still somewhat of an open question, as is deciding how to spread or coalesce application threads across caches. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Energy-aware scheduling.</B> The number of energy-constrained computers such as smartphones, tablets, and laptops, now far outstrips powered computers such as desktops and servers. As a result, we are likely to see the development of hardware to monitor and manage energy use by applications, and the operating system will need to make use of that hardware support. We are likely to see operating systems sandbox application energy use to prevent faulty or malicious applications from running down the battery. Likewise, just as applications can adapt to changing numbers of processors, we are likely to see applications that adapt their behavior to energy availability. </FONT></P></LI></UL><A id=x1-13500123 name=x1-13500123></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00438.gif" data-calibre-src="OEBPS/Images/image00438.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.23: </B>A web service often consists of a number of front-end servers who redirect incoming client requests to a larger set of back-end servers.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=Q1-1-235 name=Q1-1-235></A><A id=Q1-1-236 name=Q1-1-236></A><A id=x1-1360008 name=x1-1360008>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=problems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For shortest job first, if the scheduler assigns a task to the processor, and no other task becomes schedulable in the meantime, will the scheduler ever preempt the current task? Why or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Devise a workload where FIFO is pessimal &#8212; it does the worst possible choices &#8212; for average response time. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you do your homework assignments in SJF-order. After all, you feel like you are making a lot of progress! What might go wrong? </FONT>
<P></P>
<LI>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Given the following mix of tasks, task lengths, and arrival times, compute the completion and response time for each task, along with the average response time for the FIFO, RR, and SJF algorithms. Assume a time slice of 10 milliseconds and that all times are in milliseconds. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Task</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Length</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Arrival Time</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Completion Time</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Response Time</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 85 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 30 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 10 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 35 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 15 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 20 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 80 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 50 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 85 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>Average:</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Is it possible for an application to run slower when assigned 10 processors than when assigned 8? Why or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose your company is considering using one of two candidate scheduling algorithms. One is Round Robin, with an overhead of 1% of the processing power of the system. The second is a wizzy new system that predicts the future and so it can closely approximate SJF, but it takes an overhead of 10% of the processing power of the system. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assume randomized arrivals and random task lengths. Under what conditions will the simpler algorithm outperform the more complex, and vice versa? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Are there non-trivial workloads for which Multi-level Feedback Queue is an optimal policy? Why or why not? (A trivial workload is one with only one or a few tasks or tasks that last a single instruction.) </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">If a queueing system with one server has a workload of 1000 tasks arriving per second, and the average number of tasks waiting or getting service is 5, what is the average response time per task? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Is it possible for a system in equilibrium to have both bounded average response time and 100% utilization? Why or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For a queueing system with random arrivals and service times, how does the variance in the service time affect the system response time? Briefly explain. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Most round-robin schedulers use a fixed size quantum. Give an argument in favor of and against a small quantum. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Which provides the best average response time when there are multiple servers (e.g., bank tellers, supermarket cash registers, airline ticket takers): a single FIFO queue or a FIFO queue per server? Why? Assume that you cannot predict how long any customer is going to take at the server, and that once you have picked a queue to wait in, you are stuck and cannot change queues. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Three tasks, A, B, and C are run concurrently on a computer system. </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Task A arrives first at time 0, and uses the CPU for 100 ms before finishing. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Task B arrives shortly after A, still at time 0. Task B loops ten times; for each iteration of the loop, B uses the CPU for 2 ms and then it does I/O for 8 ms. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Task C is identical to B, but arrives shortly after B, still at time 0.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming there is no overhead to doing a context switch, identify when A, B and C will finish for each of the following CPU scheduling disciplines: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin with a 1 ms time slice </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin with a 100 ms time slice </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Multilevel feedback with four levels, and a time slice for the highest priority level is 1 ms. </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Shortest job first </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For each of the following processor scheduling policies, describe the set of workloads under which that policy is optimal in terms of minimizing average response time (does the same thing as shortest job first) and the set of workloads under which the policy is pessimal (does the same thing as longest job first). If there are no workloads under which a policy is optimal or pessimal, indicate that. </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">FIFO </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Multilevel feedback queues </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Explain how you would set up a valid experimental comparison between two scheduling policies, one of which can starve some jobs. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">As system administrator of a popular social networking website, you notice that usage peaks during working hours (10am &#8211; 5pm) and the evening (7 &#8211; 10pm) on the US east coast. The CEO asks you to design a system where during these peak hours there will be three levels of users. Users in level 1 are the center of the social network, and so they are to enjoy better response time than users in level 2, who in turn will enjoy better response time than users in level 3. You are to design such a system so that all users will still get some progress, but with the indicated preferences in place. </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Will a fixed priority scheme with pre-emption and three fixed priorities work? Why, or why not? </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Will a UNIX-style multi-feedback queue work? Why, or why not? </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the following preemptive priority-scheduling algorithm based on dynamically changing priorities. Larger numbers imply higher priority. Tasks are preempted whenever there is a higher priority task. When a task is waiting for CPU (in the ready queue, but not running), its priority changes at a rate of a: </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">P(t) = P<SUB>0</SUB> + a &#215; (t - t<SUB>0</SUB>) </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">where t<SUB>0</SUB> is the time at which the task joins the ready queue and P<SUB>0</SUB> is its initial priority, assigned when the task enters the ready queue or is preempted. Similarly, when it is running, the task&#8217;s priority changes at a rate b. The parameters a, b and P<SUB>0</SUB> can be used to obtain many different scheduling algorithms. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">What is the algorithm that results from P<SUB>0</SUB> = 0 and b &gt; a &gt; 0? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">What is the algorithm that results from P<SUB>0</SUB> = 0 and a &lt; b &lt; 0? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose tasks are assigned a priority 0 when they arrive, but they retain their priority when they are preempted. What happens if two tasks arrive at nearly the same time and a &gt; 0 &gt; b? </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">How should we adjust the algorithm to eliminate this pathology? </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">For a computer with two cores and a hyperthreading level of two, draw a graph of the rate of progress of a compute-intensive task as a function of time, depending on whether it is running alone, or with 1, 2, 3, or 4 other tasks. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Implement a test on your computer to see if your answer to the previous problem is correct. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">A countermeasure is a strategy by which a user (or an application) exploits the characteristics of the processor scheduling policy to get as much of the processing time as possible. For example, if the scheduler trusts users to give accurate estimates of how long each task will take, it can give higher priority to short tasks. However, a countermeasure would be for the user to tell the system that the user&#8217;s tasks are short even when they are not. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Devise a countermeasure strategy for each of the following processor scheduling policies; your strategy should minimize an individual application&#8217;s response time (even if it hurts overall system performance). You may assume perfect knowledge &#8212; for example, your strategy can be based on which jobs will arrive in the future, where your application is in the queue, and how long the tasks ahead of you will run before blocking. Your strategy should also be robust &#8212; it should work properly even if there are no other tasks in the system, there are only short tasks, or there are only long running tasks. If no strategy will improve your application&#8217;s response time, then explain why. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Last in first out </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Round robin, assuming tasks are put at the end of the ready list when they become ready to run </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Multilevel feedback queues, where tasks are put on the highest priority queue when they become ready to run </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a computer system running a general-purpose workload. Measured utilizations (in terms of time, not space) are given in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-13600124"}'><FONT style="BACKGROUND-COLOR: #7be1e1">7.24</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-13600124 name=x1-13600124></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Processor utilization </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 20.0% </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Disk </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 99.7%</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Network </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 5.0% </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;7.24: </B>Measured utilizations of a computer system.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For each of the following changes, say what its likely impact will be on processor utilization, and explain why. Is it likely to significantly increase, marginally increase, significantly decrease, marginally decrease, or have no effect on the processor utilization? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Get a faster CPU </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Get a faster disk </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Increase the degree of multiprogramming </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Get a faster network </FONT></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></LI></OL><A id=Q1-1-239 name=Q1-1-239></A><A id=Q1-1-240 name=Q1-1-240></A><A id=Q1-1-241 name=Q1-1-241></A><A id=Q1-1-242 name=Q1-1-242></A><A id=Q1-1-243 name=Q1-1-243></A><A id=Q1-1-244 name=Q1-1-244></A><A id=Q1-1-245 name=Q1-1-245></A><A id=Q1-1-246 name=Q1-1-246></A><A id=Q1-1-247 name=Q1-1-247></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-1370008 name=x1-1370008><BR><BR><FONT style="BACKGROUND-COLOR: #7be1e1">