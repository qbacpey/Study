<STRONG><FONT style="BACKGROUND-COLOR: #7be1e1" color=blue>Operating Systems: Principles and Practice (Second Edition) Volume II : </FONT></STRONG>
<H2 class=chapter_name><I><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=headers>Title:</SPAN></B><SPAN class=RefText> Operating Systems: Principles and Practice (Second Edition) Volume II : 6. Multi-Object Synchronization</SPAN></FONT></FONT></I></H2></A>
<DIV class=chapterQuote>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When two trains approach each other at a crossing, both shall come to a full stop and neither shall start up again until the other has gone. &#8212;<I>Kansas state law, early 1900s</I> </FONT></P>
<DL>
<DT><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DD><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DD></DL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
<BR></FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the previous chapter, we described a key building block for writing concurrent programs: how to design an object that can be shared between multiple threads. In this chapter, we need to go one step further: what happens as programs become more complex, with multiple shared objects and multiple locks? To answer this, we need to reason about the interactions between shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several considerations arise in this context: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiprocessor performance.</B> Modern computers have increasing numbers of processors because of the difficulty of improving single CPU performance. The design of shared objects can have a large impact on multiprocessor performance. For example, a lock protecting a frequently accessed shared object can become a bottleneck, since only one thread can hold the lock at a time. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Correctness.</B> Performance considerations often cause designers to re-engineer their data structures for increased concurrency. Splitting a single shared object into a set of related objects each with their own lock can improve performance. However, it also raises issues of correctness. For programs with multiple shared objects, we face a problem similar to the one faced when reasoning about atomic loads and stores: even if each individual operation on a shared object is atomic, we must reason about interactions of sequences of operations across objects. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Deadlock.</B> One way to help reason about the behavior of operations across multiple objects is to hold multiple locks. This approach raises the possibility of deadlock, where threads are permanently stuck waiting for each other in a cycle. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">No cookbook recipe always works for addressing these challenges. In particular, current techniques have two basic limitations. First, they pose engineering trade-offs. Some solutions are general but complex or expensive; others are simple but slow; still others are simple and cheap but not general. Second, many solutions are inherently <EM>non-modular</EM>: they require reasoning about the global structure of the system and internal implementation details of modules to understand or restrict how different modules can interact. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Chapter roadmap:</B> </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiprocessor Lock Performance.</B> Can we predict when a lock will become a bottleneck on a multiprocessor? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-790001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Lock Design Patterns.</B> If a lock is a bottleneck, can we restructure the program to reduce the problem? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-800002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Lock Contention.</B> If a lock is still a bottleneck after re-structuring, what then? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-850003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multi-Object Atomicity.</B> How can we make a sequence of operations across multiple objects appear atomic to other threads? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-920004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Deadlock.</B> What causes deadlock in multi-threaded programs, and what solutions exist to prevent or break deadlocks? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-960005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">) </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Non-Blocking Synchronization.</B> Are there ways to eliminate locks in complex multi-object programs? (Section </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-1040006"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">)</FONT></P></LI></UL><A id=x1-78001r124 name=x1-78001r124></A><A id=x1-790001 name=x1-790001>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.1 Multiprocessor Lock Performance</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Client-server applications often have ample parallelism for modern multicore architectures with dozens of processors. Each separate client request can be handled by a different thread running on a different processor; this is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:request parallelism"}'>request parallelism</A></EM>. Likewise, server operating systems often have ample parallelism &#8211; applications with large numbers of threads can make a large number of concurrent system calls into the kernel. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even with ample request parallelism, however, performance can often be disappointing. Once locks and condition variables are added to a server application to allow it to process requests concurrently, throughput may be only slightly faster on a fifty-way multiprocessor than on a uniprocessor. Most often, this can be due to three causes: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-79002x1 name=x1-79002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Locking.</B> A lock implies mutual exclusion &#8212; only one thread at a time can hold the lock. As a result, access to a shared object can limit parallelism. </FONT></P>
<LI class=enumerate><A id=x1-79004x2 name=x1-79004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Communication of shared data.</B> The performance of a modern processor can vary by a factor of ten (or more) depending on whether the data needed by the processor is already in its cache or not. Modern processors are designed with large caches, so that almost all of the data needed by the processor will already be stored in the cache. On a uniprocessor, it is rare that the processor needs to wait. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, on a multiprocessor, the situation is different. Shared data protected by a lock will often need to be copied from one cache to another. Shared data is often in the cache of the processor that last held the lock, and it is needed in the cache of the processor that is next to acquire the lock. Moving data can slow critical section performance significantly compared to a uniprocessor. </FONT></P>
<LI class=enumerate><A id=x1-79006x3 name=x1-79006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>False sharing.</B> A further complication is that the hardware keeps track of shared data at a fixed granularity, often in units of a cache entry of 32 or 64 bytes. This reduces hardware management overhead, but it can cause performance problems if multiple data structures with different sharing behavior fit in the same cache entry. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:false sharing"}'>false sharing</A></EM>.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Fortunately, these effects can be reduced through careful design of shared objects. We caution, however, that you should keep your shared object design simple until you have proven, through detailed measurement, that a more complex design is necessary to achieve your performance target. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>The evolution of Linux kernel locking</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The first versions of Linux ran only on uniprocessor machines. To allow Linux to run on multiprocessors, version 2.0 introduced the Big Kernel Lock (BKL) &#8212; a single lock that protected all of the kernel&#8217;s shared data structures. The BKL allowed the kernel to function on multiprocessor machines, but scalability and performance were limited. So, over time, different subsystems and different data structures got their own locks, allowing them to be accessed without holding the BKL. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By version 2.6, Linux has been highly optimized to run well on multiprocessor machines. It now has thousands of different locks, and researchers have demonstrated scalability for a range of benchmarks on a 48 processor machine. Still, the BKL remains in use in a few &#8212; mostly less performance-critical &#8212; parts of the Linux kernel, like the reboot system call, some older file systems, and some device drivers. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To illustrate these concepts, consider a web server with an in-memory cache of recently fetched pages. It is often faster to simply return a page from memory rather than regenerating it from scratch. For example, Google might receive a large number of searches for election results on election night, and there is little reason to do all of the work of a general search in that case. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To implement caching of web pages, the server might have a shared data structure, such as a hash table on the search terms, to point to the cached page if it exists. The hash table is shared among the threads handling client requests, and therefore needs a lock. The hash table is updated whenever a thread generates a new page that is not in the cache. The code might also mark pages that have been recently fetched, to keep them in memory in preference to other requests that do not occur as frequently. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An important question in this design is whether the single lock on the hash table will significantly limit server parallelism. How can we tell if the lock on a shared object is going to be a problem? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A convenient approach is to derive a bound on the performance of a parallel program by assuming that the rest of the program is perfectly parallelizable &#8212; in other words, that the only limiting factor is that only one thread at a time can hold the shared lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose that, on average, a web server spends 5% of each request accessing and manipulating its hash table of recently used web pages. If the hash table is protected by a single lock, what is the maximum possible throughput gain? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The time spent in the critical section is inherently sequential. If we assume all other parts of the server are perfectly parallelizable, then the maximum speedup is a <B>factor of 20</B> regardless of how many processors are used. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As we mentioned earlier, a further complication is that it can take much longer to fetch shared data on a multiprocessor because the data is unlikely to be in the processor cache. If the portion of the program holding the lock is slower on a multiprocessor than on a uniprocessor, the potential gain in throughput can be severely limited. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>In the example above, what is the maximum throughput improvement if the hash table code runs four times slower on a multiprocessor due to the need to move shared data between processor caches? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>The potential throughput improvement would be small even if a large number of processors are used. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Throughput gain </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; &#8804; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; 1&#8725;4 &#215; 0.05 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; = </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp; <B>5</B> </FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can study the effect of cache behavior on multiprocessor performance with a simple experiment. The experiment is a intended only as an illustration; it is not meant a reflection of normal program behavior, but rather as a way of isolating the effect of hardware on the performance of code using shared objects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we set up an array of a thousand shared objects, where each object is a simple integer counter protected by a spinlock. (We use a spinlock rather than a lock to avoid measuring context switch time.) The program iterates through the array. For each item, it acquires the lock, increments the counter, and releases the lock. We repeat the loop a thousand times to improve measurement precision. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the following scenarios: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>One thread, one array.</B> When one thread iterates through the array, incrementing each counter in turn, the test gives the time it takes to acquire and release an array of uncontended locks. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Two threads, two arrays.</B> When two threads iterate through disjoint arrays, this gives the slowdown when doing work in parallel. On most architectures, there is little to no slowdown to parallel execution. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Two threads, one array.</B> When two threads iterate through the <EM>same</EM> array, each lock is acquired by a thread running on one processor, and then, shortly afterwards, acquired by a different thread running on a different processor. Thus, the performance illustrates the added cost of moving the shared object data from one processor to another. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Two threads, alternate elements of one array.</B> To measure the impact of false sharing, one thread can iterate through the array acquiring the odd entries, and the other thread can iterate through the array acquiring the even entries. If there was no effect to false sharing, this would be identical to the two array case &#8212; the threads never use the same data.</FONT></P></LI></UL><A id=x1-790071 name=x1-790071></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">One thread, one array </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">51.2 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Two threads, two arrays </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">52.5 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Two threads, one array </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">197.4 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Two threads, alternating </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">127.3 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.1: </B>Number of CPU cycles to execute a simple critical section to increment a counter. Measurements taken on a 64-core AMD Opteron 6262, with threads assigned to processor cores that do not share a cache. The performance difference between these cases largely disappears when threads are assigned to cores that share an L2 cache.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Table&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-790071"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows example results for a single multiprocessor, a 64-core AMD Opteron; the performance on different machines will vary. The threads were assigned to cores that do not share a cache. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On this machine, there is very little slowdown in critical section performance when threads access disjoint locks. However, critical section execution time slows down by a factor of four when multiple processors access the same data. The slowdown is also significant when false sharing occurs. </FONT><A id=x1-79008r130 name=x1-79008r130></A></P><A id=x1-800002 name=x1-800002>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2 Lock Design Patterns</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">We next discuss a set of approaches that can reduce the impact of locking on multiprocessor performance. Often, the best practice is to start simple, with a single lock per shared object. If an object&#8217;s interface is well designed, then refactoring its implementation to increase concurrency and performance can be done once the system is built and performance measurements can identify any bottlenecks. An adage to follow is: &#8220;It is easier to go from a working system to a working, fast system than to go from a fast system to a fast, working system.&#8221; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We discuss four design patterns to increase concurrency when it is necessary: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Fine-Grained Locking.</B> Partition an object&#8217;s state into different subsets each protected by a different lock. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Per-Processor Data Structures.</B> Partition an object&#8217;s state so that all or most accesses are performed by threads running on the same processor. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Ownership Design Pattern.</B> Remove a shared object from a shared container so that only a single thread can read or modify the object. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Staged Architecture.</B> Divide system into multiple stages, so that only those threads within each stage can access that stage&#8217;s shared data.</FONT></P></LI></UL><A id=x1-80001r125 name=x1-80001r125></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.1 </FONT><A id=x1-810001 name=x1-810001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Fine-Grained Locking</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A simple and widely used approach to decrease contention for a shared lock is to partition the shared object&#8217;s state into different subsets, each protected by its own lock. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:fine-grained locking"}'>fine-grained locking</A></EM>. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The web server cache discussed above provides an example. The cache can use a shared hash table to store and locate recently used web pages; because the hash table is shared, it needs a lock to provide mutual exclusion. The lock is acquired and released at the start and end of each of the hash table methods: put(key, value), value = get(key), and value = remove(key). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the single lock limits performance, an alternative is to have one lock per hash bucket. The methods acquire the lock for bucket b before accessing any record that hashes to that bucket. Provided that the number of buckets is large enough, and no single bucket receives a large fraction of requests, then different threads can use and update the hash table in parallel. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, there is no free lunch. Dividing an object&#8217;s state into different pieces protected by different locks can significantly increase the object&#8217;s complexity. Suppose we want to implement a hash table whose number of hash buckets grows as the number of objects it stores increases. If we have a single lock, this is easy to do. But, what if we use fine-grained locking? Then, the design becomes more complex because we have some methods, like put and get, that operate on one bucket and other methods, like resize, that operate across multiple buckets. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Several solutions are possible: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-81002x1 name=x1-81002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Introduce a readers/writers lock.</B> Suppose we have a readers/writers lock on the overall structure of the hash table (e.g., the number of buckets and the array of buckets) and a mutual exclusion lock on each bucket. Methods that work on a single bucket at a time, such as put and get, acquire the table&#8217;s readers/writers lock in read mode and also acquire the relevant bucket&#8217;s mutual exclusion lock. Methods that change the table&#8217;s structure, such as resize, must acquire the readers/writers lock in write mode; the readers/writers lock prevents any other threads from using the hash table while it is being resized. </FONT></P>
<LI class=enumerate><A id=x1-81004x2 name=x1-81004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Acquire every lock.</B> Methods that change the structure of the hash table, such as resize, must first iterate through every bucket, acquiring its lock, before proceeding. Once resize has a lock on every bucket, it is guaranteed that no other thread is concurrently accessing or modifying the hash table. </FONT></P>
<LI class=enumerate><A id=x1-81006x3 name=x1-81006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Divide the hash key space.</B> Another solution is to divide the hash key space into r regions, to have a mutual exclusion lock for each region, and to allow each region to be resized independently when it becomes heavily loaded. Then, get, put, and resizeRegion each acquire the relevant region&#8217;s mutual exclusion lock.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Which solution is best? It is not obvious. The first solution is simple and appears to allow high concurrency, but acquiring the readers/writers lock even in read mode may have high overhead. For example, we gave an implementation of a readers/writers lock in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> where acquiring a read-only lock involves acquiring a mutual exclusion lock on both entrance and exit. Access to the underlying mutual exclusion lock may become a bottleneck. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The second solution makes resize expensive, but if resize is a rare operation, that may be acceptable. The third solution balances concurrency for get/put against the cost of resize, but it is more complex and may require tuning the number of groups to get good performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Further, these trade-offs may change as the implementation becomes more complex. For example, to trigger resize at appropriate times, we probably need to maintain an additional nObjects count of the number of objects currently stored in the hash table, so whatever locking approach we use would need to be extended to cover this information. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>How might you use fine-grained locking to reduce contention for the lock protecting the shared memory heap in malloc/free or new/delete? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>One approach would be to partition the heap into separate memory regions, each with its own lock.</B> For example, a fast implementation of a heap on a uniprocessor uses n buckets, where the ith bucket contains blocks of size 2<SUP>i</SUP>, and serves requests of size 2<SUP>i-1</SUP> + 1 to 2<SUP>i</SUP>. If there are no free blocks in the ith bucket, an item from the next larger bucket i + 1 is split in two. Using fine-grained locking, each bucket can be given its own lock. &#9633; </FONT><A id=x1-81007r133 name=x1-81007r133></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.2 </FONT><A id=x1-820002 name=x1-820002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Per-Processor Data Structures</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A related technique to fine-grained locking is to partition the shared data structure based on the number of processors on the machine. For example, instead of one shared hash table of cached pages, an alternative design would have N separate hash tables, where N is the number of processors. Each thread uses the hash table based on the processor where it is currently running. Each hash table still needs its own lock in case a thread is context switched in the middle of an operation, but in the common case, only threads running on the same processor contend for the same lock. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Often, this is combined with a per-processor ready list, ensuring that each thread preferentially runs on the same processor each time it is context switched, further improving execution speed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An advantage of this approach is better hardware cache behavior; as we saw in the previous section, shared data that must be communicated between processors can slow down the execution of critical sections. Of course, the disadvantage is that the hash tables are now partitioned, so that a web page may be cached in one processor&#8217;s hash table, and needed in another. Whether this is a performance benefit depends on the relative impact of reducing communication of shared data versus the decreased effectiveness of the cache. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>How might you use per-processor data structures to reduce contention for the memory heap? Under what conditions would this work well? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B><B>The heap can be partitioned into N separate memory regions, one for each processor.</B> Calls to malloc/new would use the local heap; free/delete would return the data to the heap where it was allocated. <B>This would perform well provided that (i) rebalancing the heaps was rare and (ii) most allocated data is freed by the thread that acquires it.</B> &#9633; </FONT><A id=x1-82001r134 name=x1-82001r134></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.3 </FONT><A id=x1-830003 name=x1-830003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Ownership Design Pattern</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A common synchronization technique in large, multi-threaded programs is an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:ownership design pattern"}'>ownership design pattern</A></EM>. In this pattern, a thread removes an object from a container and can then access the object without holding a lock: the program structure guarantees that at most one thread owns an object at a time. </FONT><A id=x1-830012 name=x1-830012></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00403.gif" data-calibre-src="OEBPS/Images/image00403.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.2: </B>A multi-stage server based on the ownership pattern. In the first stage, one thread exclusively owns each network connection. In later stages, one thread parses and renders a given object at a time.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As an example, a single web page can contain multiple objects, including HTML frames, style sheets, and images. Consider a multi-threaded web browser whose processing is divided into three stages: receiving an object via the network, parsing the object, and rendering the object (see Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-830012"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">). The first stage has one thread per network connection; the other stages have several worker threads, each of which processes one object at a time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The work queues between stages coordinate object ownership. Objects in the queues are not being accessed by any thread. When a worker thread in the <EM>parse</EM> stage removes an object from the stage&#8217;s work queue, it owns the object and has exclusive access to it. When the thread is done parsing the object, it puts it into the second queue and stops accessing it. A worker thread from the <EM>render</EM> stage then removes it from the second queue, gaining exclusive access to it to render it to the screen. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>How might you use the ownership design pattern to reduce contention for the memory heap? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Ownership can be seen as an extension of per-processor data structures; instead of one heap per processor, <B>we can have one heap per thread.</B> Provided that the same thread that allocates memory also frees it, the thread can safely use its own heap without a lock and only return to the global heap when the local heap is out of space. &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Commutative interface design</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Class and interface design can often constrain implementations in ways that require locking. An example is the UNIX API. Like most operating systems, the UNIX open system call returns a file handle that is used for further operations on the file; the same system call is also used to initialize a network socket. The open call gives the operating system the ability to allocate internal data structures to track the current state of the file or network socket, and more broadly, which files and sockets are in use. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">UNIX also specifies that each successive call to open returns the next integer file handle; as we saw in Chapter&nbsp;3, the UNIX shell uses this feature when redirecting stdin and stdout to a file or pipe. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A consequence of the design of the UNIX API is that the implementation of open requires a lock. For early UNIX systems, this was not an issue, but modern multi-threaded web servers open extremely large numbers of network sockets and files. Because of the semantics of the API, the implementation of open cannot use fine-grained locking or a per-processor data structure. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A better choice, where possible, is to design the API to be <EM>commutative</EM>: the result of two calls is the same regardless of which call was made first. For example, if the implementation can return any unique integer as a file handle, rather than the next successive one, then the implementation could allocate out of a per-processor bucket of open file handles. The implementation would then need a lock only for the special case of allocating specific handles such as stdin and stdout. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV><A id=x1-83002r135 name=x1-83002r135></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.2.4 </FONT><A id=x1-840004 name=x1-840004></A><FONT style="BACKGROUND-COLOR: #7be1e1">Staged Architecture</FONT></H4><A id=x1-840013 name=x1-840013></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00404.gif" data-calibre-src="OEBPS/Images/image00404.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.3: </B>A staged architecture for a simple web server.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
The <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:staged architecture"}'>staged architecture</A></EM> pattern, illustrated in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-840013"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, divides a system into multiple subsystems, called stages. Each stage includes state private to the stage and a set of one or more worker threads that operate on that state. Different stages communicate by sending messages to each other via shared producer-consumer queues. Each worker thread repeatedly pulls the next message from a stage&#8217;s incoming queue and then processes it, possibly producing one or more messages for other stages&#8217; queues. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-840013"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows a staged architecture for a simple web server that has a first <EM>connect</EM> stage that uses one thread to set up network connections and that passes each connection to a second <EM>read and parse</EM> stage. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The <EM>read and parse</EM> stage has several threads, each of which repeatedly gets a connection from the incoming queue, reads a request from the connection, parses the request to determine what web page is being requested, and checks to see if the page is already cached. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Assuming the page is not already cached, if the request is for a static web page (e.g., an HTML file), the <EM>read and parse</EM> stage passes the request and connection to the <EM>read static page</EM> stage, where one of the stage&#8217;s threads reads the specified page from disk. Otherwise, the <EM>read and parse</EM> stage passes the request and connection to the <EM>generate dynamic page</EM> stage, where one of the stage&#8217;s threads runs a program that dynamically generates a page in response to the request. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Once the page has been fetched or generated, the page and connection are passed to the <EM>send page</EM> stage, where one of the threads transmits the page over the connection. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The key property of a staged architecture is that the state of each stage is private to that stage. This improves modularity, making it easier to reason about each stage individually and about interactions across stages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As an example of the modularity benefits, consider a system where different stages are produced by different teams or even different companies. Each stage can be designed and tested almost independently, and the system is likely to work as expected when the stages are brought together. For example, it is common practice for a web site to use a web server from one company and a database from another company and for the two to communicate via messages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another benefit is improved cache locality. A thread operating on a subset of the system&#8217;s state may have better cache behavior than a thread that accesses state from all stages. On the other hand, for some workloads, passing a request from stage to stage could hurt cache behavior compared to doing all of the processing for a request on one processor. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Also note that for good performance, the processing in each stage must be large enough to amortize the cost of sending and receiving messages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The special case of exactly one thread per stage is <EM>event-driven programming</EM>, described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. With event-driven programming, there is no concurrency within a stage, so no locking is required. Each message is processed atomically with respect to that stage&#8217;s state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One challenge with staged architectures is dealing with overload. System throughput is limited by the throughput of the slowest stage. If the system is overloaded, the slowest stage will fall behind, and its work queue will grow. Depending on the system&#8217;s implementation, two bad things could happen. First, the queue could grow indefinitely, consuming more and more memory until the system memory heap is exhausted. Second, if the queue is limited to a finite size, once that size is reached, earlier stages must either discard work for the overloaded stage or block until the queue has room. Notice that if they block, then the backpressure will limit the throughput of earlier stages to that of the bottleneck stage, and their queues in turn may begin to grow. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One solution is to dynamically vary the number of threads per stage. If a stage&#8217;s incoming queue is growing, the program can shift processing resources to it by reducing the number of threads for a lightly-loaded stage in favor of more threads for the stage that is falling behind. </FONT><A id=x1-84002r132 name=x1-84002r132></A></P><A id=x1-850003 name=x1-850003>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.3 Lock Contention</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Sometimes, even after applying the techniques described in the previous section, locking may remain a bottleneck to good performance on a multiprocessor. For example, with fine-grained locking of a hash table, if a bucket contains a particularly popular item, say the cached page for Justin Bieber, then the lock on that bucket can be a source of contention. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this section, we discuss two alternate implementations of the lock abstraction that work better for locks that are bottlenecks: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>MCS Locks.</B> MCS is an implementation of a spinlock optimized for the case when there are a significant number of waiting threads. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>RCU Locks.</B> RCU is an implementation of a reader/writer lock, optimized for the case when there are many more readers than writers. RCU reduces the overhead for readers at a cost of increased overhead for writers. More importantly, RCU has somewhat different semantics than a normal reader/writer lock, placing a burden on the user of the lock to understand its dangers.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although both approaches are used in modern operating system kernels, we caution that neither is a panacea. They should only be used once profiling has shown that the lock is a source of contention and no other options are available. </FONT><A id=x1-85001r137 name=x1-85001r137></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.3.1 </FONT><A id=x1-860001 name=x1-860001></A><FONT style="BACKGROUND-COLOR: #7be1e1">MCS Locks</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Recall that the lock implementation described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> was tuned for the common case where the lock was usually FREE. Is there an efficient implementation of locks when the lock is usually BUSY? </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unfortunately, the overhead of acquiring and releasing a lock can <EM>increase</EM> dramatically with the number of threads contending for the lock. For a contended lock, this can further increase the number of threads waiting for the lock. Consider again the example we used earlier, of a spinlock protecting a shared counter: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;Counter::Increment()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(test_and_set(&amp;lock))&nbsp;//&nbsp;while&nbsp;BUSY
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//&nbsp;spin
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value++;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock&nbsp;=&nbsp;FREE;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Even if many threads try to increment the same counter, only one thread at a time can execute the critical section; the other threads must wait their turn. As we observed earlier, because the counter value must be communicated from one lock holder to the next, the critical section will take significantly longer on a multiprocessor than on a single processor. </FONT><A id=x1-860014 name=x1-860014></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00405.gif" data-calibre-src="OEBPS/Images/image00405.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.4: </B>The overhead of three alternative lock implementations as a function of the number of processors contending for the lock: (a) test-and-set, (b) test and test-and-set, and (c) MCS. Measurements taken on a 64-core AMD Opteron 6262. The non-smooth curves are typical of measurements of real systems.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, the situation with multiple waiting threads is even worse. The time to execute a critical section protected by a spinlock increases linearly with the number of spinning processors. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-860014"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.4</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this effect. The problem is that before a processor can execute an atomic read-modify-write instruction, the hardware must obtain exclusive access to that memory location. Any other read-modify-write instruction must occur either before or afterwards. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Thus, if a number of processors are executing a spin loop, they will all be trying to gain exclusive access to the memory location of the lock. The store instruction to clear the lock also needs exclusive access, and the hardware has no way to know that it should prioritize the lock release ahead of the competing requests to see if the lock is free. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might think that it would help to check that the lock is free before trying to acquire it with a test-and-set; this is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:test and test-and-set"}'>test and test-and-set</A></EM>: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;void&nbsp;Counter::Increment()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(lock&nbsp;==&nbsp;BUSY&nbsp;||&nbsp;test_and_set(&amp;lock))&nbsp;//&nbsp;while&nbsp;BUSY
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//&nbsp;spin
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;value++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock&nbsp;=&nbsp;FREE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, it turns out this does not help. When the lock is released, the new value of the lock, FREE, must be communicated to the other waiting processors. On modern systems, each processor separately fetches the data into its cache. Eventually one of them gets the new value and acquires the lock. If the critical section is not very long, the other processors will still be busy fetching the new value and trying to acquire the lock, preventing the lock release from completing. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One approach is to adjust the frequency of polling to the length of time that the thread has been waiting. A more scalable solution is to assign each waiting thread a separate memory location where it can spin. To release a lock, the bit is set for <EM>one</EM> thread, telling it that it is the next to acquire the lock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The most widely used implementation of this idea is known as the MCS lock, after the initials of its authors, Mellor-Crummey and Scott. The MCS lock takes advantage of an atomic read-modify-write instruction called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:compare-and-swap"}'>compare-and-swap</A></EM> that is supported on most modern multiprocessor architectures. Compare-and-swap tests the value of a memory location and swaps in a new value if the old value has not changed. </FONT><A id=x1-860025 name=x1-860025></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;MCSLock&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;*tail&nbsp;=&nbsp;NULL;
&nbsp;}
&nbsp;
&nbsp;MCSLock::release()&nbsp;{
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(compare_and_swap(&amp;tail,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB,&nbsp;NULL))&nbsp;{
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;If&nbsp;tail&nbsp;==&nbsp;myTCB,&nbsp;no&nbsp;one&nbsp;is
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;waiting.&nbsp;MCSLock&nbsp;is&nbsp;now&nbsp;free.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Someone&nbsp;is&nbsp;waiting.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(myTCB-&gt;next&nbsp;==&nbsp;NULL)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//&nbsp;spin&nbsp;until&nbsp;next&nbsp;is&nbsp;set
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Tell&nbsp;next&nbsp;thread&nbsp;to&nbsp;proceed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB-&gt;next-&gt;needToWait&nbsp;=&nbsp;FALSE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;
&nbsp;MCSLock::acquire()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue&nbsp;*oldTail&nbsp;=&nbsp;tail;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB-&gt;next&nbsp;=&nbsp;NULL;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(!compare_and_swap(&amp;tail,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oldTail,&nbsp;&amp;myTCB))&nbsp;{
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Try&nbsp;again&nbsp;if&nbsp;someone&nbsp;else
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;changed&nbsp;tail&nbsp;in&nbsp;the&nbsp;meantime.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oldTail&nbsp;=&nbsp;tail;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;If&nbsp;oldTail&nbsp;==&nbsp;NULL,&nbsp;lock&nbsp;acquired.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(oldTail&nbsp;!=&nbsp;NULL)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Need&nbsp;to&nbsp;wait.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;myTCB-&gt;needToWait&nbsp;=&nbsp;TRUE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;oldTail-&gt;next&nbsp;=&nbsp;myTCB;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(myTCB-&gt;needToWait)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;//spin
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.5: </B>Pseudo-code for an MCS queueing lock, where each waiting thread spins on a separate memory location in its thread control block (myTCB). The operation, compare-and-swap, atomically inserts the TCB at the tail of the queue.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-860036 name=x1-860036></A>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00406.gif" data-calibre-src="OEBPS/Images/image00406.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.6: </B>The behavior of the MCS queueing lock. Initially (a), tail is NULL indicating that the lock is FREE. To acquire the lock (b), thread A atomically sets tail to point to A&#8217;s TCB. Additional threads B and C queue by adding themselves (atomically) to the tail (c) and (d); they then spin on their respective TCB&#8217;s needToWait flag. Thread A hands the lock to B by clearing B&#8217;s needToWait flag (e); B hands the lock to C by clearing C&#8217;s needToWait fla (f). C releases the lock by setting tail back to NULL (a) iff no one else is waiting &#8212; that is, iff tail still points to C&#8217;s TCB.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Compare-and-swap can be used to build a queue of waiting threads, without a separate spinlock. A waiting thread atomically adds itself to the <EM>tail</EM> of the queue, and then spins on a flag in its queue entry. When a thread releases the lock, it sets the flag in the next queue entry, signaling to the thread that its turn is next. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-860025"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> provides an implementation, and Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-860036"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.6</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates the algorithm in action. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because each thread in the queue spins on its own queue entry, the lock can be passed efficiently from one thread to another along the queue. Of course, the overhead of setting up the queue means that an MCS lock is less efficient than a normal spinlock unless there are a large number of waiting threads. </FONT><A id=x1-86004r140 name=x1-86004r140></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.3.2 </FONT><A id=x1-870002 name=x1-870002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Read-Copy-Update (RCU)</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:read-copy-update"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Read-copy-update</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> (RCU) provides high-performance synchronization for data structures that are frequently read and occasionally updated. In particular, RCU optimizes the read path to have extremely low synchronization costs even with a large number of concurrent readers. However, writes can be delayed for a long time &#8212; tens of milliseconds in some implementations. </FONT>
<H5 class=subsubsectionHead><A id=x1-880002 name=x1-880002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Why Not Use a Readers/Writers Lock?</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Standard readers/writers locks are a poor fit for certain types of read-dominated workloads. Recall that these locks allow an arbitrary number of concurrent active readers, but when there is an active writer, no other writer or reader can be active. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The problem occurs when there are many concurrent reads with short critical sections. Before reading, each reader must acquire a readers/writers lock in read mode and release it afterwards. On both entrance and exit, the reader must update some state in the readers/writers synchronization object. Even when there are only readers, the readers/writers synchronization object can become a bottleneck. This limits the rate at which readers can enter the critical section, because they can only acquire the lock one at a time. For critical sections of less than a few thousand cycles, and for programs with dozens of threads simultaneously reading a shared object, the standard readers/writers lock can limit performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">While the readers/writers synchronization object could be implemented with an MCS lock and thereby reduce some of the effects of lock contention, it does not change the inherent serial access of the readers/writers control structure. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-890002 name=x1-890002></A><FONT style="BACKGROUND-COLOR: #7be1e1">The RCU Approach</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">How can concurrent reads access a data structure &#8212; one that can also be written &#8212; without having to update the state of a synchronization variable on each read? </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To meet this challenge, an RCU lock retains the basic structure of a reader/writers lock: readers (and writers) surround each critical section with calls to acquire and release the RCU lock in read-mode (or write-mode). An RCU lock makes three important changes to the standard interface: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-89002x1 name=x1-89002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Restricted update.</B> With RCU, the writer thread must <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:publish"}'>publish</A></EM> its changes to the shared data structure with a single, atomic memory write. Typically, this is done by updating a single pointer, as we illustrate below by using RCU to update a shared list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although restricted updates might seem to severely limit the types of data structure operations that are possible under RCU, this is not the case. A common pattern is for the writer thread to make a <EM>copy</EM> of a complex data structure (or a portion of it), update the copy, and then publish a pointer to the copy into a shared location where it can be accessed by new readers. </FONT></P>
<LI class=enumerate><A id=x1-89004x2 name=x1-89004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Multiple concurrent versions.</B> RCU allows any number of read-only critical sections to be in progress at the same time as the update. These read-only critical sections may see the old or new version of the data structure. </FONT></P>
<LI class=enumerate><A id=x1-89006x3 name=x1-89006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Integration with the thread scheduler.</B> Because there may be readers still in progress when an update is made, the shared object must maintain multiple versions of its state, to guarantee that an old version is not freed until all readers have finished accessing it. The time from when an update is published until the last reader is done with the previous version is called the <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:grace period"}'>grace period</A></EM>. The RCU lock uses information provided by the thread scheduler to determine when a grace period ends. </FONT></P></LI></OL><A id=x1-890077 name=x1-890077></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00407.gif" data-calibre-src="OEBPS/Images/image00407.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.7: </B>Timeline for an update concurrent with several reads for a data structure accessed with read-copy-update (RCU) synchronization.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-890077"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the timeline for the critical sections of a writer and several reader threads under RCU. If a function that reads the data structure completes before a write is published, it sees the old version of the data structure; if a reader begins after a write is published, it sees the new version. But, if a reader begins <EM>before</EM> and ends <EM>after</EM> a write is published, it may see either the old version or the new one. If it reads the updated pointer more than once, it may see the old one and then the new one. Which version it sees depends on which version of the single, atomically-updated memory location it observes. However, the system guarantees that the old version is not deleted until the grace period expires. The deletion of the old version must be delayed until all reads that might observe the old version have completed. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-900002 name=x1-900002></A><FONT style="BACKGROUND-COLOR: #7be1e1">RCU API and Use</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">RCU is a synchronization abstraction that allows concurrent access to a data structure by multiple readers and a single writer at a time. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-900018"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.8</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows a typical API. </FONT><A id=x1-900018 name=x1-900018></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td colSpan=2 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Reader API</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">readLock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Enter read-only critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">readUnlock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Exit read-only critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td colSpan=2 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Writer API</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">writeLock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Enter read-write critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">publish() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Atomically update shared data structure. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">writeUnlock() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Exit read-write critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">synchronize() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Wait for all currently active readers to exit critical section, to allow for garbage collection of old versions of the object. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td colSpan=2 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Scheduler API</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">quiescentState() </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Of the read-only threads on this processor who were active during the most recent RCU::publish, all have exited the critical section. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.8: </B>Sample programming interface for read-copy-update (RCU) synchronization. In Java&#8217;s implementation of RCU locks, synchronize and quiescentState are not needed because the language-level garbage collector automatically detects when old versions can no longer be accessed. In the implementation of RCU in the Linux kernel, synchronize is split into two calls: one to start the grace period, and one to wait until the grace period completes.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A reader calls RCU::readLock and RCU::readUnlock before and after accessing the shared data structure. A writer calls: RCU::writeLock to exclude other writers; RCU::publish to issue the write that atomically updates the data structure so that reads can see the updates; RCU::writeUnlock to let other writers proceed; and RCU::synchronize to wait for the grace period to expire so that the old version of the object can be freed. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-900029"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates, writes are serialized &#8212; only one write can proceed at a time. However, a write can be concurrent with any number of reads. A write can also be concurrent with another write&#8217;s grace period: there may be any number of versions of the object until multiple overlapping grace periods expire. </FONT><A id=x1-900029 name=x1-900029></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00408.gif" data-calibre-src="OEBPS/Images/image00408.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.9: </B>RCU allows one write at a time, and it allows reads to overlap each other and writes. The initial version is v0, and overlapping writes update the version to v1, v2, and then v3.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>For each read in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-900029"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.9</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, which version(s) of the shared state can the read observe? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>If a read overlaps a publish, it can return the published value or the previous value: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Read</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Value Returned</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Reason</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>1</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v0 or v1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v1. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>2</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; After publish v2, before publish v3. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>3</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; After publish v3. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>4</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v0 or v1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v1. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>5</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v1 or v2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v2. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>6</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v0, v1, or v2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Overlaps publish v1 and v2. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">read<SUB>7</SUB> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; v3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; After publish v2. </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><BR><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT><A id=x1-9000310 name=x1-9000310></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;typedef&nbsp;struct&nbsp;ElementS{
&nbsp;&nbsp;&nbsp;int&nbsp;key;
&nbsp;&nbsp;&nbsp;int&nbsp;value;
&nbsp;&nbsp;&nbsp;struct&nbsp;ElementS&nbsp;*next;
&nbsp;}&nbsp;Element;
&nbsp;
&nbsp;class&nbsp;RCUList&nbsp;{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RCULock&nbsp;rcuLock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*head;
&nbsp;&nbsp;&nbsp;public:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;search(int&nbsp;key,&nbsp;int&nbsp;*value);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;insert(Element&nbsp;*item,&nbsp;value);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;remove(int&nbsp;key);
&nbsp;};</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;bool
&nbsp;RCUList::search(int&nbsp;key,&nbsp;int&nbsp;*valuep)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;result&nbsp;=&nbsp;FALSE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*current;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.readLock();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;=&nbsp;head;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(current&nbsp;=&nbsp;head;&nbsp;current&nbsp;!=&nbsp;NULL;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;=&nbsp;current-&gt;next)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(current-&gt;key&nbsp;==&nbsp;key)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*valuep&nbsp;=&nbsp;current-&gt;value;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;TRUE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.readUnlock();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;result;
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.10: </B>Declaration of data structures and API for a linked list that uses RCU for synchronization, and the implementation of a read-only method for searching the linked list using RCU.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: RCU linked list.</B> Figures&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9000310"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.10</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9000411"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.11</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> show how to use RCU locks to implement a linked list that can be accessed concurrently by many readers, while also being updated by one writer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The list data structure comprises an RCU lock and a pointer to the head of the list. Each entry in the list has two data fields &#8212; key and value &#8212; as well as a pointer to the next record on the list. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The search method is read-only: after registering with readLock, it scans down the list until it finds an element with a matching key. If the element is found, the method uses the parameter to return the value field and then returns TRUE. Otherwise, the method returns FALSE&nbsp;to indicate that no matching record was found. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The methods to update the list are more subtle. Each of them is arranged so that a single pointer update is sufficient to publish the new version of the list to the readers. In particular, it is important that insert initialize the data structure <EM>before</EM> updating the head pointer to make the new element visible to readers. </FONT><A id=x1-9000411 name=x1-9000411></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;void
&nbsp;RCUList::insert(int&nbsp;key,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;value)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*item;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;One&nbsp;write&nbsp;at&nbsp;a&nbsp;time.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.writeLock();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Initialize&nbsp;item.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item&nbsp;=&nbsp;(Element*)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;malloc(sizeof(Element));
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item-&gt;key&nbsp;=&nbsp;key;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item-&gt;value&nbsp;=&nbsp;value;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;item-&gt;next&nbsp;=&nbsp;head;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Atomically&nbsp;update&nbsp;list.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.publish(&amp;head,&nbsp;item);
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Allow&nbsp;other&nbsp;writes
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;to&nbsp;proceed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.writeUnlock();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Wait&nbsp;until&nbsp;no&nbsp;reader
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;has&nbsp;old&nbsp;version.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.synchronize();
&nbsp;}</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;bool
&nbsp;RCUList::remove(int&nbsp;key)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;found&nbsp;=&nbsp;FALSE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Element&nbsp;*prev,&nbsp;*current;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;One&nbsp;write&nbsp;at&nbsp;a&nbsp;time.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.WriteLock();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;(prev&nbsp;=&nbsp;NULL,&nbsp;current&nbsp;=&nbsp;head;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;!=&nbsp;NULL;&nbsp;prev&nbsp;=&nbsp;current,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current&nbsp;=&nbsp;current-&gt;next)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(current-&gt;key&nbsp;==&nbsp;key)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;found&nbsp;=&nbsp;TRUE;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Publish&nbsp;update&nbsp;to&nbsp;readers
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(prev&nbsp;==&nbsp;NULL)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.publish(&amp;head,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current-&gt;next);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.publish(&amp;(prev-&gt;next),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;current-&gt;next);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;break;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Allow&nbsp;other&nbsp;writes&nbsp;to&nbsp;proceed.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.writeUnlock();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Wait&nbsp;until&nbsp;no&nbsp;reader&nbsp;has&nbsp;old&nbsp;version.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(found)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rcuLock.synchronize();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;free(current);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;found;
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.11: </B>Implementation of a linked list using RCU for synchronization.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<H5 class=subsubsectionHead><A id=x1-910002 name=x1-910002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Implementing RCU</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">When implementing RCU, the central goal is to minimize the cost of read critical sections: the system must allow an arbitrary number of concurrent readers. Conversely, writes can have high <EM>latency</EM>. In particular, grace periods can be long, with tens of milliseconds from when an update is published until the system can guarantee that no readers are still using the old version. Even so, write <EM>overhead</EM> &#8212; the CPU time needed per write &#8212; should be modest. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A common technique for achieving these goals is to integrate the RCU implementation with that of the thread scheduler. This is in contrast with the readers/writers lock described in the previous chapter, which makes no assumptions about the thread scheduler, but which must track exactly how many readers are active at any given time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In particular, the implementation we present requires two things from the scheduler: (1) read-only critical sections complete without being interrupted and (2) whenever a thread on a processor is interrupted, the scheduler updates some per-processor RCU state. Then, once a write completes, RCULock::Synchronize simply waits for all processors to be interrupted at least once. At that point, the old version of the object is known to be <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:quiescent"}'>quiescent</A></EM> &#8212; no thread has access to the old version (other than the writer who changed it). </FONT><A id=x1-9100112 name=x1-9100112></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;RCULock{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;//&nbsp;Global&nbsp;state
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spinlock&nbsp;globalSpin;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;long&nbsp;globalCounter;
&nbsp;&nbsp;&nbsp;//&nbsp;One&nbsp;per&nbsp;processor
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DEFINE_PER_PROCESSOR(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;static&nbsp;long,&nbsp;quiescentCount);
&nbsp;
&nbsp;&nbsp;&nbsp;//&nbsp;Per-lock&nbsp;state
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Spinlock&nbsp;writerSpin;
&nbsp;
&nbsp;&nbsp;&nbsp;//&nbsp;Public&nbsp;API&nbsp;omitted
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::ReadLock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disableInterrupts();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::ReadUnlock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;enableInterrupts();
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Called&nbsp;by&nbsp;scheduler
&nbsp;void&nbsp;RCULock::QuiescentState(){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PER_PROC_VAR(quiescentCount)&nbsp;=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globalCounter;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::writeLock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writerSpin.acquire();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::writeUnlock()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;writerSpin.release();
&nbsp;}
&nbsp;
&nbsp;void&nbsp;RCULock::publish&nbsp;(void&nbsp;**pp1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;void&nbsp;*p2){
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*pp1&nbsp;=&nbsp;p2;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;memory_barrier();
&nbsp;}
&nbsp;
&nbsp;void
&nbsp;RCULock::synchronize()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;p,&nbsp;c;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globalSpin.acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c&nbsp;=&nbsp;++globalCounter;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;globalSpin.release();
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FOREACH_PROCESSOR(p)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while((PER_PROC_VAR(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;quiescentCount,&nbsp;p)&nbsp;-&nbsp;c)&nbsp;&lt;&nbsp;0)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;release&nbsp;CPU&nbsp;for&nbsp;10ms
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sleep(10);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.12: </B>A quiescence-based RCU implementation. The code assumes that spinlock acquire/release and interrupt enable/disable trigger a memory barrier. <EM>Credit:</EM> This pseudo-code is based on an implementation by Paul McKenney in &#8220;Is Parallel Programming Hard, And, If So, What Can be Done About It?&#8221;</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9100112"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.12</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows an implementation of RCU based on quiescent states. Notice first that readLock and readUnlock are inexpensive: they update no state and merely ensure that the read is not interrupted. RCU::writeLock and writeUnlock are also inexpensive. They acquire and release a spinlock to ensure that at most one write per RCULock can proceed at a time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">RCU::publish is also simple. It executes a memory barrier so that all modifications to the shared object are completed before the pointer is updated. It then updates the pointer, and then executes another memory barrier so that other processors observe the update. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">RCU::synchronize and quiescentState work together to ensure that when synchronize returns, all threads are guaranteed to be done with the old version of the object. RCU::synchronize increments a global counter and then waits until all processors&#8217; match the new value of that counter. RCU::quiescentState is called by the scheduler whenever that processor is interrupted. It updates that processor&#8217;s quiescentCount to match the current globalCounter. Thus, once quiescentCount is at least as large as c, on every processor, synchronize knows that no remaining readers can observe the old version. </FONT><A id=x1-91002r139 name=x1-91002r139></A></P><A id=x1-920004 name=x1-920004>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4 Multi-Object Atomicity</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Once a program has multiple shared objects, it becomes both necessary and challenging to reason about interactions across objects. For example, consider a system storing a bank&#8217;s accounts. A reasonable design choice might be for each customer&#8217;s account to be a shared object with a lock (either a </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-490003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">mutual exclusion lock</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> or a </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-630001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">readers/writers lock</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, as described in Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">). Consider, however, transferring $100 from account A to account B, as follows: </FONT>
<P><BR></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;A-&gt;subtract(100);
   &nbsp;B-&gt;add(100);</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although each individual action is atomic, the sequence of actions is not. As a result, there may be a time where, say, A tells B that the money has been sent, but the money is not yet in B&#8217;s account. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Similarly, consider a bank manager who wants to answer a question: &#8220;How much money does the bank have?&#8221; If the manager&#8217;s program simply reads from each account, the calculation may exclude or double-count money &#8220;in flight&#8221; between accounts, such as in the transfer from A to B. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These examples illustrate a general problem that arises whenever a program contains multiple shared objects. Even if the object guarantees that each method operates atomically, <EM>sequences</EM> of operations by different threads can be interleaved. The same issues of managing multiple locks also apply to fine-grained locking within an object. </FONT><A id=x1-92001r144 name=x1-92001r144></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4.1 </FONT><A id=x1-930001 name=x1-930001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Careful Class Design</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Sometimes it is possible to address this issue through careful class and interface design. This includes the design of individual objects (e.g., specifying clean interfaces that expose the right abstractions). It also includes the architecture of how those objects interact (e.g., structuring a system architecture in well-defined layers). </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, you would face the same issues if you tried to solve Too Much Milk problem with a Note object that has two methods, readNote and writeNote, and a Fridge object with two methods, checkForMilk and addMilk. Atomicity of these individual operations is not sufficient to provide the desired behavior without considerable programming effort. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On the other hand, if we refactor the objects so that we have: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Fridge::checkforMilkAndSetNoteIfNeeded();
   &nbsp;Fridge::addMilk();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Then, the problem becomes straightforward. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This advice may seem obvious: of course, you should strive for elegant designs for both single- and multi-threaded code. Nonetheless, we emphasize that the choices you make for your interfaces, abstractions, and software architecture can dramatically affect the complexity or feasibility of your designs. </FONT><A id=x1-93001r156 name=x1-93001r156></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4.2 </FONT><A id=x1-940002 name=x1-940002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Acquire-All/Release-All</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Better interface design has limits, however. Sometimes, multiple locks are needed for program structure or for greater concurrency. Is there a general technique to perform a set of operations that require multiple locks, so that the group of operations appears atomic? For clarity, we will refer to a group of operations as a <EM>request</EM>. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One approach, called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:acquire-all/release-all"}'>acquire-all/release-all</A></EM> is to acquire <EM>every</EM> lock that may be needed at any point while processing the entire set of operations in the request. Then, once the thread has all of the locks it might need, the thread can execute the request, and finally, release the locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Consider a hash table with one lock per hash bucket. To move an item from one bucket to another, the hash table supports a changeKey(item, k1, k2) operation. With acquire-all/release-all, this function could be implemented to first acquire both the locks for k1 and k2, then remove the item under k1 and insert it under k2, and finally release both locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Acquire-all/release-all allows significant concurrency. When individual requests touch non-overlapping subsets of state protected by different locks, they can proceed in parallel. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A key property of this approach is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:serializability"}'>serializability</A></EM> across requests: the result of any program execution is equivalent to an execution in which requests are processed one at a time in some sequential order. Serializability allows one to reason about multi-step tasks <EM>as if </EM>each task executed alone. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9400113"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates, requests that access non-overlapping data can proceed in parallel. The result is the same as if the system first executed one request and then the other (or equivalently, the reverse). On the other hand, if two requests touch the same data, then the fact that all locks are acquired at the beginning and released at the end implies that one request is completed before the other one begins. </FONT><A id=x1-9400113 name=x1-9400113></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00409.gif" data-calibre-src="OEBPS/Images/image00409.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.13: </B>Locking multiple objects using an acquire-all/release-all pattern results in a serializable execution that is equivalent to an execution where requests are executed sequentially in some order.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One challenge to using this approach is knowing exactly what locks will be needed by a request before beginning to process it. A potential solution is to conservatively acquire more locks than needed (e.g., acquire any locks that <EM>may</EM> be needed by a particular request), but this may be difficult to determine. Without first executing the request, how can we know which locks will be needed? </FONT><A id=x1-94002r157 name=x1-94002r157></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.4.3 </FONT><A id=x1-950003 name=x1-950003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Two-Phase Locking</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:two-phase locking"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Two phase locking</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> refines the acquire-all/release-all pattern to address this concern. Instead of acquiring all locks before processing the request, locks can be acquired as needed for each operation. However, locks are not <EM>released</EM> until all locks needed by the request have been acquired. Most implementations simply release all locks at the end of the request. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Two-phase locking avoids needing to know what locks to grab <EM>a priori</EM>. Therefore, programs can avoid acquiring locks they do not need, and they may not need to hold locks as long. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>The changeKey(item, k1, k2) function for a hash table with per-bucket locks could be implemented to acquire k1&#8217;s lock, remove the item using key k1, acquire k2&#8217;s lock, insert the item using key k2, and release both locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Like acquire-all/release-all, two-phase locking is serializable. If two requests have non-overlapping data, they are commutative and therefore serializable. Otherwise, there is some overlapping data between the two requests, protected by one or more locks. Provided a request completes, it must have acquired all of those locks, and made its changes to the overlapping data, before releasing any of them. Thus, any overlapping request must have read or modified the data in the overlap either entirely before or after the other request. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Unlike acquire-all/release-all, however, two-phase locking can in some cases lead to deadlock, the topic of the next section. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Suppose one thread starts executing changeKey(item, k1, k2) and another thread simultaneously tries to move a different item in the other direction from k2 to k1. If the first thread acquires k1&#8217;s lock and the second thread acquires k2&#8217;s lock, neither will be able to make progress. </FONT><A id=x1-95001r155 name=x1-95001r155></A></P><A id=x1-960005 name=x1-960005>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5 Deadlock</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">A challenge to constructing complex multi-threaded programs is the possibility of deadlock. A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:deadlock"}'>deadlock</A></EM> is a cycle of waiting among a set of threads, where each thread waits for some other thread in the cycle to take some action. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock can occur in many different situations, but one of the simplest is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:mutually recursive locking"}'>mutually recursive locking</A></EM>, shown in the code fragment below: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;A
   &nbsp;
   &nbsp;lock1.acquire();
   &nbsp;lock2.acquire();
   &nbsp;lock2.release();
   &nbsp;lock1.release();</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;B
   &nbsp;
   &nbsp;lock2.acquire();
   &nbsp;lock1.acquire();
   &nbsp;lock1.release();
   &nbsp;lock2.release();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose two shared objects with mutual exclusion locks can call into each other while holding their locks. Deadlock can occur when one thread holds the lock on the first object, and another thread holds the lock on the second object. If the first thread calls into the second object while still holding onto its lock, it will need to wait for the second object&#8217;s lock. If the other thread does the same thing in reverse, neither will be able to make progress. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We can also get into deadlock with two locks and a condition variable, shown below: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;A
   &nbsp;
   &nbsp;lock1.acquire();
   &nbsp;...
   &nbsp;lock2.acquire();
   &nbsp;while&nbsp;(need&nbsp;to&nbsp;wait)&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv.wait(&amp;lock2);
   &nbsp;}
   &nbsp;...
   &nbsp;lock2.release();
   &nbsp;...
   &nbsp;lock1.release();</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;B
   &nbsp;
   &nbsp;lock1.acquire();
   &nbsp;...
   &nbsp;lock2.acquire();
   &nbsp;...
   &nbsp;cv.signal();
   &nbsp;lock2.release();
   &nbsp;...
   &nbsp;lock1.release();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:nested waiting"}'>nested waiting</A></EM>, one shared object calls into another shared object while holding the first object&#8217;s lock, and then waits on a condition variable. CV::wait releases the lock of the second object, but not the first. Deadlock results if the thread that can signal the condition variable needs the first lock to make progress. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The problem of deadlock is much broader than just locks and condition variables. Deadlock can occur anytime a thread waits for an event that cannot happen because of a cycle of waiting for a resource held by the first thread. As in the examples above, resources can be locks, but they can also be any other scarce quantity: memory, processing time, disk blocks, or space in a buffer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we have two bounded buffers, where one thread puts a request into one buffer, and gets a response out of the other. Deadlock can result if another thread does the reverse. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;A
   &nbsp;
   &nbsp;buffer1.put();
   &nbsp;buffer1.put();
   &nbsp;...
   &nbsp;buffer2.get();
   &nbsp;buffer2.get();</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;//&nbsp;Thread&nbsp;B
   &nbsp;
   &nbsp;buffer2.put();
   &nbsp;buffer2.put();
   &nbsp;...
   &nbsp;buffer1.get();
   &nbsp;buffer1.get();</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If the buffers are almost full, both threads will need to wait for there to be room, and so neither will be able to reach the point where they can pull data out of the other buffer to allow the other thread to make progress. </FONT><A id=x1-9600114 name=x1-9600114></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00410.gif" data-calibre-src="OEBPS/Images/image00410.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.14: </B>An example of deadlock where three tractor-trailer trucks enter an intersection without first checking whether they can clear the intersection.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlocks also occur in real life. We encourage you to develop your intuition about deadlocks by considering why deadlocks occur and how we might prevent them. For example, if we lived in a world without stop signs, we might see the deadlock in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9600114"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.14</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> more often. </FONT><A id=x1-9600215 name=x1-9600215></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00411.gif" data-calibre-src="OEBPS/Images/image00411.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.15: </B>In this example of the dining philosophers problem, there are 5 philosophers, 5 plates, and 5 chopsticks.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The scarce resource leading to deadlock can even be a chopstick. The Dining Philosophers problem is a classic illustration of both the challenges and solutions to deadlock; an example is shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9600215"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.15</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. There is a round table with n plates alternating with n chopsticks around the circle. A philosopher sitting at a plate requires two chopsticks to eat. Suppose that each philosopher proceeds by picking up the chopstick on the left, picking up the chopstick on the right, eating, and then putting down both chopsticks. If every philosopher follows this approach, there can be a deadlock: each philosopher takes the chopstick on the left but can be stuck waiting for the philosopher on the right to release the chopstick. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Note that mutually recursive locking is equivalent to Dining Philosophers with n = 2. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The rest of this section addresses the following questions: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Deadlock vs. Starvation.</B> How does deadlock relate to the concepts of liveness and starvation? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Necessary Conditions for Deadlock.</B> What conditions are required for deadlock to be possible? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Preventing Deadlock.</B> What techniques can be used to prevent deadlock? </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>The Banker&#8217;s Algorithm for Avoiding Deadlock.</B> The Banker&#8217;s Algorithm is a general-purpose mechanism for preventing deadlock by exploiting knowledge of what resources may be needed in the future. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Detecting and Recovering From Deadlock.</B> In some systems, deadlock is not prevented but repaired when it occurs. How can we detect deadlock and then recover?</FONT></P></LI></UL><A id=x1-96003r159 name=x1-96003r159></A>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.1 </FONT><A id=x1-970001 name=x1-970001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock vs. Starvation</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Deadlock and starvation are both liveness concerns. In <EM>starvation</EM>, a thread fails to make progress for an indefinite period of time. Deadlock is a form of starvation but with the stronger condition: a group of threads forms a cycle where none of the threads make progress because each thread is waiting for some other thread in the cycle to take action. Thus, deadlock implies starvation (literally, for the dining philosophers), but starvation does not imply deadlock. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, recall the readers/writers example discussed in Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-630001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5.6.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. A writer only waits if a reader or writer is active. In the writers-preferred solution we gave, waiting readers can starve if new writers arrive sufficiently frequently; likewise, waiting writers can starve if there is an active reader, and new readers arrive and become active before the last one completes. Note that such starvation would not be deadlock because there is no cycle. The waiting readers are waiting on the active writers to finish, and the waiting writers are waiting on the active readers to finish, but no active thread is waiting on a waiting reader or writer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Just because a system can suffer deadlock or starvation does not mean that it always will. A system is <EM>subject to starvation</EM> if a thread could starve in some circumstances. A system is <EM>subject to deadlock</EM> if a group of threads could deadlock in some circumstances. Here, the circumstances that affect whether deadlock or starvation occurs could include a broad range of factors, such as: the choices made by the scheduler, the number of threads running, the workload or sequence of requests processed by the system, which threads win races to acquire locks, and which threads are enabled in what order when signals or broadcasts occur. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A system that is subject to starvation or deadlock may be live in many or most runs and starve or deadlock only for particular workloads or &#8220;unlucky&#8221; interleavings. For example, in mutually recursive locking, the deadlock only occurs if both threads obtain the outer locks at about the same time. For the Dining Philosophers problem, philosophers may succeed in eating for a long time before hitting the unlucky sequence of events that causes them to deadlock. Similarly, in the readers/writers example, the writers-preferred solution will allow some reads to complete as long as the rate of writes stays below some threshold. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Since testing may not discover deadlock problems, it is important to construct systems that are deadlock-free by design. </FONT><A id=x1-97001r163 name=x1-97001r163></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.2 </FONT><A id=x1-980002 name=x1-980002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Necessary Conditions for Deadlock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">There are four necessary conditions for deadlock to occur. Knowing these conditions is useful for designing solutions: if you can prevent any one of these conditions, then you can eliminate the possibility of deadlock. </FONT>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-98002x1 name=x1-98002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bounded resources.</B> There are a finite number of threads that can simultaneously use a resource. </FONT></P>
<LI class=enumerate><A id=x1-98004x2 name=x1-98004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>No preemption.</B> Once a thread acquires a resource, its ownership cannot be revoked until the thread acts to release it. </FONT></P>
<LI class=enumerate><A id=x1-98006x3 name=x1-98006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Wait while holding.</B> A thread holds one resource while waiting for another. This condition is sometimes called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:multiple independent requests"}'>multiple independent requests</A></EM> because it occurs when a thread first acquires one resource and then tries to acquire another. </FONT></P>
<LI class=enumerate><A id=x1-98008x4 name=x1-98008x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Circular waiting.</B> There is a set of waiting threads such that each thread is waiting for a resource held by another. </FONT></P></LI></OL><A id=x1-9800916 name=x1-9800916></A><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00412.gif" data-calibre-src="OEBPS/Images/image00412.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.16: </B>Graph representation of the state of a deadlocked Dining Philosophers system. Circles represent threads, boxes represent resources, an arrow from a box/resource to a circle/thread represents an <EM>owned by</EM> relationship, and an arrow from a circle/thread to a box/resource represents a <EM>waiting for</EM> relationship.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Show that the Dining Philosophers meet all four conditions for deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>To see that all four conditions are met, observe that </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-98011x1 name=x1-98011x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bounded resources.</B> Each chopstick can be held by a single philosopher at a time. </FONT></P>
<LI class=enumerate><A id=x1-98013x2 name=x1-98013x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>No preemption.</B> Once a philosopher picks up a chopstick, she does not release it until she is done eating, even if that means no one will ever eat. </FONT></P>
<LI class=enumerate><A id=x1-98015x3 name=x1-98015x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Wait while holding.</B> When a philosopher needs to wait for a chopstick, she continues to hold onto any chopsticks she has already picked up. </FONT></P>
<LI class=enumerate><A id=x1-98017x4 name=x1-98017x4></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Circular waiting.</B> Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9800916"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> maps the state of a deadlocked Dining Philosophers implementation to an abstract graph that shows which resources are <EM>owned by</EM> which threads and which threads <EM>wait for</EM> which resources. In this type of graph, if there is one instance of each type of resource (e.g., a particular chopstick), then a cycle implies deadlock assuming the system does not allow preemption.</FONT></P></LI></OL><FONT style="BACKGROUND-COLOR: #7be1e1">&#9633; </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The four conditions are necessary <EM>but not sufficient</EM> for deadlock. When there are multiple instances of a type of resource, there can be a cycle of waiting without deadlock because a thread not in the cycle may return resources that enable a waiting thread to proceed. </FONT><A id=x1-9801817 name=x1-9801817></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00413.gif" data-calibre-src="OEBPS/Images/image00413.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.17: </B>Graph representation of the state of a Dining Philosophers system that includes a cycle among waiting threads and resources but that is not deadlocked. Circles represent threads, boxes represent resources, dots within a box represent multiple instances of a resource, an arrow from a dot/resource instance to a circle/thread represents an <EM>owned by</EM> relationship and an arrow from a circle/thread to a box/resource represents a <EM>waiting for</EM> relationship.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we have 5 philosophers at a table with 5 chopsticks, but the chopsticks are placed in a tray at the center of the table when not in use. We could be in the state illustrated in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9801817"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, where philosopher 1 has two chopsticks, philosophers 2, 3, and 4 each have one chopstick and are waiting for another chopstick, while philosopher 5 has no chopsticks. In this state, we have bounded resources (five chopsticks), no preemption (we cannot forcibly remove a chopstick from a hungry philosopher&#8217;s hand), wait while holding (philosophers 2, 3 and 4 are holding a chopstick while waiting for another), and circular waiting (each of philosophers 2, 3, and 4 are waiting for a resource held by another of them). However, we do not have deadlock. Eventually, philosopher 1 will release its two chopsticks, which may, for example, allow philosophers 2 and 3 to eat and release their chopsticks. In turn, this would allow philosophers 4 and 5 to eat. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although the system shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9801817"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> is not currently deadlocked, it is still <EM>subject to deadlock</EM>. For example, if philosopher 1 returns two chopsticks, philosopher 5 picks up one, and philosopher 1 picks up the other, then the system would deadlock. </FONT><A id=x1-98019r164 name=x1-98019r164></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.3 </FONT><A id=x1-990003 name=x1-990003></A><FONT style="BACKGROUND-COLOR: #7be1e1">Preventing Deadlock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Preventing deadlock can be challenging. For example, consider a system with three resources &#8212; A, B, and C &#8212; and two threads that access them. Thread 1 acquires A then C then B, and thread 2 acquires B then C then A. The following sequence can lead to deadlock: </FONT>
<P><BR></P>
<TABLE width="100%" border=0>
<TBODY>
<TR>
<TD>
<DIV align=center>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread 1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; <B>Thread 2</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Acquire A </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Acquire B </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Acquire C </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Wait for C </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp; Wait for B </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;&nbsp;&nbsp;</FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV></DIV></TD></TR></TBODY></TABLE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">How could we avoid this deadlock? The deadlock&#8217;s circular waiting occurs when we reach step 5, but our fate was sealed much earlier. In particular, once we complete step 2 and thread 2 acquires B, deadlock is inevitable. To prevent the deadlock, we have to realize at step 2 that it will occur at step 5. Once step 1 completes and thread 1 acquires A, we cannot let thread 2 complete step 2 and acquire B or deadlock will follow. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">This example illustrates that for an arbitrary program, preventing deadlock can take one of three approaches: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-99002x1 name=x1-99002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Exploit or limit the behavior of the program.</B> Often, we can change the behavior of a program to prevent one of the four necessary conditions for deadlock, and thereby eliminate the possibility of deadlock. In the above example, we can eliminate deadlock by changing the program to never wait for B while holding C. </FONT></P>
<LI class=enumerate><A id=x1-99004x2 name=x1-99004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Predict the future.</B> If we can know what threads may or will do, then we can avoid deadlock by having threads wait (e.g., thread 2 can wait at step 2 above) <EM>before</EM> they would head into a possible deadlock. </FONT></P>
<LI class=enumerate><A id=x1-99006x3 name=x1-99006x3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Detect and recover.</B> Another alternative is to allow threads to recover or &#8220;undo&#8221; actions that take a system into a deadlock; in the above example, when thread 2 finds itself in deadlock, it can recover by reverting to an earlier state.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">We discuss these three options in this and the following two sub-sections. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-980002"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.2</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> listed four necessary conditions for deadlock. These conditions are useful because they suggest approaches for preventing deadlock: if a system is structured to prevent at least one of the conditions, then the system cannot deadlock. Considering these conditions in the context of a given system often points to a viable deadlock prevention strategy. Below, we discuss some commonly used approaches. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Bounded resources: Provide sufficient resources.</B> One way to ensure deadlock freedom is to arrange for sufficient resources to satisfy all threads&#8217; demands. A simple example would be to add a single chopstick to the middle of the table in Dining Philosophers; that is enough to eliminate the possibility of deadlock. As another example, thread implementations often reserve space in the TCB for the thread to be inserted into a waiting list or the ready list. While it would be theoretically possible to dynamically allocate space for the list entry only when it is needed, that could open up the chance that the system would run out of memory at exactly the wrong time, leading to deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>No preemption: Preempt resources.</B> Another technique is to allow the runtime system to forcibly reclaim resources held by a thread. For example, an operating system can preempt a page of memory from a running process by copying it to disk in order to prevent applications from deadlocking as they acquire memory pages. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Wait while holding: Release lock when calling out of module.</B> For nested modules, each of which has its own lock, waiting on a condition variable in an inner module can lead to a nested waiting deadlock. One solution is to restructure a module&#8217;s code so that no locks are held when calling other modules. For example, we can change the code on the left to the code on the right, provided that the program does not depend on the three steps occurring atomically: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Module::foo()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doSomeStuff();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherModule-&gt;bar();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doOtherStuff();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
   &nbsp;}
   &nbsp;
   &nbsp;Module::doSomeStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;x&nbsp;+&nbsp;1;
   &nbsp;}
   &nbsp;
   &nbsp;Module::doOtherStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;y&nbsp;-&nbsp;2;
   &nbsp;}</FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;Module::foo()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doSomeStuff();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherModule-&gt;bar();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doOtherStuff();
   &nbsp;}
   &nbsp;
   &nbsp;Module::doSomeStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;x&nbsp;+&nbsp;1;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
   &nbsp;}
   &nbsp;
   &nbsp;Module::doOtherStuff()&nbsp;{
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.acquire();
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y&nbsp;=&nbsp;y&nbsp;-&nbsp;2;
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.release();
   &nbsp;}</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Deadlock and kernel paging</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Early operating systems were often run on machines with very limited amounts of main memory. In response, going back at least as far as Multics, portions of the kernel (both code and data) could be swapped to disk in order to save space. Then, when the code and data was needed, they could be brought into main memory, swapping with some other portion of the kernel that was not currently in use. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A challenge to making this work was deadlock. The code to swap in or out portions of the kernel needed to be kept in memory, along with any code or data it might touch along any possible execution path. Without very strict module layering, it would be easy to miss a dependency that would, in rare cases, trigger a latent deadlock. Often, the only possible repair would be to reboot. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because of the inherent complexity of this approach, most modern operating systems keep all kernel code and almost all data structures memory resident; the one exception is that some kernels still swap the page tables for application virtual memory, a topic we will discuss in Chapter&nbsp;9. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In theory, one could eliminate the risk of deadlocks due to nested monitors by always releasing locks when calling code outside of a module. In practice, doing so is likely to be cumbersome, not only from the extra code needed to acquire and release locks, but also because of the extra thought needed to transform a single atomic method that holds a lock across a series of actions to a sequence of atomic methods that each acquire and release the lock. As a result, programmers often take the decidedly non-modular and admittedly unsatisfying approach of considering whether the outside module being called is likely to wait on something that depends on enclosing monitor lock. If such waiting is unlikely, the call can made with the enclosing lock held. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Circular waiting: Lock ordering.</B> An approach used in many systems is to identify an ordering among locks and only acquire locks in that order. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, C printf acquires a lock to ensure printed messages appear atomic rather than mixed up with those of other threads. Because waiting for that lock does not lead to circular waiting, printf can be safely called while holding most kernel locks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For a hash table with per-bucket locks and an operation changeKeys(item, k1, k2) to move an item from one bucket to another, we can avoid deadlock by always acquiring the lock for the lower-numbered bucket before the one for the higher-numbered bucket. This prevents circular waiting since a thread only waits for threads holding higher-numbered locks. Those threads can be waiting as well, but only for threads with even higher-numbered locks, and so forth. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Likewise, we can eliminate deadlock among the dining philosophers if &#8212; instead of always picking up the chopstick on the left and then the one on the right &#8212; the philosophers number the chopsticks from 1 to n and always pick up the lower-numbered chopstick before the higher-numbered one. </FONT><A id=x1-99007r167 name=x1-99007r167></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.4 </FONT><A id=x1-1000004 name=x1-1000004></A><FONT style="BACKGROUND-COLOR: #7be1e1">The Banker&#8217;s Algorithm for Avoiding Deadlock</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">A general technique to eliminate wait-while-holding is to wait until all needed resources are available and then to acquire them atomically at the beginning of an operation, rather than incrementally as the operation proceeds. We saw this earlier with acquire-all/release-all; it cannot deadlock as long as the implementation acquires all of the locks atomically rather than one at a time. As another example, a dining philosopher might wait until the two neighboring chopsticks are available and then simultaneously pick them both up. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Of course, a thread may not know exactly which resources it will need to complete its work, but it can still acquire all resources that it <EM>might</EM> need. Consider an operating system for mobile phones where memory is constrained and cannot be preempted by copying it to disk. Rather than having applications request additional memory as needed, we might instead have each application state its maximum memory needs and allocate that much memory when it starts. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Disadvantages of this approach include: the effect on program modularity, the challenge of having applications accurately estimate their worst-case needs, and the cost of allocating significantly more resources than may be necessary in the common case. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Dijkstra developed the Banker&#8217;s Algorithm as a way to improve on the performance of acquire-all. Although few systems use it in its full generality, we include the discussion because simplified versions of the algorithm are common. The Banker&#8217;s Algorithm also sheds light on the distinction between <EM>safe</EM> and <EM>unsafe</EM> states and how the occurrence of deadlocks often depends on a system&#8217;s workload and sequence of operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In the Banker&#8217;s Algorithm, a thread states its maximum resource requirements when it begins a task, but it then acquires and releases those resources incrementally as the task runs. The runtime system delays granting some requests to ensure that the system never deadlocks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The insight behind the algorithm is that a system that may deadlock will not necessarily do so: for some interleavings of requests it will deadlock, but for others it will not. By delaying when some resource requests are processed, a system can avoid interleavings that could lead to deadlock. </FONT><A id=x1-10000118 name=x1-10000118></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00414.gif" data-calibre-src="OEBPS/Images/image00414.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.18: </B>A process can be in a <EM>safe</EM>, <EM>unsafe</EM>, or <EM>deadlocked</EM> state. The dashed line illustrates a sequence of states visited by a thread &#8212; some are safe, some are unsafe, and the final state is a deadlock.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A deadlock-prone system can be in one of three states: a <EM>safe state</EM>, an <EM>unsafe state</EM>, and a <EM>deadlocked state</EM> (see Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000118"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.) </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:safe state"}'>safe state</A></EM>, for any possible sequence of resource requests, there is at least one <EM>safe sequence</EM> of processing the requests that eventually succeeds in granting all pending and future requests. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:unsafe state"}'>unsafe state</A></EM>, there is at least one sequence of future resource requests that leads to deadlock no matter what processing order is tried. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:deadlocked state"}'>deadlocked state</A></EM>, the system has at least one deadlock.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A system in a safe state controls its own destiny: for any workload, it can avoid deadlock by delaying the processing of some requests. In particular, the Banker&#8217;s Algorithm delays any request that takes it from a safe to an unsafe state. Once the system enters an unsafe state, it may not be able to avoid deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Notice that an unsafe state does not always lead to deadlock. A system in an unsafe state may remain that way or return to a safe state, depending on the specific interleaving of resource requests and completions. However, as long as the system remains in an unsafe state, a bad workload or unlucky scheduling of requests can force it to deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Banker&#8217;s Algorithm keeps a system in a safe state. The algorithm is based on a loose analogy with a small-town banker who has a maximum amount, total, that can be loaned at one time and a set of businesses that each have a credit line, max[i], for business i. A business borrows and pays back amounts of money as various projects start and end, so that business i always has an outstanding loan amount between 0 and max[i]. If all of a business&#8217;s requests within the credit line are granted, the business eventually reaches a state where all current projects are finished, and the loan balance returns to zero. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A conservative banker might issue credit lines only until the sum is at most the total funds that the banker has available. This approach is analogous to <EM>acquire-all</EM> or <EM>provide sufficient resources</EM>. It guarantees that the system remains in a safe state. All businesses with credit lines eventually complete their projects. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, a more aggressive banker can issue more credit as long as the bank can cover its commitment to each business &#8212; i.e., to provide a loan of max[i] if business i requests it. The algorithm assumes the bank is permitted to <EM>delay</EM> requests to increase a loan amount. For example, the bank might lose the paperwork for a few hours, days, or weeks. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By delaying loan requests, the bank remains in a safe state &#8212; a state for which there exists at least one series of loan fulfillments by which every business i can eventually receive its maximal loan max[i], complete its projects, and pay back all of its loan. The bank can then use that repaid money to grant pending loans to other businesses. </FONT><A id=x1-10000219 name=x1-10000219></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;class&nbsp;ResourceMgr{
&nbsp;&nbsp;&nbsp;private:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lock&nbsp;lock;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CV&nbsp;cv;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;r;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Number&nbsp;of&nbsp;resources
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;t;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Number&nbsp;of&nbsp;threads
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;avail[];&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;avail[i]:&nbsp;instances&nbsp;of&nbsp;resource&nbsp;i&nbsp;available
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;max[][];&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;max[i][j]:&nbsp;max&nbsp;of&nbsp;resource&nbsp;i&nbsp;needed&nbsp;by&nbsp;thread&nbsp;j
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;alloc[][];&nbsp;&nbsp;//&nbsp;alloc[i][j]:&nbsp;current&nbsp;allocation&nbsp;of&nbsp;resource&nbsp;i&nbsp;to&nbsp;thread&nbsp;j
&nbsp;&nbsp;&nbsp;...
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.19: </B>State maintained by the Banker Algorithm&#8217;s resource manager. Resource manager code is in Figures&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000320"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.20</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> and </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-10000320 name=x1-10000320></A><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Invariant:&nbsp;the&nbsp;system&nbsp;is&nbsp;in&nbsp;a&nbsp;safe&nbsp;state.
&nbsp;ResourceMgr::Request(int&nbsp;resourceID,&nbsp;int&nbsp;threadID)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.Acquire();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert(isSafe());
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(!wouldBeSafe(resourceID,&nbsp;threadID))&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cv.Wait(&amp;lock);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alloc[resourceID][threadID]++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avail[resourceID]--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;assert(isSafe());
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lock.Release();
&nbsp;}</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.20: </B>High-level pseudo-code for the Banker&#8217;s Algorithm. The state maintained by the algorithm is defined in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000219"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.19</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. The methods isSafe and wouldBeSafe are defined in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT><A id=x1-10000421 name=x1-10000421></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;A&nbsp;state&nbsp;is&nbsp;safe&nbsp;iff&nbsp;there&nbsp;exists&nbsp;a&nbsp;safe&nbsp;sequence&nbsp;of&nbsp;grants&nbsp;that&nbsp;are&nbsp;sufficient
&nbsp;//&nbsp;to&nbsp;allow&nbsp;all&nbsp;threads&nbsp;to&nbsp;eventually&nbsp;receive&nbsp;their&nbsp;maximum&nbsp;resource&nbsp;needs.
&nbsp;
&nbsp;bool
&nbsp;ResourceMgr::isSafe()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;j;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;toBeAvail[]&nbsp;=&nbsp;copy&nbsp;avail[];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;need[][]&nbsp;=&nbsp;max[][]&nbsp;-&nbsp;alloc[][];&nbsp;&nbsp;//&nbsp;need[i][j]&nbsp;is&nbsp;initialized&nbsp;to
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;max[i][j]&nbsp;-&nbsp;alloc[i][j]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;finish[]&nbsp;=&nbsp;[false,&nbsp;false,&nbsp;false,&nbsp;...];&nbsp;//&nbsp;finish[j]&nbsp;is&nbsp;true
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;if&nbsp;thread&nbsp;j&nbsp;is&nbsp;guaranteed&nbsp;to&nbsp;finish
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while&nbsp;(true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;j&nbsp;=&nbsp;any&nbsp;threadID&nbsp;such&nbsp;that:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(finish[j]&nbsp;==&nbsp;false)&nbsp;&amp;&amp;&nbsp;forall&nbsp;i:&nbsp;need[i][j]&nbsp;&lt;=&nbsp;toBeAvail[i];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(no&nbsp;such&nbsp;j&nbsp;exists)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(forall&nbsp;j:&nbsp;finish[j]&nbsp;==&nbsp;true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;false;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{&nbsp;&nbsp;//&nbsp;Thread&nbsp;j&nbsp;will&nbsp;eventually&nbsp;finish&nbsp;and&nbsp;return&nbsp;its
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;current&nbsp;allocation&nbsp;to&nbsp;the&nbsp;pool.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;finish[j]&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forall&nbsp;i:&nbsp;&nbsp;toBeAvail[i]&nbsp;=&nbsp;toBeAvail[i]&nbsp;+&nbsp;alloc[i][j];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
&nbsp;
&nbsp;//&nbsp;Hypothetically&nbsp;grant&nbsp;request&nbsp;and&nbsp;see&nbsp;if&nbsp;resulting&nbsp;state&nbsp;is&nbsp;safe.
&nbsp;
&nbsp;bool
&nbsp;ResourceMgr::wouldBeSafe(int&nbsp;resourceID,&nbsp;int&nbsp;threadID)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;result&nbsp;=&nbsp;false;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avail[resourceID]--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alloc[resourceID][threadID]++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(isSafe())&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avail[resourceID]++;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alloc[resourceID][threadID]--;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;result;
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.21: </B>Pseudo-code for the Banker&#8217;s Algorithm test whether the next state would be safe to enter. If not, the system delays until it would be safe.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000320"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.20</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows pseudo-code for a version of the Banker&#8217;s Algorithm that manages a set of r resources for a set of t threads. To simplify the discussion, threads request each unit of resource separately, but the algorithm can be extended to allow multiple resources to be requested at the same time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The high-level idea is simple: when a request arrives, wait to grant the request until it is safe to do so. As Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000219"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.19</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows, we can realize this high-level approach by tracking: (i) the current allocation of each resource to each thread, (ii) the maximum allocation possible for each thread, and (iii) the current set of available, unallocated resources. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows how to test whether a state is safe. Recall that a state is safe if some sequence of thread executions allows each thread to obtain its maximum resource need, finish its work, and release its resources. We first see if the currently free resources suffice to allow any thread to finish. If so, then the resources held by that thread will eventually be released back to the system. Next, we see if the currently free resources plus any resources held by the thread identified in the first step suffice to allow any other thread to finish; if so, the second thread&#8217;s resources will also eventually be released back to the system. We continue this process until we have identified all threads guaranteed to finish, provided we serve requests in a particular order. If that set includes all of the threads, the state is safe. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: Page allocation with the Banker&#8217;s Algorithm.</B> Suppose we have a system with 8 pages of memory and three processes: A, B, and C, which need 4, 5, and 5 pages to complete, respectively. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If they take turns requesting one page each, and the system grants requests in order, the system deadlocks, reaching a state where each process is stuck until some other process releases memory: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Process</B> </FONT></P></TD>
<TD class=td colSpan=6 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Allocation</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">A </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">B </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">C </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Total </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 6 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">On the other hand, if the system follows the Banker&#8217;s Algorithm, then it can delay some processes and guarantee that all processes eventually complete: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<DIV align=center>
<TABLE class=texttable border=0>
<TBODY>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Process</B> </FONT></P></TD>
<TD class=td colSpan=6 align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=multicolumn align=center noWrap><B><FONT style="BACKGROUND-COLOR: #7be1e1">Allocation</FONT></B></DIV></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">A </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>4</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>0</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">B </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>4</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>5</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>0</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">C </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>1</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>2</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>3</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>wait</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>4</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>5</B> </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; <B>0</B></FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">Total </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 1 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 2 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 3 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 6 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 6 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 7 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 8 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 4 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 5 </FONT></P></TD>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp; 0 </FONT></P></TD></TR>
<TR class=tr>
<TD class=td align=left>
<P class=tabp><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE></DIV>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">By delaying B and C in the ninth through twelfth steps, A can complete and release its resources. Then, by delaying C in the fifteenth and sixteenth steps, B can complete and release its resources. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The Banker&#8217;s Algorithm is noticeably more involved than other approaches we discuss. Although it is rarely used in its entirety, understanding the distinction between <EM>safe</EM>, <EM>unsafe</EM>, and <EM>deadlocked</EM> states and how deadlock events depend on request ordering are key to preventing deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Additionally, understanding the Banker&#8217;s Algorithm can help to design simple solutions for specific problems. For example, if we apply the Banker&#8217;s Algorithm to the Dining Philosopher&#8217;s problem, then it is safe for a philosopher to pick up a chopstick provided that afterwards (a) some philosopher will have two chopsticks or (b) a chopstick will remain on the table. In case (a), eventually that philosopher will finish eating and the other philosophers will be able to proceed. In case (b), the philosopher can pick up the chopstick because deadlock can still be avoided in the future. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Use the Banker&#8217;s Algorithm to devise a rule for when it is safe for a thread to acquire a pair of locks, A and B, with mutually recursive locking. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>Suppose a thread needs to acquire locks A and B, in that order, while another thread needs to acquire lock B first, then A. <B>A thread is always allowed to acquire its second lock. It may acquire its first lock provided the other thread does not already hold its first lock.</B> &#9633; </FONT><A id=x1-100005r168 name=x1-100005r168></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.5.5 </FONT><A id=x1-1010005 name=x1-1010005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Detecting and Recovering From Deadlocks</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Rather than preventing deadlocks, some systems allow deadlocks to occur and recover from them when they arise. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Why allow deadlocks to occur at all? Sometimes, it is difficult or expensive to enforce sufficient structure on the system&#8217;s data and workloads to guarantee that deadlock will never occur. If deadlocks are rare, why pay the overhead in the common case to prevent them? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For this approach to work, we need: (i) a way to recover from deadlock when it occurs, ideally with minimal harm to the goals of the user, and (ii) a way to detect deadlock so that we know when to invoke the recovery mechanism. We discuss recovery first because it provides context for understanding the tradeoffs in implementing detection. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-1020005 name=x1-1020005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Recovering From Deadlocks</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Recovering from a deadlock once it has occurred is challenging. A deadlock implies that some threads hold resources while waiting for others, and that progress is impossible. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because the resources are by definition not revocable, forcibly taking resources away from some or all of the deadlocked threads is not an ideal solution. As a simple example, if a process is part of a deadlock, some operating systems give the user the option to kill the process and release the process&#8217;s resources. Although this sounds drastic, if a deadlocked process cannot make any progress, killing it does not make the user much worse off. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">However, under the lock-based shared object programming abstractions we have discussed, killing all of the threads in a given process can be dangerous. If a deadlocked thread holds a lock on a shared kernel object, killing the thread and marking the lock as free could leave the kernel object in an inconsistent state. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Instead, we need some systematic way to recover when some required resource is unavailable. Two widely used approaches have been developed to deal with this issue: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Proceed without the resource.</B> Web services are often designed to be resilient to resource unavailability. A rule of thumb for the web is that a significant fraction of a web site&#8217;s customers will give up and go elsewhere if the site&#8217;s latency becomes too long, for whatever reason. Whether the problem is a hardware failure, software failure, or deadlock, does not really matter. The web site needs to be designed to quickly respond back to the user, regardless of the type of problem. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Amazon&#8217;s web site is a good example of this design paradigm. It is designed as an interlocking set of modules, where any individual module can be offline because of a failure. Thus, all other parts of the web site must be designed to be able to cope when some needed resource is unavailable. For example, under normal operation, Amazon&#8217;s software will check the inventory to ensure that an item is in stock before completing an order. However, if a deadlock or failure causes the inventory server to delay responding beyond some threshold, the front-end web server will give up, complete the order, and then queue a background check to make sure the item was in fact in the inventory. If the item was in fact not available (e.g., because some other customer purchased it in the meantime), an apology is sent to the customer. As long as that does not happen often, it can be better than making the customer wait, especially in the case of deadlock, where the wait could be indefinite. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Because deadlocks are rare and hard to test for, this requires coding discipline to handle error conditions systematically throughout the program. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<DIV class=sidebar align=center><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><SPAN class=sidebar_name><B><I>Optimistic concurrency control</I></B></SPAN> </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Transactions can also be used to avoid deadlocks. Optimistic concurrency control lets transactions execute in parallel without locking any data, but it only lets a transaction commit if none of the objects accessed by the transaction have been modified since the transaction began. Otherwise, the transaction must abort and retry. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To implement transactions with optimistic concurrency control, Each transaction keeps track of which versions of which objects it reads and updates. All updates are applied to a local copy. Then, before a transaction commits, the system verifies that no object the transaction accessed has been modified in the meantime; if there is a conflict, the transaction must abort. Of course, committing a transaction may invalidate other transactions that are in progress (ones that use data modified by this transaction). Those conflicts will be detected when the later transactions try to commit. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Optimistic concurrency control works well when different transactions most commonly use different subsets of data. In these cases, the approach not only eliminates deadlock, but it also maximizes concurrency since threads do not wait for locks. On the other hand, many conflicting, concurrent transactions increase overhead by repeatedly rolling back and re-executing transactions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT></DIV>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Transactions: rollback and retry.</B> A more general technique is used by <EM>transactions</EM>; transactions provide a safe mechanism for revoking resources assigned to a thread. We discuss transactions in detail in Chapter&nbsp;14; they are widely used in both databases and file systems. For deadlock recovery, transactions provide two important services: </FONT></P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-102002x1 name=x1-102002x1></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread rollback.</B> Transactions ensure that revoking locks from a thread does not leave the system&#8217;s objects in an inconsistent state. Instead, we rollback, or undo, the deadlocked thread&#8217;s actions to a clean state. To fix the deadlock, we can choose one or more victim threads, stop them, undo their actions, and let other threads proceed. </FONT></P>
<LI class=enumerate><A id=x1-102004x2 name=x1-102004x2></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Thread restarting.</B> Once the deadlock is broken and other threads have completed some or all of their work, the victim thread is restarted. When these threads complete, the system behaves as if the victim threads never caused a deadlock but, instead, just had their executions delayed.</FONT></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A transaction defines a safe point for rollback and restart. Each transaction has a beginTransaction and endTransaction statement; rollback undoes all changes back to beginTransaction. After a rollback, the thread can be safely restarted at the beginTransaction. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A key feature of transactions is that no other thread is allowed to see the results of a transaction until the transaction completes. That way, if the changes a transaction makes need to be rolled back due to a deadlock, only that one thread is affected. This can be accomplished with two-phase locking, provided locks are not released until after the transaction is complete. If the transaction is successful, it <EM>commits</EM>, the transaction&#8217;s locks are released, and the transaction&#8217;s changes to shared state become visible to other threads. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If, however, a transaction fails to reach its endTransaction statement (e.g., because of a deadlock or because some other exception occurred), the transaction <EM>aborts</EM>. The system can reset all of the state modified by the transaction to what it was when the transaction began. One way to support this is to maintain a copy of the initial values of all state modified by each transaction; this copy can be discarded when the transaction commits. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If a transactional system becomes deadlocked, the system can abort one or more of the deadlocked transactions. Aborting these transactions rolls back the system&#8217;s state to what it would have been if these transactions had never started and releases the aborted transactions&#8217; locks and other resources. If aborting the chosen transactions releases sufficient resources, the deadlock is broken, and the remaining transactions can proceed. If not, the system can abort additional transactions. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A related question that arises in transactional systems is <EM>which</EM> thread to abort and which threads to allow to proceed. An important consideration is liveness. Progress can be ensured, and starvation avoided, by prioritizing the oldest transactions. Then, when the system needs to abort some transaction, it can abort the <EM>youngest</EM>. This ensures that <EM>some</EM> transaction, e.g., the oldest, will eventually complete. The aborted transaction eventually becomes the oldest, and so it also will complete. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An example of this approach is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:wound wait"}'>wound wait</A></EM>. With wound wait, a younger transaction may wait for a resource held by an older transaction. Eventually, the older transaction will complete and release the resource, so deadlock cannot result. However, if an older transaction needs to wait on a resource held by a younger transaction, the resource is preempted and the younger transaction is aborted and restarted. </FONT></P></LI></UL>
<H5 class=subsubsectionHead><A id=x1-1030005 name=x1-1030005></A><FONT style="BACKGROUND-COLOR: #7be1e1">Detecting Deadlock</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Once we have a general way to recover from a deadlock, we need a way to tell if a deadlock has occurred, so we know when to trigger the recovery. An important consideration is that the detection mechanism can be conservative: it can trigger the repair if we <EM>might</EM> be in a deadlock state. This approach risks a false positive where a non-deadlocked thread is incorrectly classified as deadlocked. Depending on the overhead of the repair operation, it can sometimes be more efficient to use a simpler mechanism for detection even if that leads to the occasional false positive. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, a program can choose to wait only briefly (or not to wait at all) before declaring that recovery is needed. We saw an example earlier with how Amazon&#8217;s web site is designed. As another example, in old-style, circuit-switched telephone networks, a call reserved a circuit at a series of switches along its path. If the connection setup failed to find a free circuit at any hop, rather than wait for a circuit at the next hop to become free, it cancelled the connection attempt and gave the user an error message, &#8220;All circuits are busy. Please try again later." </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A modern analogue is the Internet. When a router is overloaded and runs out of packet buffers, it simply drops incoming packets. An alternative would be for each router to wait to send a packet until it knows the next router has room &#8212; an approach that could lead to deadlock. Precisely identifying whether deadlock has occurred would incur more overhead than simply dropping and resending some packets. </FONT><A id=x1-10300122 name=x1-10300122></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00415.gif" data-calibre-src="OEBPS/Images/image00415.gif"> </FONT></P>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.22: </B>Example graphs used for deadlock detection. Left: single instance of each resource. Right: multiple instances of one resource. Threads and resources are nodes; directed edges represent the <EM>owned by</EM> and <EM>waiting for</EM> relationships among them. </FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">There are various ways to identify deadlocks more precisely. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If there are several resources and only one thread can hold each resource at a time (e.g., one printer, one keyboard, and one audio speaker or several mutual exclusion locks), then we can detect a deadlock by analyzing a simple graph. In the graph, shown on the left in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10300122"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.22</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, each thread and each resource is represented by a node. There is a directed edge (i) from a resource to a thread if the resource is <EM>owned by</EM> the thread and (ii) from a thread to a resource if the thread is <EM>waiting for</EM> the resource. There is a deadlock if and only if there is a cycle in such a graph. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">If there are multiple instances of some resources, then we represent a resource with k interchangeable instances (e.g., k equivalent printers) as a node with k connection points. This is illustrated by the right graph in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10300122"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.22</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. Now, a cycle is a necessary but not sufficient condition for deadlock. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another solution, described by Coffman, Elphick, and Shoshani in 1971 is a variation of Dijkstra&#8217;s Banker&#8217;s Algorithm. In this algorithm, we assume we no longer know max[][], so we cannot assess whether the current state is safe or whether some future sequence of requests can force deadlock. However, we can look at the current set of resources, granted requests, and pending requests and ask whether it is possible for the current set of requests to eventually be satisfied assuming no more requests come and all threads eventually complete. If so, there is no deadlock (although we may be in an unsafe state); otherwise, there is a deadlock. </FONT><A id=x1-10300223 name=x1-10300223></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;A&nbsp;state&nbsp;is&nbsp;safe&nbsp;iff&nbsp;there&nbsp;exists&nbsp;a&nbsp;safe&nbsp;sequence&nbsp;of&nbsp;grants&nbsp;that&nbsp;would&nbsp;allow
&nbsp;//&nbsp;all&nbsp;threads&nbsp;to&nbsp;eventually&nbsp;receive&nbsp;their&nbsp;maximum&nbsp;resource&nbsp;needs.
&nbsp;//
&nbsp;//&nbsp;avail[]&nbsp;holds&nbsp;free&nbsp;resource&nbsp;count
&nbsp;//&nbsp;alloc[][]&nbsp;holds&nbsp;current&nbsp;allocation
&nbsp;//&nbsp;request[][]&nbsp;holds&nbsp;currently-blocked&nbsp;requests
&nbsp;bool
&nbsp;ResourceMgr::isDeadlocked()&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;j;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;toBeAvail[]&nbsp;=&nbsp;copy&nbsp;avail[];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bool&nbsp;finish[]&nbsp;=&nbsp;[false,&nbsp;false,&nbsp;false,&nbsp;...];&nbsp;//&nbsp;finish[j]&nbsp;is&nbsp;true&nbsp;if&nbsp;thread
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;j&nbsp;is&nbsp;guaranteed&nbsp;to&nbsp;finish
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while(true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;j&nbsp;=&nbsp;any&nbsp;threadID&nbsp;such&nbsp;that&nbsp;(finish[j]&nbsp;==&nbsp;false)&nbsp;&amp;&amp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(forall&nbsp;i:&nbsp;request[i][j]&nbsp;&lt;=&nbsp;toBeAvail[i]);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(no&nbsp;such&nbsp;j&nbsp;exists)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;(forall&nbsp;j:&nbsp;finish[j]&nbsp;==&nbsp;true)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;false;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;else&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Thread&nbsp;j&nbsp;*may*&nbsp;eventually&nbsp;finish&nbsp;and
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;return&nbsp;its&nbsp;current&nbsp;allocation&nbsp;to&nbsp;the&nbsp;pool.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;finish[j]&nbsp;=&nbsp;true;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;forall&nbsp;i:&nbsp;toBeAvail[i]&nbsp;=&nbsp;toBeAvail[i]&nbsp;+&nbsp;alloc[i][j];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}
&nbsp;}
</FONT></PRE>
<DIV class=caption align=center>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;6.23: </B>Coffman et al.&#8217;s test for deadlock. This algorithm is similar to the isSafe() test of the Banker&#8217;s Algorithm shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">.</FONT></P></TD></TR></TBODY></TABLE></DIV><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10300223"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.23</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the pseudo-code for the isDeadlocked method, a variation of the isSafe method shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-10000421"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.21</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> for the Banker&#8217;s Algorithm. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One might hope that we could avoid deadlock by asking, &#8220;Will satisfying the current request put us in a deadlocked state?&#8221; and then blocking any request that does. The Coffman et al. algorithm highlights that deadlock is determined not just by what requests are granted but also by what requests are waiting. The request that triggers deadlock (&#8220;circular wait&#8221;) will be a request that waits, not one that is granted. </FONT><A id=x1-103003r160 name=x1-103003r160></A></P><A id=x1-1040006 name=x1-1040006>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.6 Non-Blocking Synchronization</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Chapter&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'><FONT style="BACKGROUND-COLOR: #7be1e1">5</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> described a core abstraction for synchronization &#8212; shared objects, with one lock per object. This abstraction works well for building multi-threaded programs the vast majority of the time. As concurrent programs become more complicated, however, issues of lock contention, the semantics of operations that span multiple objects, and deadlock can arise. Worse, the solutions to these issues often require us to compromise modularity; for example, whether a particular program can deadlock requires understanding in detail how the implementations of various shared objects interact. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Some researchers have posed a radical question: would it be better to write complex concurrent programs without locks? By eliminating locking, we would remove lock contention and deadlock as design considerations, fostering a more modular program structure. However, these techniques can be <EM>much</EM> more complex to use. To date, concurrent implementations without locks have only been used for a few carefully designed runtime library modules written by expert programmers. We sketch the ideas because there is a chance that they will become more important as the number of processors per computer continues to increase. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Today, the cases where these approaches are warranted are rare. These advanced techniques should only be considered by experienced programmers who have mastered the basic lock-based approaches. Many of you will probably never need to use these techniques. If you are tempted to do so, take extra care. Measure the performance of your system to ensure that these techniques yield significant gains, and seek out extra peer review from trusted colleagues to help ensure that the code works as intended. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Programmers often assume that acquiring a lock is an expensive operation, and therefore try to reduce locking throughout their programs. The most likely result from premature optimization is a program that is buggy, hard to maintain, no faster than a clean implementation, and, ironically, harder to tune than a cleanly architected program. On most platforms, acquiring or releasing a lock is a highly tuned primitive &#8212; acquiring an uncontended lock is often nearly free. If there is contention, you probably needed the lock! </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-850003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, we saw an example of synchronization without locks. RCU lets reads proceed without acquiring a lock or updating shared synchronization state, but it still requires updates to acquire locks. If the thread that holds the lock is interrupted, has a bug that causes it to stop making progress, or becomes deadlocked, other threads can be delayed for a long &#8212; perhaps unlimited &#8212; period of time. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">It is possible to build data structures that are completely <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:non-blocking data structure"}'>non-blocking</A></EM> for both read and write operations. A non-blocking method is one where one thread is never required to wait for another thread to complete its operation. Acquiring a lock is a blocking operation: if the thread holding the lock stops, is delayed, or deadlocks, all other threads must wait for it to finish the critical section. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">More formally, a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:wait-free data structures"}'>wait-free data structure</A></EM> is one that guarantees progress for every thread: every method finishes in a finite number of steps, regardless of the state of other threads executing in the data structure or their rate of execution. A <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:lock-free data structures"}'>lock-free data structure</A></EM> is one that guarantees progress for some thread: some method will finish in a finite number of steps. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">A common building block for wait-free and lock-free data structures is the atomic compare-and-swap instruction available on most modern processors. We saw a taste of this in the implementation of the MCS lock in Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-850003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.3</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. There, we used compare-and-swap to atomically append to a linked list of waiting threads <EM>without first acquiring a lock</EM>. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Wait-free and lock-free data structures apply this idea more generally to completely eliminate the use of locks. For example, a lock-free hash table could be built as an array of pointers to each bucket: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Lookup.</B> A lookup de-references the pointer and checks the bucket. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Update.</B> To update a bucket, the thread allocates a new copy of the bucket, and then uses compare-and-swap to atomically replace the pointer if and only if it has not been changed in the meantime. If two threads simultaneously attempt to update the bucket (for example, to add a new entry), one succeeds and the other must retry.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The logic can be much more complex for more intricate data structures, and as a result, designing efficient wait-free and lock-free data structures remains the domain of experts. Nonetheless, non-blocking algorithms exist for a wide range of data structures, including FIFO queues, double-ended queues, LIFO stacks, sets, and balanced trees. Several of these can be found in the Java Virtual Machine runtime library. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In addition, considerable effort has also gone into studying ways to automate the construction of wait-free and lock-free data structures. For example, transactions with optimistic concurrency control provide a very flexible approach to implementing lock-free applications. Recall that optimistic concurrency control lets transactions proceed without locking the data they access. Transactions abort if, at commit-time, any of their accessed data has changed in the meantime. Most modern databases use a form of optimistic concurrency control to provide atomic and fault-tolerant updates of on-disk data structures. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>EXAMPLE: </B>Is optimistic concurrency control lock-free, wait-free, or both? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>ANSWER: </B>To see that <B>optimistic concurrency control is lock-free,</B> consider two conflicting transactions executing at the same time. The first one to commit succeeds, and the second must abort and retry. <B>An implementation is wait-free if it uses wound wait or some other mechanism to bound the number of retries for a transaction to successfully commit.</B> &#9633; </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Extending this idea, <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:software transactional memory (STM)"}'>software transactional memory (STM)</A></EM> is a promising approach to support general-purpose transactions for in-memory data structures. Unfortunately, the cost of an STM transaction is often significantly higher than that of a traditional critical section; this is because of the need to maintain the state required to check dependencies and the state required either to update the object if there is no conflict or to roll back its state if a conflict is detected. It is an open question whether the overhead of STM can be reduced to where it can be used more widely. In situations where STM can be used, it provides a way to compose different modules without having to lock contention or deadlock concerns. </FONT><A id=x1-104001r178 name=x1-104001r178></A></P><A id=x1-1050007 name=x1-1050007>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">6.7 Summary and Future Directions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Advanced synchronization techniques should be approached with caution. Your first goal should be to construct a program that works, even if doing so means putting &#8220;one big lock&#8221; around everything in a data structure or even in an entire program. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Resist the temptation to do anything more complicated unless you <B>know</B> that doing so is necessary. How do you know? Do not guess. Measure your system&#8217;s performance. Measuring the &#8220;before&#8221; and &#8220;after&#8221; performance of a program and its subsystems not only helps you make good decisions about the program on which you are working, but it also helps you develop good intuition for the programs you write in the future. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Spend time early in the design process developing a clean structure for your program. Given that issues with multi-object synchronization often blur module boundaries, it is vital to have an overall structure that lets you reason about how the different pieces of your program will interact. Strive for a strict layering or hierarchy of modules. It is easier to make such programs deadlock-free, and it is easier to test them as well. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Although performance is important, it is usually easier to start with a clean, simple, and correct design, measure it to identify its bottlenecks, and then optimize the bottlenecks than to start with a complex design and try to tune its performance, let alone fix its bugs. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In this chapter, we have presented a set of conceptual tools and techniques for managing complex, multi-object concurrent programs. We have addressed: estimating the impact of locks on multiprocessor performance, design patterns to reduce contention for locks, implementation techniques such as MCS and RCU for high-contention locks, strategies for achieving atomicity across multiple operations on the same object or across objects, and algorithms for deadlock prevention and recovery. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Yet, writing concurrent programs remains frustratingly complex. We believe that an important area for future work will be to develop better tools for managing and reducing that complexity. The last decade has seen the development of a new generation of tools for helping programmers improve software reliability, by automatically identifying test coverage, memory leaks, reuse of de-allocated data, buffer overflows, and bad pointer arithmetic. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Extending this approach to concurrent programs is a grand challenge. A promising avenue is to use automated tools for detecting memory races; a well-written program should have no reads or writes to shared memory without holding the lock that protects that data structure. Once a program has been shown to be without races, model checking can be used to systematically test that shared objects work for all possible thread interleavings. </FONT><A id=Q1-1-180 name=Q1-1-180></A><A id=Q1-1-181 name=Q1-1-181></A></P><A id=x1-1060007 name=x1-1060007>
<H3 class=likesectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">Exercises</FONT></H3></A>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=problems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9400113"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.13</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> shows the parallel execution of some requests and an equivalent sequential execution &#8212; request 1 then request 2 then request 3. Two other sequential executions are also equivalent to the parallel execution shown in the figure. What are these other equivalent sequential executions? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Generalize the rules for two-phase locking to include both mutual exclusion locks and readers/writers locks. What can be done in the expanding phase? What can be done in the contracting phase? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Consider the variation of the Dining Philosophers problem shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-9801817"}'><FONT style="BACKGROUND-COLOR: #7be1e1">6.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, where all unused chopsticks are placed in the center of the table and any philosopher can eat with any two chopsticks. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One way to prevent deadlock in this system is to </FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-990003"}'><FONT style="BACKGROUND-COLOR: #7be1e1">provide sufficient resources</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. For a system with n philosophers, what is the minimum number of chopsticks that ensures deadlock freedom? Why? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">If the queues between stages are finite, is it possible for a staged architecture to deadlock even if each individual stage is internally deadlock free? If so, give an example. If not, prove it. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose you build a system using a staged architecture with some fixed number of threads operating in each stage. Assuming each stage is individually deadlock free, describe two ways to guarantee that your system as a whole cannot deadlock. Each way should eliminate a different one of the 4 necessary conditions for deadlock. </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a system with four mutual exclusion locks (A, B, C, and D) and a readers/writers lock (E). Suppose the programmer follows these rules: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<OL class=subproblems>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Processing for each request is divided into two parts. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">During the first part, no lock may be released, and, if E is held in writing mode, it cannot be downgraded to reading mode. Furthermore, lock A may not be acquired if any of locks B, C, D, or E are held in any mode. Lock B may not be acquired if any of locks C, D, or E are held in any mode. Lock C may not be acquired if any of locks D or E are held in any mode. Lock D may not be acquired if lock E is held in any mode. Lock E may always be acquired in read mode or write mode, and it can be upgraded from read to write mode but not downgraded from write to read mode. </FONT>
<P></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">During the second part, any lock may be released, and lock E may be downgraded from write mode to read mode; releases and downgrades can happen in any order; by the end of part 2, all locks must be released; and no locks may be acquired or upgraded. </FONT>
<P></P></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Do these rules ensure serializability? Do they ensure freedom from deadlock? Why? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">In RCUList::remove, a possible strategy to increase concurrency would be to hold a read lock while searching for the target item, and to grab the write lock once it is found. Specifically: (i) replace the writeLock and writeUnlock calls with readLock and readUnlock calls, and (ii) insert new writeLock and writeUnlock calls at the beginning and end of the code that is executed when the if conditional test succeeds. Will this work? </FONT>
<P></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Implement a highly concurrent, multi-threaded file buffer cache. A buffer cache stores recently used disk blocks in memory for improved latency and throughput. Disk blocks have unique numbers and are fixed size. The cache provides two routines: </FONT>
<P><BR></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;void&nbsp;blockread(char&nbsp;*x,&nbsp;int&nbsp;blocknum);
   &nbsp;
   &nbsp;void&nbsp;blockwrite(char&nbsp;*x,&nbsp;int&nbsp;blocknum);</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">These routines read/write complete, block-aligned, fixed-size blocks. blockread reads a block of data into x; blockwrite (eventually) writes the data in x to disk. On a read, if the requested data is in the cache, the buffer will return it. Otherwise, the buffer must fetch the data from disk, making room in the cache by evicting a block as necessary. If the evicted block is modified, the cache must first write the modified data back to disk. On a write, if the block is not already in the buffer, it must make room for the new block. Modified data is stored in the cache and written back later to disk when the block is evicted. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multiple threads can call blockread and blockwrite concurrently, and to the maximum degree possible, those operations should be allowed to complete in parallel. You should assume the disk driver has been implemented; it provides the same interface as the file buffer cache: diskblockread and diskblockwrite. The disk driver routines are synchronous (the calling thread blocks until the disk operation completes) and re-entrant (while one thread is blocked, other threads can call into the driver to queue requests). </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Suppose we have a version of the Dining Philosopher&#8217;s problem where the chopsticks are placed in the middle of the table, each Philosopher needs three chopsticks before she will start to eat, and every Philosopher will return all of their chopsticks to the shared pool when done eating. (For example, the Philosopher needs two chopsticks to eat with and one to point at the white board.) </FONT>
<P></P>
<OL class=subproblems>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Using the Banker&#8217;s Algorithm, devise a rule for when is it safe for a Philosopher to pick up a chopstick. Explain why. </FONT>
<LI><FONT style="BACKGROUND-COLOR: #7be1e1">Now suppose each Philosopher needs k chopsticks, for k &gt; 3. Generalize the rule you developed above to work for any k. </FONT></LI></OL></LI></OL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT>
<DIV style="break-after: always; -webkit-column-break-after: always"><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></DIV><BR><BR><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><A id=x1-1070007 name=x1-1070007><BR><BR><FONT style="BACKGROUND-COLOR: #7be1e1">