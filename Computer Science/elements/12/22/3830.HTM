<strong><font color="blue">Operating Systems: Principles and Practice (Second Edition) Volume II : </font></strong><h3 class=sectionHead>6.2 Lock Design Patterns</H3></A><FONT style="BACKGROUND-COLOR: #ffffff">We next discuss a set of approaches that can reduce the impact of locking on multiprocessor performance. Often, the best practice is to start simple, with a single lock per shared object. If an object&#8217;s interface is well designed, then refactoring its implementation to increase concurrency and performance can be done once the system is built and performance measurements can identify any bottlenecks. An adage to follow is: &#8220;It is easier to go from a working system to a working, fast system than to go from a fast system to a fast, working system.&#8221; </FONT>
<P>We discuss four design patterns to increase concurrency when it is necessary: </P>
<UL class=itemize1>
<LI class=itemize>
<P><B>Fine-Grained Locking.</B> Partition an object&#8217;s state into different subsets each protected by a different lock. </P>
<LI class=itemize>
<P><B>Per-Processor Data Structures.</B> Partition an object&#8217;s state so that all or most accesses are performed by threads running on the same processor. </P>
<LI class=itemize>
<P><B>Ownership Design Pattern.</B> Remove a shared object from a shared container so that only a single thread can read or modify the object. </P>
<LI class=itemize>
<P><B>Staged Architecture.</B> Divide system into multiple stages, so that only those threads within each stage can access that stage&#8217;s shared data.</P></LI></UL><A id=x1-80001r125 name=x1-80001r125></A>
<H4 class=subsectionHead>6.2.1 <A id=x1-810001 name=x1-810001></A>Fine-Grained Locking</H4>A simple and widely used approach to decrease contention for a shared lock is to partition the shared object&#8217;s state into different subsets, each protected by its own lock. This is called <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:fine-grained locking"}'>fine-grained locking</A></EM>. 
<P>The web server cache discussed above provides an example. The cache can use a shared hash table to store and locate recently used web pages; because the hash table is shared, it needs a lock to provide mutual exclusion. The lock is acquired and released at the start and end of each of the hash table methods: put(key, value), value = get(key), and value = remove(key). </P>
<P>If the single lock limits performance, an alternative is to have one lock per hash bucket. The methods acquire the lock for bucket b before accessing any record that hashes to that bucket. Provided that the number of buckets is large enough, and no single bucket receives a large fraction of requests, then different threads can use and update the hash table in parallel. </P>
<P>However, there is no free lunch. Dividing an object&#8217;s state into different pieces protected by different locks can significantly increase the object&#8217;s complexity. Suppose we want to implement a hash table whose number of hash buckets grows as the number of objects it stores increases. If we have a single lock, this is easy to do. But, what if we use fine-grained locking? Then, the design becomes more complex because we have some methods, like put and get, that operate on one bucket and other methods, like resize, that operate across multiple buckets. </P>
<P>Several solutions are possible: </P>
<OL class=enumerate1>
<LI class=enumerate><A id=x1-81002x1 name=x1-81002x1></A>
<P><B>Introduce a readers/writers lock.</B> Suppose we have a readers/writers lock on the overall structure of the hash table (e.g., the number of buckets and the array of buckets) and a mutual exclusion lock on each bucket. Methods that work on a single bucket at a time, such as put and get, acquire the table&#8217;s readers/writers lock in read mode and also acquire the relevant bucket&#8217;s mutual exclusion lock. Methods that change the table&#8217;s structure, such as resize, must acquire the readers/writers lock in write mode; the readers/writers lock prevents any other threads from using the hash table while it is being resized. </P>
<LI class=enumerate><A id=x1-81004x2 name=x1-81004x2></A>
<P><B>Acquire every lock.</B> Methods that change the structure of the hash table, such as resize, must first iterate through every bucket, acquiring its lock, before proceeding. Once resize has a lock on every bucket, it is guaranteed that no other thread is concurrently accessing or modifying the hash table. </P>
<LI class=enumerate><A id=x1-81006x3 name=x1-81006x3></A>
<P><B>Divide the hash key space.</B> Another solution is to divide the hash key space into r regions, to have a mutual exclusion lock for each region, and to allow each region to be resized independently when it becomes heavily loaded. Then, get, put, and resizeRegion each acquire the relevant region&#8217;s mutual exclusion lock.</P></LI></OL>
<P>Which solution is best? It is not obvious. The first solution is simple and appears to allow high concurrency, but acquiring the readers/writers lock even in read mode may have high overhead. For example, we gave an implementation of a readers/writers lock in Chapter&nbsp;<A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-390005"}'>5</A> where acquiring a read-only lock involves acquiring a mutual exclusion lock on both entrance and exit. Access to the underlying mutual exclusion lock may become a bottleneck. </P>
<P>The second solution makes resize expensive, but if resize is a rare operation, that may be acceptable. The third solution balances concurrency for get/put against the cost of resize, but it is more complex and may require tuning the number of groups to get good performance. </P>
<P>Further, these trade-offs may change as the implementation becomes more complex. For example, to trigger resize at appropriate times, we probably need to maintain an additional nObjects count of the number of objects currently stored in the hash table, so whatever locking approach we use would need to be extended to cover this information. </P>
<P><B>EXAMPLE: </B>How might you use fine-grained locking to reduce contention for the lock protecting the shared memory heap in malloc/free or new/delete? </P>
<P><B>ANSWER: </B><B>One approach would be to partition the heap into separate memory regions, each with its own lock.</B> For example, a fast implementation of a heap on a uniprocessor uses n buckets, where the ith bucket contains blocks of size 2<SUP>i</SUP>, and serves requests of size 2<SUP>i-1</SUP> + 1 to 2<SUP>i</SUP>. If there are no free blocks in the ith bucket, an item from the next larger bucket i + 1 is split in two. Using fine-grained locking, each bucket can be given its own lock. &#9633; <A id=x1-81007r133 name=x1-81007r133></A></P>
<H4 class=subsectionHead>6.2.2 <A id=x1-820002 name=x1-820002></A>Per-Processor Data Structures</H4>A related technique to fine-grained locking is to partition the shared data structure based on the number of processors on the machine. For example, instead of one shared hash table of cached pages, an alternative design would have N separate hash tables, where N is the number of processors. Each thread uses the hash table based on the processor where it is currently running. Each hash table still needs its own lock in case a thread is context switched in the middle of an operation, but in the common case, only threads running on the same processor contend for the same lock. 
<P>Often, this is combined with a per-processor ready list, ensuring that each thread preferentially runs on the same processor each time it is context switched, further improving execution speed. </P>
<P>An advantage of this approach is better hardware cache behavior; as we saw in the previous section, shared data that must be communicated between processors can slow down the execution of critical sections. Of course, the disadvantage is that the hash tables are now partitioned, so that a web page may be cached in one processor&#8217;s hash table, and needed in another. Whether this is a performance benefit depends on the relative impact of reducing communication of shared data versus the decreased effectiveness of the cache. </P>
<P><B>EXAMPLE: </B>How might you use per-processor data structures to reduce contention for the memory heap? Under what conditions would this work well? </P>
<P><B>ANSWER: </B><B>The heap can be partitioned into N separate memory regions, one for each processor.</B> Calls to malloc/new would use the local heap; free/delete would return the data to the heap where it was allocated. <B>This would perform well provided that (i) rebalancing the heaps was rare and (ii) most allocated data is freed by the thread that acquires it.</B> &#9633; <A id=x1-82001r134 name=x1-82001r134></A></P>
<H4 class=subsectionHead>6.2.3 <A id=x1-830003 name=x1-830003></A>Ownership Design Pattern</H4>A common synchronization technique in large, multi-threaded programs is an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:ownership design pattern"}'>ownership design pattern</A></EM>. In this pattern, a thread removes an object from a container and can then access the object without holding a lock: the program structure guarantees that at most one thread owns an object at a time. <A id=x1-830012 name=x1-830012></A>
<HR>

<CENTER><img alt="" src="about:../Images/image00403.gif" data-calibre-src="OEBPS/Images/image00403.gif"> </CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><B>Figure&nbsp;6.2: </B>A multi-stage server based on the ownership pattern. In the first stage, one thread exclusively owns each network connection. In later stages, one thread parses and renders a given object at a time.</P></TD></TR></TBODY></TABLE>
<HR>

<P>As an example, a single web page can contain multiple objects, including HTML frames, style sheets, and images. Consider a multi-threaded web browser whose processing is divided into three stages: receiving an object via the network, parsing the object, and rendering the object (see Figure&nbsp;<A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-830012"}'>6.2</A>). The first stage has one thread per network connection; the other stages have several worker threads, each of which processes one object at a time. </P>
<P>The work queues between stages coordinate object ownership. Objects in the queues are not being accessed by any thread. When a worker thread in the <EM>parse</EM> stage removes an object from the stage&#8217;s work queue, it owns the object and has exclusive access to it. When the thread is done parsing the object, it puts it into the second queue and stops accessing it. A worker thread from the <EM>render</EM> stage then removes it from the second queue, gaining exclusive access to it to render it to the screen. </P>
<P><B>EXAMPLE: </B>How might you use the ownership design pattern to reduce contention for the memory heap? </P>
<P><B>ANSWER: </B>Ownership can be seen as an extension of per-processor data structures; instead of one heap per processor, <B>we can have one heap per thread.</B> Provided that the same thread that allocates memory also frees it, the thread can safely use its own heap without a lock and only return to the global heap when the local heap is out of space. &#9633; </P>
<P></P>
<DIV class=sidebar align=center>
<HR>

<TABLE cellPadding=10>
<TBODY>
<TR>
<TD style="BACKGROUND-COLOR: #f0f0f0" align=left>
<P width=0><SPAN class=sidebar_name><B><I>Commutative interface design</I></B></SPAN> </P>
<P>Class and interface design can often constrain implementations in ways that require locking. An example is the UNIX API. Like most operating systems, the UNIX open system call returns a file handle that is used for further operations on the file; the same system call is also used to initialize a network socket. The open call gives the operating system the ability to allocate internal data structures to track the current state of the file or network socket, and more broadly, which files and sockets are in use. </P>
<P>UNIX also specifies that each successive call to open returns the next integer file handle; as we saw in Chapter&nbsp;3, the UNIX shell uses this feature when redirecting stdin and stdout to a file or pipe. </P>
<P>A consequence of the design of the UNIX API is that the implementation of open requires a lock. For early UNIX systems, this was not an issue, but modern multi-threaded web servers open extremely large numbers of network sockets and files. Because of the semantics of the API, the implementation of open cannot use fine-grained locking or a per-processor data structure. </P>
<P>A better choice, where possible, is to design the API to be <EM>commutative</EM>: the result of two calls is the same regardless of which call was made first. For example, if the implementation can return any unique integer as a file handle, rather than the next successive one, then the implementation could allocate out of a per-processor bucket of open file handles. The implementation would then need a lock only for the special case of allocating specific handles such as stdin and stdout. </P>
<P></P></TD></TR></TBODY></TABLE>
<HR>
</DIV><A id=x1-83002r135 name=x1-83002r135></A>
<H4 class=subsectionHead>6.2.4 <A id=x1-840004 name=x1-840004></A>Staged Architecture</H4><A id=x1-840013 name=x1-840013></A>
<HR>

<CENTER><img alt="" src="about:../Images/image00404.gif" data-calibre-src="OEBPS/Images/image00404.gif"> </CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><B>Figure&nbsp;6.3: </B>A staged architecture for a simple web server.</P></TD></TR></TBODY></TABLE>
<HR>
The <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:staged architecture"}'>staged architecture</A></EM> pattern, illustrated in Figure&nbsp;<A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-840013"}'>6.3</A>, divides a system into multiple subsystems, called stages. Each stage includes state private to the stage and a set of one or more worker threads that operate on that state. Different stages communicate by sending messages to each other via shared producer-consumer queues. Each worker thread repeatedly pulls the next message from a stage&#8217;s incoming queue and then processes it, possibly producing one or more messages for other stages&#8217; queues. 
<P>Figure&nbsp;<A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-840013"}'>6.3</A> shows a staged architecture for a simple web server that has a first <EM>connect</EM> stage that uses one thread to set up network connections and that passes each connection to a second <EM>read and parse</EM> stage. </P>
<P>The <EM>read and parse</EM> stage has several threads, each of which repeatedly gets a connection from the incoming queue, reads a request from the connection, parses the request to determine what web page is being requested, and checks to see if the page is already cached. </P>
<P>Assuming the page is not already cached, if the request is for a static web page (e.g., an HTML file), the <EM>read and parse</EM> stage passes the request and connection to the <EM>read static page</EM> stage, where one of the stage&#8217;s threads reads the specified page from disk. Otherwise, the <EM>read and parse</EM> stage passes the request and connection to the <EM>generate dynamic page</EM> stage, where one of the stage&#8217;s threads runs a program that dynamically generates a page in response to the request. </P>
<P>Once the page has been fetched or generated, the page and connection are passed to the <EM>send page</EM> stage, where one of the threads transmits the page over the connection. </P>
<P>The key property of a staged architecture is that the state of each stage is private to that stage. This improves modularity, making it easier to reason about each stage individually and about interactions across stages. </P>
<P>As an example of the modularity benefits, consider a system where different stages are produced by different teams or even different companies. Each stage can be designed and tested almost independently, and the system is likely to work as expected when the stages are brought together. For example, it is common practice for a web site to use a web server from one company and a database from another company and for the two to communicate via messages. </P>
<P>Another benefit is improved cache locality. A thread operating on a subset of the system&#8217;s state may have better cache behavior than a thread that accesses state from all stages. On the other hand, for some workloads, passing a request from stage to stage could hurt cache behavior compared to doing all of the processing for a request on one processor. </P>
<P>Also note that for good performance, the processing in each stage must be large enough to amortize the cost of sending and receiving messages. </P>
<P>The special case of exactly one thread per stage is <EM>event-driven programming</EM>, described in Chapter&nbsp;<A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-100004"}'>4</A>. With event-driven programming, there is no concurrency within a stage, so no locking is required. Each message is processed atomically with respect to that stage&#8217;s state. </P>
<P>One challenge with staged architectures is dealing with overload. System throughput is limited by the throughput of the slowest stage. If the system is overloaded, the slowest stage will fall behind, and its work queue will grow. Depending on the system&#8217;s implementation, two bad things could happen. First, the queue could grow indefinitely, consuming more and more memory until the system memory heap is exhausted. Second, if the queue is limited to a finite size, once that size is reached, earlier stages must either discard work for the overloaded stage or block until the queue has room. Notice that if they block, then the backpressure will limit the throughput of earlier stages to that of the bottleneck stage, and their queues in turn may begin to grow. </P>
<P>One solution is to dynamically vary the number of threads per stage. If a stage&#8217;s incoming queue is growing, the program can shift processing resources to it by reducing the number of threads for a lightly-loaded stage in favor of more threads for the stage that is falling behind. <A id=x1-84002r132 name=x1-84002r132></A></P><A id=x1-850003 name=x1-850003>