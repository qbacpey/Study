<STRONG><FONT style="BACKGROUND-COLOR: #7be1e1" color=blue>Operating Systems: Principles and Practice (Second Edition) Volume II : </FONT></STRONG>
<H3 class=sectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.9 Alternative Abstractions</FONT></H3></A><FONT style="BACKGROUND-COLOR: #7be1e1">Although threads are a common way to express and manage concurrency, they are not the only way. In this section, we describe two popular alternatives, each targeted at a different application domain: </FONT>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Asynchronous I/O and event-driven programming.</B> Asynchronous I/O and events allow a single-threaded program to cope with high-latency I/O devices by overlapping I/O with processing and other I/O. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Data parallel programming.</B> With data parallel programming, all processors perform the same instructions in parallel on different parts of a data set.</FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In each case, the goal is similar: to replace the complexities of multi-threading with a deterministic, sequential model that is easier for the programmer to understand and debug. </FONT><A id=x1-32001r56 name=x1-32001r56></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.9.1 </FONT><A id=x1-330001 name=x1-330001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Asynchronous I/O and Event-Driven Programming</FONT></H4><EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:asynchronous I/O"}'><FONT style="BACKGROUND-COLOR: #7be1e1">Asynchronous I/O</FONT></A></EM><FONT style="BACKGROUND-COLOR: #7be1e1"> is a way to allow a single-threaded process to issue multiple concurrent I/O requests at the same time. The process makes a system call to issue an I/O request but the call returns immediately, without waiting for the result. At a later time, the operating system provides the result to the process by either: (1) calling a signal handler, (2) placing the result in a queue in the process&#8217;s memory, or (3) storing the result in kernel memory until the process makes another system call to retrieve it. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">An example use of asynchronous I/O is to overlap reading from disk with other computation in the same process. Reading from disk can take tens of milliseconds. In Linux, rather than issuing a read system call that blocks until the requested data has been read from disk, a process can issue an aio_read (asynchronous I/O read) system call; this call tells the operating system to initiate the read from disk and then to immediately return. Later, the process can call aio_error to determine if the disk read has finished and aio_return to retrieve the read&#8217;s results, as shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3300116"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.16</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. </FONT><A id=x1-3300116 name=x1-3300116></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00396.gif" data-calibre-src="OEBPS/Images/image00396.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.16: </B>An asynchronous file read on Linux. The application calls aio_read to start the read; this system call returns immediately after the disk read is initialized. The application may then do other processing while the disk is completing the requested operation. The disk interrupts the processor when the operation is complete; this causes the kernel disk interrupt handler to run. The application at any time may ask the kernel if the results of the disk read are available, and then retrieve them with aio_return.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One common design pattern lets a single thread interleave different I/O-bound tasks by waiting for different I/O events. Consider a web server with 10 active clients. Rather than creating one thread per client and having each thread do a blocking read on the network connection, an alternative is for the server to have one thread that processes, in turn, the next message to arrive from <EM>any</EM> client. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For this, the server does a select call that blocks until <EM>any</EM> of the 10 network connections has data available to read. When the select call returns, it provides a list of connection with available data. The thread can then read from those connections, knowing that the read will always return immediately. After processing the data, the thread then calls select again to wait for the next data to arrive. Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3300217"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.17</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1"> illustrates this design pattern. </FONT><A id=x1-3300217 name=x1-3300217></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P></P>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00397.gif" data-calibre-src="OEBPS/Images/image00397.gif"></FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.17: </B>A server managing multiple concurrent connections using select. The server calls select to wait for data to arrive on any connection. The server then reads all available data, before returning to select. </FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Asynchronous I/O allows progress by many concurrent operating system requests. This approach gives rise to an <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:event-driven programming"}'>event-driven programming</A></EM> pattern where a thread spins in a loop; each iteration gets and processes the next I/O event. To process each event, the thread typically maintains for each task a <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:continuation"}'>continuation</A></EM>, a data structure that keeps track of a task&#8217;s current state and next step. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, handling a web request can involve a series of I/O steps: (a) make a network connection, (b) read a request from the network connection, (c) read the requested data from disk, and (d) write the requested data to the network connection. If a single thread is handling requests from multiple different clients at once, it must keep track of where it is in that sequence for each client. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Further, the network may divide a client&#8217;s request into several packets so that the server needs to make several read calls to assemble the full packet. The server may be doing this request assembly for multiple clients at once. Therefore, it needs to keep several per-client variables (e.g., a request buffer, the number of bytes expected, and the number of bytes received so far). When a new message arrives, the thread uses the network connection&#8217;s port number to identify which client sent the request and retrieves the appropriate client&#8217;s variables using this port number/client ID. It can then process the data. </FONT></P>
<H5 class=subsubsectionHead><A id=x1-340001 name=x1-340001></A><FONT style="BACKGROUND-COLOR: #7be1e1">Event-Driven Programming vs. Threads</FONT></H5><FONT style="BACKGROUND-COLOR: #7be1e1">Although superficially different, overlapping I/O is fundamentally the same whether using asynchronous I/O and event-driven programming or synchronous I/O and threads. In either case, the program blocks until the next task can proceed, restores the state of that task, executes the next step of that task, and saves the task&#8217;s state until it can take its next step. The differences are: (1) whether the state is stored in a continuation or TCB and (2) whether the state save/restore is done explicitly by the application or automatically by the thread system. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Consider a simple server that collects incoming data from several clients into a set of per-client buffers. The pseudo-code for the event-driven and thread-per-client cases is similar: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Event-driven
&nbsp;Hashtable&lt;Buffer*&gt;&nbsp;*hash;
&nbsp;
&nbsp;while(1)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;connection&nbsp;=&nbsp;use&nbsp;select()&nbsp;to&nbsp;find&nbsp;a
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;readable&nbsp;connection&nbsp;ID
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer&nbsp;=&nbsp;hash.remove(connection);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;got&nbsp;=&nbsp;read(connection,&nbsp;tmpBuf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TMP_SIZE);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer-&gt;append(tmpBuf,&nbsp;got);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer&nbsp;=&nbsp;hash.put(connection,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer);
&nbsp;}
 </FONT></PRE><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">&nbsp;//&nbsp;Thread-per-client
&nbsp;Buffer&nbsp;*b;
&nbsp;
&nbsp;while(1)&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;got&nbsp;=&nbsp;read(connection,&nbsp;tmpBuf,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TMP_SIZE);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;buffer-&gt;append(tmpBuf,&nbsp;got);
&nbsp;}
 </FONT></PRE>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">When these programs execute, the system performs nearly the same work, as shown in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-3400118"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.18</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">. With events, the code uses select to determine which connection&#8217;s packet to retrieve next. With threads, the kernel transparently schedules each thread when data has arrived for it. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The state in both cases is also similar. In the event-driven case, the application maintains a hash table containing the buffer state for each client. The server must do a lookup to find the buffer each time a packet arrives for a particular client. In the thread-per-client case, each thread has just one buffer, and the operating system keeps track of the different threads&#8217; states. </FONT><A id=x1-3400118 name=x1-3400118></A></P><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<CENTER><FONT style="BACKGROUND-COLOR: #7be1e1"><img alt="" src="file:///[PrimaryStorage]Images/image00398.gif" data-calibre-src="OEBPS/Images/image00398.gif"> </FONT></CENTER>
<TABLE cellPadding=10>
<TBODY>
<TR>
<TD align=left>
<P class=caption width=0><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Figure&nbsp;4.18: </B>Two alternate implementations of a server. In the upper picture, a single thread uses a hash table to keep track of connection state. In the lower picture, each thread keeps a pointer to the state for one connection.</FONT></P></TD></TR></TBODY></TABLE><FONT style="BACKGROUND-COLOR: #7be1e1">
<HR>
</FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">To compare the two approaches, consider again the various use cases for threads from Section&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-110001"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.1</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">: </FONT></P>
<UL class=itemize1>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance: Coping with high-latency I/O devices.</B> Either approach &#8212; event-driven or threads &#8212; can overlap I/O and processing. Which provides better performance? </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The common wisdom has been that the event-driven approach was significantly faster for two reasons. First, the space and context switch overheads of this approach could be lower because a thread system must use generic code that allocates a stack for each thread&#8217;s state and that saves and restores all registers on each context switch, while the event-driven approach lets programmers allocate and save/restore just the state needed for each task. Second, some past operating systems had inefficient or unscalable implementations of their thread systems, making it important not to create too many threads for each process. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Today, the comparison is less clear cut. Many systems now have large memories, so the cost of allocating a thread stack for each task is less critical. For example, allocating 1000 threads with an 8 KB stack per thread on a machine with 1 GB of memory would consume less than 1% of the machine&#8217;s memory. Also, most operating systems now have efficient and scalable threads libraries. For example, while the Linux 2.4 kernel had poor performance when processes had many threads, Linux 2.6 revamped the thread system, improving its scalability and absolute performance. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Anecdotal evidence suggests that the performance gap between the two approaches has greatly narrowed. For some applications, highly optimized thread management code and synchronous I/O paths can out-perform less-optimized application code and asynchronous I/O paths. In most cases, the performance difference is small enough that other factors (e.g., code simplicity and ease of maintenance) are more important than raw performance. If performance is crucial for a particular application, then, as is often the case, there is no substitute for careful benchmarking before making your decision. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Performance: Exploiting multiple processors.</B> By itself, the event-driven approach does not help a program exploit multiple processors. In practice, event-driven and thread approaches are often combined: a program that uses n processors can have n threads, each of which uses the event-driven pattern to multiplex multiple I/O-bound tasks on each processor. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Responsiveness: Shifting work to run in the background.</B> While event-driven programming can be effective when tasks are usually short-lived, threads can be more convenient when there is a mixture of foreground and background tasks. At some cost in coding complexity, the event-driven model can be adapted to this case, e.g., by cutting long tasks into smaller chunks whose state can be explicitly saved when higher priority work is pending. </FONT></P>
<LI class=itemize>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><B>Program structure: Expressing logically concurrent tasks.</B> Whenever there are two viable programming styles, there are strong advocates for each approach. The situation is no different here, with some advocates of event-driven programming arguing that the synchronization required when threads share data makes threads more complex than events. Advocates for threads argue that they provide a more natural way to express the control flow of a program than having to explicitly store a computation&#8217;s state in a continuation. </FONT></P></LI></UL>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">In our opinion, there remain cases where both styles are appropriate, and we use both styles in our own programs. That said, for most I/O-intensive programs, threads are preferable: they are often more natural, reasonably efficient, and simpler when running on multiple processors. </FONT><A id=x1-34002r58 name=x1-34002r58></A></P>
<H4 class=subsectionHead><FONT style="BACKGROUND-COLOR: #7be1e1">4.9.2 </FONT><A id=x1-350002 name=x1-350002></A><FONT style="BACKGROUND-COLOR: #7be1e1">Data Parallel Programming</FONT></H4><FONT style="BACKGROUND-COLOR: #7be1e1">Another important application area is parallel computing, and there is an ongoing debate as to the effectiveness of threads versus other models for expressing and managing parallelism. </FONT>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">One popular model is <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:data parallel programming"}'>data parallel programming</A></EM>, also known as SIMD (single instruction multiple data) programming or <EM><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "glo:bulk synchronous parallel programming"}'>bulk synchronous parallel programming</A></EM>. In this model, the programmer describes a computation to apply in parallel across an entire data set at the same time, operating on independent data elements. The work on every data item must complete before moving onto the next step; one processor can use the results of a different processor only in some later step. As a result, the behavior of the program is deterministic. Rather than having programmers divide work among threads, the runtime system decides how to map the parallel work across the hardware&#8217;s processors. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">For example, taking the earlier example of zeroing a buffer in parallel in Figure&nbsp;</FONT><A data-9uzdtcnnj6xqoalwmdp3qa='{"name": "OEBPS/Text/part0000.xhtml", "frag": "x1-180017"}'><FONT style="BACKGROUND-COLOR: #7be1e1">4.7</FONT></A><FONT style="BACKGROUND-COLOR: #7be1e1">, a data parallel program to zero an N item array can be as simple as: </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1"><BR></FONT></P><PRE class=code><FONT style="BACKGROUND-COLOR: #7be1e1">   &nbsp;forall&nbsp;i&nbsp;in&nbsp;0:N-1
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;array[i]&nbsp;=&nbsp;0;</FONT></PRE><BR>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">The runtime system would divide the array among processors to execute the computation in parallel. Of course, the runtime system itself might be implemented using threads, but this is invisible to the programmer. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Large data-analysis tasks often use data parallel programming. For example, Hadoop is an open source system that can process and analyze terabytes of data spread across hundreds or thousands of servers. It applies an arbitrary computation to each data element, such as to update the popularity of a web page based on a previous estimate of the popularity of the pages that refer to it. Hadoop applies the computation in parallel across all web pages, repeatedly, until the popularity of every page has converged. A search engine can then use the results to decide which pages should be returned in response to a search query. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Another example is SQL (Structured Query Language). SQL is a standard language for accessing databases in which programmers specify the database query to perform, and the database maps the query to lower-level thread and disk operations. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Multimedia streams (e.g., audio, video, and graphics) often have large amounts of data on which similar operations are repeatedly performed, so data parallel programming is frequently used for media processing; specialized hardware to support this type of parallel processing is common. Because they are optimized for highly structured data parallel programs, GPUs (Graphical Processing Units) can provide significantly higher rates of data processing. For example, in 2013 a mid-range Radeon 7850 GPU was capable of 1.69 TFLOPS (Trillion FLoating point Operations Per Second (double-precision)); for comparison, an Intel i7 3960 CPU (a high-end, six core general-purpose processor) was capable of 0.19 double-precision TFLOPS. </FONT></P>
<P><FONT style="BACKGROUND-COLOR: #7be1e1">Considerable effort is currently going towards developing and using General Purpose GPUs (GPGPUs) &#8212; GPUs that better support a wider-range of programs. It is still not clear which classes of programs can work well with GPGPUs and which require more traditional CPU architectures, but for those programs that can be ported to the more restrictive GPGPU programming model, performance gains could be dramatic. </FONT><A id=x1-35001r57 name=x1-35001r57></A></P><A id=x1-3600010 name=x1-3600010><BR><BR><FONT style="BACKGROUND-COLOR: #7be1e1">